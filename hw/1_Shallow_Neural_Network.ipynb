{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chloe Quinto \n",
    "CS 583 HW 1  \n",
    "4/13/20    \n",
    "I pledge my honor that I have abided by the Stevens Honor System - Chloe Quinto    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import sklearn \n",
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical\n",
    "from keras import layers\n",
    "from keras import models \n",
    "from keras.layers import Dense\n",
    "\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine = sklearn.datasets.load_wine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, we attempt to build a neural network that can classify wines from 3 wineries by 13 attributes: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alcohol',\n",
       " 'malic_acid',\n",
       " 'ash',\n",
       " 'alcalinity_of_ash',\n",
       " 'magnesium',\n",
       " 'total_phenols',\n",
       " 'flavanoids',\n",
       " 'nonflavanoid_phenols',\n",
       " 'proanthocyanins',\n",
       " 'color_intensity',\n",
       " 'hue',\n",
       " 'od280/od315_of_diluted_wines',\n",
       " 'proline']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine.feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.423e+01, 1.710e+00, 2.430e+00, ..., 1.040e+00, 3.920e+00,\n",
       "        1.065e+03],\n",
       "       [1.320e+01, 1.780e+00, 2.140e+00, ..., 1.050e+00, 3.400e+00,\n",
       "        1.050e+03],\n",
       "       [1.316e+01, 2.360e+00, 2.670e+00, ..., 1.030e+00, 3.170e+00,\n",
       "        1.185e+03],\n",
       "       ...,\n",
       "       [1.327e+01, 4.280e+00, 2.260e+00, ..., 5.900e-01, 1.560e+00,\n",
       "        8.350e+02],\n",
       "       [1.317e+01, 2.590e+00, 2.370e+00, ..., 6.000e-01, 1.620e+00,\n",
       "        8.400e+02],\n",
       "       [1.413e+01, 4.100e+00, 2.740e+00, ..., 6.100e-01, 1.600e+00,\n",
       "        5.600e+02]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine.data # values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "       2, 2])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine.target # class names "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(wine.data)\n",
    "df.columns = wine.feature_names\n",
    "df[\"class\"] = wine.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alcohol</th>\n",
       "      <th>malic_acid</th>\n",
       "      <th>ash</th>\n",
       "      <th>alcalinity_of_ash</th>\n",
       "      <th>magnesium</th>\n",
       "      <th>total_phenols</th>\n",
       "      <th>flavanoids</th>\n",
       "      <th>nonflavanoid_phenols</th>\n",
       "      <th>proanthocyanins</th>\n",
       "      <th>color_intensity</th>\n",
       "      <th>hue</th>\n",
       "      <th>od280/od315_of_diluted_wines</th>\n",
       "      <th>proline</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113.0</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   alcohol  malic_acid   ash  alcalinity_of_ash  magnesium  total_phenols  \\\n",
       "0    14.23        1.71  2.43               15.6      127.0           2.80   \n",
       "1    13.20        1.78  2.14               11.2      100.0           2.65   \n",
       "2    13.16        2.36  2.67               18.6      101.0           2.80   \n",
       "3    14.37        1.95  2.50               16.8      113.0           3.85   \n",
       "4    13.24        2.59  2.87               21.0      118.0           2.80   \n",
       "\n",
       "   flavanoids  nonflavanoid_phenols  proanthocyanins  color_intensity   hue  \\\n",
       "0        3.06                  0.28             2.29             5.64  1.04   \n",
       "1        2.76                  0.26             1.28             4.38  1.05   \n",
       "2        3.24                  0.30             2.81             5.68  1.03   \n",
       "3        3.49                  0.24             2.18             7.80  0.86   \n",
       "4        2.69                  0.39             1.82             4.32  1.04   \n",
       "\n",
       "   od280/od315_of_diluted_wines  proline  class  \n",
       "0                          3.92   1065.0      0  \n",
       "1                          3.40   1050.0      0  \n",
       "2                          3.17   1185.0      0  \n",
       "3                          3.45   1480.0      0  \n",
       "4                          2.93    735.0      0  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "alcohol                         0\n",
       "malic_acid                      0\n",
       "ash                             0\n",
       "alcalinity_of_ash               0\n",
       "magnesium                       0\n",
       "total_phenols                   0\n",
       "flavanoids                      0\n",
       "nonflavanoid_phenols            0\n",
       "proanthocyanins                 0\n",
       "color_intensity                 0\n",
       "hue                             0\n",
       "od280/od315_of_diluted_wines    0\n",
       "proline                         0\n",
       "class                           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    71\n",
       "0    59\n",
       "2    48\n",
       "Name: class, dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"class\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(178, 1)\n",
      "(178, 13)\n"
     ]
    }
   ],
   "source": [
    "labels = df.loc[:,[\"class\"]] # class are already integer encoded \n",
    "features = df.drop([\"class\"],axis=1)\n",
    "print(labels.shape)\n",
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain, xtest, ytrain, ytest = train_test_split(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xtrain (133, 13)\n",
      "xtest (45, 13)\n",
      "ytrain (133, 1)\n",
      "ytest (45, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"xtrain\", xtrain.shape)\n",
    "print(\"xtest\", xtest.shape)\n",
    "print(\"ytrain\", ytrain.shape)\n",
    "print(\"ytest\", ytest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrain = to_categorical(ytrain, 3)\n",
    "ytest = to_categorical(ytest,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale data between 0 and 1 \n",
    "scale = MinMaxScaler(feature_range=(0,1))\n",
    "xtrain = scale.fit_transform(xtrain)\n",
    "xtest = scale.fit_transform(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(10, input_dim=13, activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(6, activation='relu'))\n",
    "model.add(Dense(6, activation='relu'))\n",
    "model.add(Dense(4, activation='relu'))\n",
    "model.add(Dense(3, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 10)                140       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 8)                 88        \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 6)                 54        \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 6)                 42        \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 4)                 28        \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 3)                 15        \n",
      "=================================================================\n",
      "Total params: 367\n",
      "Trainable params: 367\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers \n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", \n",
    "             optimizer = optimizers.RMSprop(lr = 1e-4), \n",
    "             metrics = [\"acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 133 samples, validate on 45 samples\n",
      "Epoch 1/1200\n",
      "133/133 [==============================] - 0s 2ms/step - loss: 1.1033 - acc: 0.4060 - val_loss: 1.1178 - val_acc: 0.3778\n",
      "Epoch 2/1200\n",
      "133/133 [==============================] - 0s 155us/step - loss: 1.1025 - acc: 0.4060 - val_loss: 1.1172 - val_acc: 0.3778\n",
      "Epoch 3/1200\n",
      "133/133 [==============================] - 0s 212us/step - loss: 1.1020 - acc: 0.4060 - val_loss: 1.1166 - val_acc: 0.3778\n",
      "Epoch 4/1200\n",
      "133/133 [==============================] - 0s 207us/step - loss: 1.1014 - acc: 0.4060 - val_loss: 1.1160 - val_acc: 0.3778\n",
      "Epoch 5/1200\n",
      "133/133 [==============================] - 0s 233us/step - loss: 1.1009 - acc: 0.4060 - val_loss: 1.1155 - val_acc: 0.3778\n",
      "Epoch 6/1200\n",
      "133/133 [==============================] - 0s 153us/step - loss: 1.1003 - acc: 0.4060 - val_loss: 1.1148 - val_acc: 0.3778\n",
      "Epoch 7/1200\n",
      "133/133 [==============================] - 0s 178us/step - loss: 1.0997 - acc: 0.4060 - val_loss: 1.1142 - val_acc: 0.3778\n",
      "Epoch 8/1200\n",
      "133/133 [==============================] - 0s 135us/step - loss: 1.0991 - acc: 0.4060 - val_loss: 1.1136 - val_acc: 0.3778\n",
      "Epoch 9/1200\n",
      "133/133 [==============================] - 0s 205us/step - loss: 1.0985 - acc: 0.4060 - val_loss: 1.1130 - val_acc: 0.3778\n",
      "Epoch 10/1200\n",
      "133/133 [==============================] - 0s 249us/step - loss: 1.0979 - acc: 0.4060 - val_loss: 1.1124 - val_acc: 0.3778\n",
      "Epoch 11/1200\n",
      "133/133 [==============================] - 0s 208us/step - loss: 1.0973 - acc: 0.4060 - val_loss: 1.1118 - val_acc: 0.3778\n",
      "Epoch 12/1200\n",
      "133/133 [==============================] - 0s 244us/step - loss: 1.0967 - acc: 0.4060 - val_loss: 1.1113 - val_acc: 0.3778\n",
      "Epoch 13/1200\n",
      "133/133 [==============================] - 0s 225us/step - loss: 1.0961 - acc: 0.4060 - val_loss: 1.1107 - val_acc: 0.3778\n",
      "Epoch 14/1200\n",
      "133/133 [==============================] - 0s 208us/step - loss: 1.0956 - acc: 0.4060 - val_loss: 1.1101 - val_acc: 0.3778\n",
      "Epoch 15/1200\n",
      "133/133 [==============================] - 0s 206us/step - loss: 1.0951 - acc: 0.4060 - val_loss: 1.1096 - val_acc: 0.3778\n",
      "Epoch 16/1200\n",
      "133/133 [==============================] - 0s 226us/step - loss: 1.0946 - acc: 0.4060 - val_loss: 1.1091 - val_acc: 0.3778\n",
      "Epoch 17/1200\n",
      "133/133 [==============================] - 0s 181us/step - loss: 1.0941 - acc: 0.4060 - val_loss: 1.1086 - val_acc: 0.3778\n",
      "Epoch 18/1200\n",
      "133/133 [==============================] - 0s 212us/step - loss: 1.0936 - acc: 0.4060 - val_loss: 1.1081 - val_acc: 0.3778\n",
      "Epoch 19/1200\n",
      "133/133 [==============================] - 0s 184us/step - loss: 1.0931 - acc: 0.4060 - val_loss: 1.1077 - val_acc: 0.3778\n",
      "Epoch 20/1200\n",
      "133/133 [==============================] - 0s 239us/step - loss: 1.0927 - acc: 0.4060 - val_loss: 1.1073 - val_acc: 0.3778\n",
      "Epoch 21/1200\n",
      "133/133 [==============================] - 0s 204us/step - loss: 1.0923 - acc: 0.4060 - val_loss: 1.1068 - val_acc: 0.3778\n",
      "Epoch 22/1200\n",
      "133/133 [==============================] - 0s 209us/step - loss: 1.0918 - acc: 0.4060 - val_loss: 1.1064 - val_acc: 0.3778\n",
      "Epoch 23/1200\n",
      "133/133 [==============================] - 0s 156us/step - loss: 1.0914 - acc: 0.4060 - val_loss: 1.1060 - val_acc: 0.3778\n",
      "Epoch 24/1200\n",
      "133/133 [==============================] - 0s 182us/step - loss: 1.0910 - acc: 0.4060 - val_loss: 1.1056 - val_acc: 0.3778\n",
      "Epoch 25/1200\n",
      "133/133 [==============================] - 0s 181us/step - loss: 1.0905 - acc: 0.4060 - val_loss: 1.1052 - val_acc: 0.3778\n",
      "Epoch 26/1200\n",
      "133/133 [==============================] - 0s 132us/step - loss: 1.0901 - acc: 0.4060 - val_loss: 1.1048 - val_acc: 0.3778\n",
      "Epoch 27/1200\n",
      "133/133 [==============================] - 0s 201us/step - loss: 1.0897 - acc: 0.4060 - val_loss: 1.1045 - val_acc: 0.3778\n",
      "Epoch 28/1200\n",
      "133/133 [==============================] - 0s 213us/step - loss: 1.0893 - acc: 0.4060 - val_loss: 1.1041 - val_acc: 0.3778\n",
      "Epoch 29/1200\n",
      "133/133 [==============================] - 0s 237us/step - loss: 1.0888 - acc: 0.4060 - val_loss: 1.1038 - val_acc: 0.3778\n",
      "Epoch 30/1200\n",
      "133/133 [==============================] - 0s 172us/step - loss: 1.0884 - acc: 0.4060 - val_loss: 1.1034 - val_acc: 0.3778\n",
      "Epoch 31/1200\n",
      "133/133 [==============================] - 0s 167us/step - loss: 1.0880 - acc: 0.4060 - val_loss: 1.1031 - val_acc: 0.3778\n",
      "Epoch 32/1200\n",
      "133/133 [==============================] - 0s 151us/step - loss: 1.0876 - acc: 0.4060 - val_loss: 1.1027 - val_acc: 0.3778\n",
      "Epoch 33/1200\n",
      "133/133 [==============================] - 0s 181us/step - loss: 1.0872 - acc: 0.4060 - val_loss: 1.1024 - val_acc: 0.3778\n",
      "Epoch 34/1200\n",
      "133/133 [==============================] - 0s 191us/step - loss: 1.0867 - acc: 0.4060 - val_loss: 1.1021 - val_acc: 0.3778\n",
      "Epoch 35/1200\n",
      "133/133 [==============================] - 0s 160us/step - loss: 1.0864 - acc: 0.4060 - val_loss: 1.1017 - val_acc: 0.3778\n",
      "Epoch 36/1200\n",
      "133/133 [==============================] - 0s 178us/step - loss: 1.0859 - acc: 0.4060 - val_loss: 1.1014 - val_acc: 0.3778\n",
      "Epoch 37/1200\n",
      "133/133 [==============================] - 0s 217us/step - loss: 1.0855 - acc: 0.4060 - val_loss: 1.1011 - val_acc: 0.3778\n",
      "Epoch 38/1200\n",
      "133/133 [==============================] - 0s 232us/step - loss: 1.0852 - acc: 0.4060 - val_loss: 1.1007 - val_acc: 0.3778\n",
      "Epoch 39/1200\n",
      "133/133 [==============================] - 0s 183us/step - loss: 1.0847 - acc: 0.4060 - val_loss: 1.1004 - val_acc: 0.3778\n",
      "Epoch 40/1200\n",
      "133/133 [==============================] - 0s 148us/step - loss: 1.0843 - acc: 0.4060 - val_loss: 1.1000 - val_acc: 0.3778\n",
      "Epoch 41/1200\n",
      "133/133 [==============================] - 0s 152us/step - loss: 1.0838 - acc: 0.4060 - val_loss: 1.0996 - val_acc: 0.3778\n",
      "Epoch 42/1200\n",
      "133/133 [==============================] - 0s 131us/step - loss: 1.0834 - acc: 0.4060 - val_loss: 1.0993 - val_acc: 0.3778\n",
      "Epoch 43/1200\n",
      "133/133 [==============================] - 0s 187us/step - loss: 1.0830 - acc: 0.4060 - val_loss: 1.0989 - val_acc: 0.3778\n",
      "Epoch 44/1200\n",
      "133/133 [==============================] - 0s 110us/step - loss: 1.0825 - acc: 0.4060 - val_loss: 1.0985 - val_acc: 0.3778\n",
      "Epoch 45/1200\n",
      "133/133 [==============================] - 0s 110us/step - loss: 1.0821 - acc: 0.4060 - val_loss: 1.0982 - val_acc: 0.3778\n",
      "Epoch 46/1200\n",
      "133/133 [==============================] - 0s 130us/step - loss: 1.0816 - acc: 0.4060 - val_loss: 1.0978 - val_acc: 0.3778\n",
      "Epoch 47/1200\n",
      "133/133 [==============================] - 0s 125us/step - loss: 1.0812 - acc: 0.4060 - val_loss: 1.0975 - val_acc: 0.3778\n",
      "Epoch 48/1200\n",
      "133/133 [==============================] - 0s 133us/step - loss: 1.0807 - acc: 0.4060 - val_loss: 1.0971 - val_acc: 0.3778\n",
      "Epoch 49/1200\n",
      "133/133 [==============================] - 0s 114us/step - loss: 1.0802 - acc: 0.4060 - val_loss: 1.0967 - val_acc: 0.3778\n",
      "Epoch 50/1200\n",
      "133/133 [==============================] - 0s 123us/step - loss: 1.0799 - acc: 0.4060 - val_loss: 1.0963 - val_acc: 0.3778\n",
      "Epoch 51/1200\n",
      "133/133 [==============================] - 0s 127us/step - loss: 1.0794 - acc: 0.4060 - val_loss: 1.0960 - val_acc: 0.3778\n",
      "Epoch 52/1200\n",
      "133/133 [==============================] - 0s 110us/step - loss: 1.0789 - acc: 0.4060 - val_loss: 1.0957 - val_acc: 0.3778\n",
      "Epoch 53/1200\n",
      "133/133 [==============================] - 0s 130us/step - loss: 1.0784 - acc: 0.4060 - val_loss: 1.0953 - val_acc: 0.3778\n",
      "Epoch 54/1200\n",
      "133/133 [==============================] - 0s 151us/step - loss: 1.0779 - acc: 0.4060 - val_loss: 1.0949 - val_acc: 0.3778\n",
      "Epoch 55/1200\n",
      "133/133 [==============================] - 0s 135us/step - loss: 1.0773 - acc: 0.4060 - val_loss: 1.0944 - val_acc: 0.3778\n",
      "Epoch 56/1200\n",
      "133/133 [==============================] - 0s 135us/step - loss: 1.0768 - acc: 0.4060 - val_loss: 1.0941 - val_acc: 0.3778\n",
      "Epoch 57/1200\n",
      "133/133 [==============================] - 0s 132us/step - loss: 1.0763 - acc: 0.4060 - val_loss: 1.0937 - val_acc: 0.3778\n",
      "Epoch 58/1200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133/133 [==============================] - 0s 149us/step - loss: 1.0758 - acc: 0.4060 - val_loss: 1.0933 - val_acc: 0.3778\n",
      "Epoch 59/1200\n",
      "133/133 [==============================] - 0s 263us/step - loss: 1.0752 - acc: 0.4060 - val_loss: 1.0928 - val_acc: 0.3778\n",
      "Epoch 60/1200\n",
      "133/133 [==============================] - 0s 208us/step - loss: 1.0747 - acc: 0.4060 - val_loss: 1.0924 - val_acc: 0.3778\n",
      "Epoch 61/1200\n",
      "133/133 [==============================] - 0s 195us/step - loss: 1.0742 - acc: 0.4060 - val_loss: 1.0920 - val_acc: 0.3778\n",
      "Epoch 62/1200\n",
      "133/133 [==============================] - 0s 282us/step - loss: 1.0737 - acc: 0.4060 - val_loss: 1.0916 - val_acc: 0.3778\n",
      "Epoch 63/1200\n",
      "133/133 [==============================] - 0s 251us/step - loss: 1.0731 - acc: 0.4060 - val_loss: 1.0912 - val_acc: 0.3778\n",
      "Epoch 64/1200\n",
      "133/133 [==============================] - 0s 205us/step - loss: 1.0725 - acc: 0.4060 - val_loss: 1.0907 - val_acc: 0.3778\n",
      "Epoch 65/1200\n",
      "133/133 [==============================] - 0s 202us/step - loss: 1.0720 - acc: 0.4060 - val_loss: 1.0902 - val_acc: 0.3778\n",
      "Epoch 66/1200\n",
      "133/133 [==============================] - 0s 175us/step - loss: 1.0714 - acc: 0.4060 - val_loss: 1.0898 - val_acc: 0.3778\n",
      "Epoch 67/1200\n",
      "133/133 [==============================] - 0s 250us/step - loss: 1.0709 - acc: 0.4060 - val_loss: 1.0894 - val_acc: 0.3778\n",
      "Epoch 68/1200\n",
      "133/133 [==============================] - 0s 194us/step - loss: 1.0704 - acc: 0.4060 - val_loss: 1.0890 - val_acc: 0.3778\n",
      "Epoch 69/1200\n",
      "133/133 [==============================] - 0s 118us/step - loss: 1.0698 - acc: 0.4060 - val_loss: 1.0886 - val_acc: 0.3778\n",
      "Epoch 70/1200\n",
      "133/133 [==============================] - 0s 122us/step - loss: 1.0692 - acc: 0.4060 - val_loss: 1.0880 - val_acc: 0.3778\n",
      "Epoch 71/1200\n",
      "133/133 [==============================] - 0s 150us/step - loss: 1.0685 - acc: 0.4060 - val_loss: 1.0876 - val_acc: 0.3778\n",
      "Epoch 72/1200\n",
      "133/133 [==============================] - 0s 177us/step - loss: 1.0680 - acc: 0.4060 - val_loss: 1.0871 - val_acc: 0.3778\n",
      "Epoch 73/1200\n",
      "133/133 [==============================] - 0s 183us/step - loss: 1.0674 - acc: 0.4060 - val_loss: 1.0867 - val_acc: 0.3778\n",
      "Epoch 74/1200\n",
      "133/133 [==============================] - 0s 147us/step - loss: 1.0668 - acc: 0.4060 - val_loss: 1.0862 - val_acc: 0.3778\n",
      "Epoch 75/1200\n",
      "133/133 [==============================] - 0s 117us/step - loss: 1.0662 - acc: 0.4060 - val_loss: 1.0857 - val_acc: 0.3778\n",
      "Epoch 76/1200\n",
      "133/133 [==============================] - 0s 100us/step - loss: 1.0655 - acc: 0.4060 - val_loss: 1.0851 - val_acc: 0.3778\n",
      "Epoch 77/1200\n",
      "133/133 [==============================] - 0s 168us/step - loss: 1.0650 - acc: 0.4060 - val_loss: 1.0846 - val_acc: 0.3778\n",
      "Epoch 78/1200\n",
      "133/133 [==============================] - 0s 281us/step - loss: 1.0643 - acc: 0.4060 - val_loss: 1.0840 - val_acc: 0.3778\n",
      "Epoch 79/1200\n",
      "133/133 [==============================] - 0s 225us/step - loss: 1.0637 - acc: 0.4060 - val_loss: 1.0835 - val_acc: 0.3778\n",
      "Epoch 80/1200\n",
      "133/133 [==============================] - 0s 264us/step - loss: 1.0631 - acc: 0.4060 - val_loss: 1.0830 - val_acc: 0.3778\n",
      "Epoch 81/1200\n",
      "133/133 [==============================] - 0s 240us/step - loss: 1.0626 - acc: 0.4060 - val_loss: 1.0825 - val_acc: 0.3778\n",
      "Epoch 82/1200\n",
      "133/133 [==============================] - 0s 470us/step - loss: 1.0620 - acc: 0.4060 - val_loss: 1.0821 - val_acc: 0.3778\n",
      "Epoch 83/1200\n",
      "133/133 [==============================] - 0s 223us/step - loss: 1.0614 - acc: 0.4060 - val_loss: 1.0816 - val_acc: 0.3778\n",
      "Epoch 84/1200\n",
      "133/133 [==============================] - 0s 170us/step - loss: 1.0608 - acc: 0.4060 - val_loss: 1.0811 - val_acc: 0.3778\n",
      "Epoch 85/1200\n",
      "133/133 [==============================] - 0s 167us/step - loss: 1.0602 - acc: 0.4060 - val_loss: 1.0806 - val_acc: 0.3778\n",
      "Epoch 86/1200\n",
      "133/133 [==============================] - 0s 166us/step - loss: 1.0595 - acc: 0.4060 - val_loss: 1.0800 - val_acc: 0.3778\n",
      "Epoch 87/1200\n",
      "133/133 [==============================] - 0s 196us/step - loss: 1.0588 - acc: 0.4060 - val_loss: 1.0794 - val_acc: 0.3778\n",
      "Epoch 88/1200\n",
      "133/133 [==============================] - 0s 182us/step - loss: 1.0582 - acc: 0.4060 - val_loss: 1.0789 - val_acc: 0.3778\n",
      "Epoch 89/1200\n",
      "133/133 [==============================] - 0s 185us/step - loss: 1.0575 - acc: 0.4060 - val_loss: 1.0783 - val_acc: 0.3778\n",
      "Epoch 90/1200\n",
      "133/133 [==============================] - 0s 143us/step - loss: 1.0568 - acc: 0.4060 - val_loss: 1.0778 - val_acc: 0.3778\n",
      "Epoch 91/1200\n",
      "133/133 [==============================] - 0s 123us/step - loss: 1.0561 - acc: 0.4060 - val_loss: 1.0772 - val_acc: 0.3778\n",
      "Epoch 92/1200\n",
      "133/133 [==============================] - 0s 283us/step - loss: 1.0554 - acc: 0.4060 - val_loss: 1.0767 - val_acc: 0.3778\n",
      "Epoch 93/1200\n",
      "133/133 [==============================] - 0s 292us/step - loss: 1.0547 - acc: 0.4060 - val_loss: 1.0761 - val_acc: 0.3778\n",
      "Epoch 94/1200\n",
      "133/133 [==============================] - 0s 255us/step - loss: 1.0540 - acc: 0.4060 - val_loss: 1.0755 - val_acc: 0.3778\n",
      "Epoch 95/1200\n",
      "133/133 [==============================] - 0s 382us/step - loss: 1.0533 - acc: 0.4060 - val_loss: 1.0749 - val_acc: 0.3778\n",
      "Epoch 96/1200\n",
      "133/133 [==============================] - 0s 222us/step - loss: 1.0525 - acc: 0.4060 - val_loss: 1.0743 - val_acc: 0.3778\n",
      "Epoch 97/1200\n",
      "133/133 [==============================] - 0s 230us/step - loss: 1.0518 - acc: 0.4060 - val_loss: 1.0737 - val_acc: 0.3778\n",
      "Epoch 98/1200\n",
      "133/133 [==============================] - 0s 237us/step - loss: 1.0511 - acc: 0.4060 - val_loss: 1.0731 - val_acc: 0.3778\n",
      "Epoch 99/1200\n",
      "133/133 [==============================] - 0s 230us/step - loss: 1.0503 - acc: 0.4060 - val_loss: 1.0725 - val_acc: 0.3778\n",
      "Epoch 100/1200\n",
      "133/133 [==============================] - 0s 1ms/step - loss: 1.0496 - acc: 0.4060 - val_loss: 1.0719 - val_acc: 0.3778\n",
      "Epoch 101/1200\n",
      "133/133 [==============================] - 0s 229us/step - loss: 1.0488 - acc: 0.4060 - val_loss: 1.0712 - val_acc: 0.3778\n",
      "Epoch 102/1200\n",
      "133/133 [==============================] - 0s 179us/step - loss: 1.0481 - acc: 0.4060 - val_loss: 1.0706 - val_acc: 0.3778\n",
      "Epoch 103/1200\n",
      "133/133 [==============================] - 0s 290us/step - loss: 1.0473 - acc: 0.4060 - val_loss: 1.0700 - val_acc: 0.3778\n",
      "Epoch 104/1200\n",
      "133/133 [==============================] - 0s 199us/step - loss: 1.0464 - acc: 0.4060 - val_loss: 1.0693 - val_acc: 0.3778\n",
      "Epoch 105/1200\n",
      "133/133 [==============================] - 0s 241us/step - loss: 1.0457 - acc: 0.4060 - val_loss: 1.0686 - val_acc: 0.3778\n",
      "Epoch 106/1200\n",
      "133/133 [==============================] - 0s 429us/step - loss: 1.0448 - acc: 0.4060 - val_loss: 1.0680 - val_acc: 0.3778\n",
      "Epoch 107/1200\n",
      "133/133 [==============================] - 0s 207us/step - loss: 1.0440 - acc: 0.4060 - val_loss: 1.0673 - val_acc: 0.3778\n",
      "Epoch 108/1200\n",
      "133/133 [==============================] - 0s 210us/step - loss: 1.0432 - acc: 0.4060 - val_loss: 1.0666 - val_acc: 0.3778\n",
      "Epoch 109/1200\n",
      "133/133 [==============================] - 0s 170us/step - loss: 1.0424 - acc: 0.4060 - val_loss: 1.0660 - val_acc: 0.3778\n",
      "Epoch 110/1200\n",
      "133/133 [==============================] - 0s 116us/step - loss: 1.0416 - acc: 0.4060 - val_loss: 1.0653 - val_acc: 0.3778\n",
      "Epoch 111/1200\n",
      "133/133 [==============================] - 0s 123us/step - loss: 1.0407 - acc: 0.4060 - val_loss: 1.0646 - val_acc: 0.3778\n",
      "Epoch 112/1200\n",
      "133/133 [==============================] - 0s 129us/step - loss: 1.0398 - acc: 0.4060 - val_loss: 1.0638 - val_acc: 0.3778\n",
      "Epoch 113/1200\n",
      "133/133 [==============================] - 0s 156us/step - loss: 1.0389 - acc: 0.4060 - val_loss: 1.0630 - val_acc: 0.3778\n",
      "Epoch 114/1200\n",
      "133/133 [==============================] - 0s 133us/step - loss: 1.0380 - acc: 0.4060 - val_loss: 1.0623 - val_acc: 0.3778\n",
      "Epoch 115/1200\n",
      "133/133 [==============================] - 0s 138us/step - loss: 1.0371 - acc: 0.4060 - val_loss: 1.0615 - val_acc: 0.3778\n",
      "Epoch 116/1200\n",
      "133/133 [==============================] - 0s 117us/step - loss: 1.0361 - acc: 0.4060 - val_loss: 1.0607 - val_acc: 0.3778\n",
      "Epoch 117/1200\n",
      "133/133 [==============================] - 0s 155us/step - loss: 1.0352 - acc: 0.4060 - val_loss: 1.0600 - val_acc: 0.3778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 118/1200\n",
      "133/133 [==============================] - 0s 160us/step - loss: 1.0343 - acc: 0.4060 - val_loss: 1.0593 - val_acc: 0.3778\n",
      "Epoch 119/1200\n",
      "133/133 [==============================] - 0s 142us/step - loss: 1.0334 - acc: 0.4060 - val_loss: 1.0586 - val_acc: 0.3778\n",
      "Epoch 120/1200\n",
      "133/133 [==============================] - 0s 148us/step - loss: 1.0325 - acc: 0.4060 - val_loss: 1.0579 - val_acc: 0.3778\n",
      "Epoch 121/1200\n",
      "133/133 [==============================] - 0s 191us/step - loss: 1.0317 - acc: 0.4060 - val_loss: 1.0572 - val_acc: 0.3778\n",
      "Epoch 122/1200\n",
      "133/133 [==============================] - 0s 192us/step - loss: 1.0308 - acc: 0.4060 - val_loss: 1.0565 - val_acc: 0.3778\n",
      "Epoch 123/1200\n",
      "133/133 [==============================] - 0s 135us/step - loss: 1.0300 - acc: 0.4060 - val_loss: 1.0558 - val_acc: 0.3778\n",
      "Epoch 124/1200\n",
      "133/133 [==============================] - 0s 136us/step - loss: 1.0290 - acc: 0.4060 - val_loss: 1.0551 - val_acc: 0.3778\n",
      "Epoch 125/1200\n",
      "133/133 [==============================] - 0s 127us/step - loss: 1.0282 - acc: 0.4060 - val_loss: 1.0544 - val_acc: 0.3778\n",
      "Epoch 126/1200\n",
      "133/133 [==============================] - 0s 195us/step - loss: 1.0273 - acc: 0.4060 - val_loss: 1.0537 - val_acc: 0.3778\n",
      "Epoch 127/1200\n",
      "133/133 [==============================] - 0s 246us/step - loss: 1.0264 - acc: 0.4060 - val_loss: 1.0530 - val_acc: 0.3778\n",
      "Epoch 128/1200\n",
      "133/133 [==============================] - 0s 303us/step - loss: 1.0255 - acc: 0.4060 - val_loss: 1.0522 - val_acc: 0.3778\n",
      "Epoch 129/1200\n",
      "133/133 [==============================] - 0s 287us/step - loss: 1.0245 - acc: 0.4060 - val_loss: 1.0514 - val_acc: 0.3778\n",
      "Epoch 130/1200\n",
      "133/133 [==============================] - 0s 277us/step - loss: 1.0236 - acc: 0.4060 - val_loss: 1.0506 - val_acc: 0.3778\n",
      "Epoch 131/1200\n",
      "133/133 [==============================] - 0s 361us/step - loss: 1.0225 - acc: 0.4060 - val_loss: 1.0498 - val_acc: 0.3778\n",
      "Epoch 132/1200\n",
      "133/133 [==============================] - 0s 915us/step - loss: 1.0215 - acc: 0.4060 - val_loss: 1.0490 - val_acc: 0.3778\n",
      "Epoch 133/1200\n",
      "133/133 [==============================] - 0s 455us/step - loss: 1.0204 - acc: 0.4060 - val_loss: 1.0481 - val_acc: 0.3778\n",
      "Epoch 134/1200\n",
      "133/133 [==============================] - 0s 238us/step - loss: 1.0194 - acc: 0.4060 - val_loss: 1.0472 - val_acc: 0.3778\n",
      "Epoch 135/1200\n",
      "133/133 [==============================] - 0s 385us/step - loss: 1.0184 - acc: 0.4060 - val_loss: 1.0464 - val_acc: 0.3778\n",
      "Epoch 136/1200\n",
      "133/133 [==============================] - 0s 356us/step - loss: 1.0173 - acc: 0.4060 - val_loss: 1.0455 - val_acc: 0.3778\n",
      "Epoch 137/1200\n",
      "133/133 [==============================] - 0s 201us/step - loss: 1.0162 - acc: 0.4060 - val_loss: 1.0447 - val_acc: 0.3778\n",
      "Epoch 138/1200\n",
      "133/133 [==============================] - 0s 367us/step - loss: 1.0152 - acc: 0.4060 - val_loss: 1.0439 - val_acc: 0.3778\n",
      "Epoch 139/1200\n",
      "133/133 [==============================] - 0s 173us/step - loss: 1.0142 - acc: 0.4060 - val_loss: 1.0430 - val_acc: 0.3778\n",
      "Epoch 140/1200\n",
      "133/133 [==============================] - 0s 206us/step - loss: 1.0130 - acc: 0.4060 - val_loss: 1.0422 - val_acc: 0.3778\n",
      "Epoch 141/1200\n",
      "133/133 [==============================] - 0s 178us/step - loss: 1.0119 - acc: 0.4060 - val_loss: 1.0413 - val_acc: 0.3778\n",
      "Epoch 142/1200\n",
      "133/133 [==============================] - 0s 198us/step - loss: 1.0108 - acc: 0.4060 - val_loss: 1.0404 - val_acc: 0.3778\n",
      "Epoch 143/1200\n",
      "133/133 [==============================] - 0s 227us/step - loss: 1.0098 - acc: 0.4060 - val_loss: 1.0396 - val_acc: 0.3778\n",
      "Epoch 144/1200\n",
      "133/133 [==============================] - 0s 161us/step - loss: 1.0086 - acc: 0.4060 - val_loss: 1.0387 - val_acc: 0.3778\n",
      "Epoch 145/1200\n",
      "133/133 [==============================] - 0s 162us/step - loss: 1.0076 - acc: 0.4060 - val_loss: 1.0378 - val_acc: 0.3778\n",
      "Epoch 146/1200\n",
      "133/133 [==============================] - 0s 189us/step - loss: 1.0066 - acc: 0.4060 - val_loss: 1.0370 - val_acc: 0.3778\n",
      "Epoch 147/1200\n",
      "133/133 [==============================] - 0s 125us/step - loss: 1.0055 - acc: 0.4060 - val_loss: 1.0362 - val_acc: 0.3778\n",
      "Epoch 148/1200\n",
      "133/133 [==============================] - 0s 142us/step - loss: 1.0043 - acc: 0.4060 - val_loss: 1.0353 - val_acc: 0.3778\n",
      "Epoch 149/1200\n",
      "133/133 [==============================] - 0s 149us/step - loss: 1.0032 - acc: 0.4060 - val_loss: 1.0343 - val_acc: 0.3778\n",
      "Epoch 150/1200\n",
      "133/133 [==============================] - 0s 142us/step - loss: 1.0021 - acc: 0.4060 - val_loss: 1.0334 - val_acc: 0.3778\n",
      "Epoch 151/1200\n",
      "133/133 [==============================] - 0s 195us/step - loss: 1.0010 - acc: 0.4060 - val_loss: 1.0326 - val_acc: 0.3778\n",
      "Epoch 152/1200\n",
      "133/133 [==============================] - 0s 132us/step - loss: 1.0000 - acc: 0.4060 - val_loss: 1.0318 - val_acc: 0.3778\n",
      "Epoch 153/1200\n",
      "133/133 [==============================] - 0s 116us/step - loss: 0.9988 - acc: 0.4060 - val_loss: 1.0310 - val_acc: 0.3778\n",
      "Epoch 154/1200\n",
      "133/133 [==============================] - 0s 158us/step - loss: 0.9977 - acc: 0.4060 - val_loss: 1.0301 - val_acc: 0.3778\n",
      "Epoch 155/1200\n",
      "133/133 [==============================] - 0s 118us/step - loss: 0.9968 - acc: 0.4060 - val_loss: 1.0293 - val_acc: 0.3778\n",
      "Epoch 156/1200\n",
      "133/133 [==============================] - 0s 109us/step - loss: 0.9956 - acc: 0.4060 - val_loss: 1.0284 - val_acc: 0.3778\n",
      "Epoch 157/1200\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.9711 - acc: 0.333 - 0s 157us/step - loss: 0.9945 - acc: 0.4060 - val_loss: 1.0275 - val_acc: 0.3778\n",
      "Epoch 158/1200\n",
      "133/133 [==============================] - 0s 179us/step - loss: 0.9934 - acc: 0.4060 - val_loss: 1.0266 - val_acc: 0.3778\n",
      "Epoch 159/1200\n",
      "133/133 [==============================] - 0s 216us/step - loss: 0.9922 - acc: 0.4060 - val_loss: 1.0258 - val_acc: 0.3778\n",
      "Epoch 160/1200\n",
      "133/133 [==============================] - 0s 157us/step - loss: 0.9910 - acc: 0.4060 - val_loss: 1.0248 - val_acc: 0.3778\n",
      "Epoch 161/1200\n",
      "133/133 [==============================] - 0s 329us/step - loss: 0.9899 - acc: 0.4060 - val_loss: 1.0239 - val_acc: 0.3778\n",
      "Epoch 162/1200\n",
      "133/133 [==============================] - 0s 282us/step - loss: 0.9887 - acc: 0.4060 - val_loss: 1.0230 - val_acc: 0.3778\n",
      "Epoch 163/1200\n",
      "133/133 [==============================] - 0s 238us/step - loss: 0.9874 - acc: 0.4060 - val_loss: 1.0221 - val_acc: 0.3778\n",
      "Epoch 164/1200\n",
      "133/133 [==============================] - 0s 146us/step - loss: 0.9863 - acc: 0.4060 - val_loss: 1.0212 - val_acc: 0.3778\n",
      "Epoch 165/1200\n",
      "133/133 [==============================] - 0s 125us/step - loss: 0.9850 - acc: 0.4060 - val_loss: 1.0203 - val_acc: 0.3778\n",
      "Epoch 166/1200\n",
      "133/133 [==============================] - 0s 136us/step - loss: 0.9838 - acc: 0.4060 - val_loss: 1.0194 - val_acc: 0.3778\n",
      "Epoch 167/1200\n",
      "133/133 [==============================] - 0s 274us/step - loss: 0.9828 - acc: 0.4060 - val_loss: 1.0185 - val_acc: 0.3778\n",
      "Epoch 168/1200\n",
      "133/133 [==============================] - 0s 149us/step - loss: 0.9816 - acc: 0.4060 - val_loss: 1.0177 - val_acc: 0.3778\n",
      "Epoch 169/1200\n",
      "133/133 [==============================] - 0s 291us/step - loss: 0.9805 - acc: 0.4060 - val_loss: 1.0169 - val_acc: 0.3778\n",
      "Epoch 170/1200\n",
      "133/133 [==============================] - 0s 226us/step - loss: 0.9794 - acc: 0.4060 - val_loss: 1.0159 - val_acc: 0.3778\n",
      "Epoch 171/1200\n",
      "133/133 [==============================] - 0s 323us/step - loss: 0.9781 - acc: 0.4060 - val_loss: 1.0150 - val_acc: 0.3778\n",
      "Epoch 172/1200\n",
      "133/133 [==============================] - 0s 197us/step - loss: 0.9769 - acc: 0.4060 - val_loss: 1.0141 - val_acc: 0.3778\n",
      "Epoch 173/1200\n",
      "133/133 [==============================] - 0s 211us/step - loss: 0.9757 - acc: 0.4060 - val_loss: 1.0131 - val_acc: 0.3778\n",
      "Epoch 174/1200\n",
      "133/133 [==============================] - 0s 322us/step - loss: 0.9747 - acc: 0.4060 - val_loss: 1.0122 - val_acc: 0.3778\n",
      "Epoch 175/1200\n",
      "133/133 [==============================] - 0s 133us/step - loss: 0.9733 - acc: 0.4060 - val_loss: 1.0113 - val_acc: 0.3778\n",
      "Epoch 176/1200\n",
      "133/133 [==============================] - 0s 223us/step - loss: 0.9721 - acc: 0.4060 - val_loss: 1.0103 - val_acc: 0.3778\n",
      "Epoch 177/1200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133/133 [==============================] - 0s 139us/step - loss: 0.9708 - acc: 0.4060 - val_loss: 1.0092 - val_acc: 0.3778\n",
      "Epoch 178/1200\n",
      "133/133 [==============================] - 0s 155us/step - loss: 0.9697 - acc: 0.4060 - val_loss: 1.0083 - val_acc: 0.3778\n",
      "Epoch 179/1200\n",
      "133/133 [==============================] - 0s 111us/step - loss: 0.9685 - acc: 0.4060 - val_loss: 1.0074 - val_acc: 0.3778\n",
      "Epoch 180/1200\n",
      "133/133 [==============================] - 0s 105us/step - loss: 0.9673 - acc: 0.4060 - val_loss: 1.0064 - val_acc: 0.3778\n",
      "Epoch 181/1200\n",
      "133/133 [==============================] - 0s 136us/step - loss: 0.9661 - acc: 0.4060 - val_loss: 1.0054 - val_acc: 0.3778\n",
      "Epoch 182/1200\n",
      "133/133 [==============================] - 0s 182us/step - loss: 0.9648 - acc: 0.4060 - val_loss: 1.0043 - val_acc: 0.3778\n",
      "Epoch 183/1200\n",
      "133/133 [==============================] - 0s 160us/step - loss: 0.9636 - acc: 0.4060 - val_loss: 1.0032 - val_acc: 0.3778\n",
      "Epoch 184/1200\n",
      "133/133 [==============================] - 0s 197us/step - loss: 0.9624 - acc: 0.4060 - val_loss: 1.0022 - val_acc: 0.3778\n",
      "Epoch 185/1200\n",
      "133/133 [==============================] - 0s 113us/step - loss: 0.9610 - acc: 0.4060 - val_loss: 1.0011 - val_acc: 0.3778\n",
      "Epoch 186/1200\n",
      "133/133 [==============================] - 0s 153us/step - loss: 0.9598 - acc: 0.4060 - val_loss: 1.0001 - val_acc: 0.3778\n",
      "Epoch 187/1200\n",
      "133/133 [==============================] - 0s 170us/step - loss: 0.9585 - acc: 0.4060 - val_loss: 0.9990 - val_acc: 0.3778\n",
      "Epoch 188/1200\n",
      "133/133 [==============================] - 0s 108us/step - loss: 0.9573 - acc: 0.4060 - val_loss: 0.9980 - val_acc: 0.3778\n",
      "Epoch 189/1200\n",
      "133/133 [==============================] - 0s 154us/step - loss: 0.9559 - acc: 0.4060 - val_loss: 0.9969 - val_acc: 0.3778\n",
      "Epoch 190/1200\n",
      "133/133 [==============================] - 0s 110us/step - loss: 0.9548 - acc: 0.4060 - val_loss: 0.9959 - val_acc: 0.3778\n",
      "Epoch 191/1200\n",
      "133/133 [==============================] - 0s 105us/step - loss: 0.9535 - acc: 0.4060 - val_loss: 0.9948 - val_acc: 0.3778\n",
      "Epoch 192/1200\n",
      "133/133 [==============================] - 0s 101us/step - loss: 0.9523 - acc: 0.4060 - val_loss: 0.9938 - val_acc: 0.3778\n",
      "Epoch 193/1200\n",
      "133/133 [==============================] - 0s 106us/step - loss: 0.9511 - acc: 0.4060 - val_loss: 0.9928 - val_acc: 0.3778\n",
      "Epoch 194/1200\n",
      "133/133 [==============================] - 0s 112us/step - loss: 0.9498 - acc: 0.4060 - val_loss: 0.9917 - val_acc: 0.3778\n",
      "Epoch 195/1200\n",
      "133/133 [==============================] - 0s 117us/step - loss: 0.9486 - acc: 0.4060 - val_loss: 0.9907 - val_acc: 0.3778\n",
      "Epoch 196/1200\n",
      "133/133 [==============================] - 0s 107us/step - loss: 0.9472 - acc: 0.4060 - val_loss: 0.9896 - val_acc: 0.3778\n",
      "Epoch 197/1200\n",
      "133/133 [==============================] - 0s 125us/step - loss: 0.9460 - acc: 0.4060 - val_loss: 0.9885 - val_acc: 0.3778\n",
      "Epoch 198/1200\n",
      "133/133 [==============================] - 0s 177us/step - loss: 0.9447 - acc: 0.4060 - val_loss: 0.9874 - val_acc: 0.3778\n",
      "Epoch 199/1200\n",
      "133/133 [==============================] - 0s 147us/step - loss: 0.9435 - acc: 0.4060 - val_loss: 0.9864 - val_acc: 0.3778\n",
      "Epoch 200/1200\n",
      "133/133 [==============================] - 0s 135us/step - loss: 0.9422 - acc: 0.4060 - val_loss: 0.9853 - val_acc: 0.3778\n",
      "Epoch 201/1200\n",
      "133/133 [==============================] - 0s 109us/step - loss: 0.9409 - acc: 0.4060 - val_loss: 0.9842 - val_acc: 0.3778\n",
      "Epoch 202/1200\n",
      "133/133 [==============================] - 0s 116us/step - loss: 0.9397 - acc: 0.4060 - val_loss: 0.9831 - val_acc: 0.3778\n",
      "Epoch 203/1200\n",
      "133/133 [==============================] - 0s 147us/step - loss: 0.9386 - acc: 0.4060 - val_loss: 0.9823 - val_acc: 0.3778\n",
      "Epoch 204/1200\n",
      "133/133 [==============================] - 0s 256us/step - loss: 0.9373 - acc: 0.4060 - val_loss: 0.9813 - val_acc: 0.3778\n",
      "Epoch 205/1200\n",
      "133/133 [==============================] - 0s 214us/step - loss: 0.9362 - acc: 0.4060 - val_loss: 0.9804 - val_acc: 0.3778\n",
      "Epoch 206/1200\n",
      "133/133 [==============================] - 0s 171us/step - loss: 0.9348 - acc: 0.4060 - val_loss: 0.9794 - val_acc: 0.4000\n",
      "Epoch 207/1200\n",
      "133/133 [==============================] - 0s 311us/step - loss: 0.9335 - acc: 0.4887 - val_loss: 0.9783 - val_acc: 0.4000\n",
      "Epoch 208/1200\n",
      "133/133 [==============================] - 0s 286us/step - loss: 0.9322 - acc: 0.4962 - val_loss: 0.9772 - val_acc: 0.4000\n",
      "Epoch 209/1200\n",
      "133/133 [==============================] - 0s 270us/step - loss: 0.9308 - acc: 0.4962 - val_loss: 0.9760 - val_acc: 0.4222\n",
      "Epoch 210/1200\n",
      "133/133 [==============================] - 0s 224us/step - loss: 0.9294 - acc: 0.4962 - val_loss: 0.9748 - val_acc: 0.4222\n",
      "Epoch 211/1200\n",
      "133/133 [==============================] - 0s 274us/step - loss: 0.9278 - acc: 0.5113 - val_loss: 0.9735 - val_acc: 0.4444\n",
      "Epoch 212/1200\n",
      "133/133 [==============================] - 0s 192us/step - loss: 0.9266 - acc: 0.5038 - val_loss: 0.9724 - val_acc: 0.4444\n",
      "Epoch 213/1200\n",
      "133/133 [==============================] - 0s 270us/step - loss: 0.9251 - acc: 0.5188 - val_loss: 0.9712 - val_acc: 0.4444\n",
      "Epoch 214/1200\n",
      "133/133 [==============================] - 0s 201us/step - loss: 0.9239 - acc: 0.5188 - val_loss: 0.9701 - val_acc: 0.4444\n",
      "Epoch 215/1200\n",
      "133/133 [==============================] - 0s 137us/step - loss: 0.9225 - acc: 0.5188 - val_loss: 0.9690 - val_acc: 0.4667\n",
      "Epoch 216/1200\n",
      "133/133 [==============================] - 0s 110us/step - loss: 0.9211 - acc: 0.5188 - val_loss: 0.9678 - val_acc: 0.4667\n",
      "Epoch 217/1200\n",
      "133/133 [==============================] - 0s 119us/step - loss: 0.9199 - acc: 0.5188 - val_loss: 0.9666 - val_acc: 0.4667\n",
      "Epoch 218/1200\n",
      "133/133 [==============================] - 0s 118us/step - loss: 0.9187 - acc: 0.5188 - val_loss: 0.9656 - val_acc: 0.4667\n",
      "Epoch 219/1200\n",
      "133/133 [==============================] - 0s 112us/step - loss: 0.9172 - acc: 0.5263 - val_loss: 0.9646 - val_acc: 0.4667\n",
      "Epoch 220/1200\n",
      "133/133 [==============================] - 0s 123us/step - loss: 0.9160 - acc: 0.5338 - val_loss: 0.9635 - val_acc: 0.4667\n",
      "Epoch 221/1200\n",
      "133/133 [==============================] - 0s 114us/step - loss: 0.9147 - acc: 0.5263 - val_loss: 0.9624 - val_acc: 0.4889\n",
      "Epoch 222/1200\n",
      "133/133 [==============================] - 0s 140us/step - loss: 0.9135 - acc: 0.5338 - val_loss: 0.9613 - val_acc: 0.4889\n",
      "Epoch 223/1200\n",
      "133/133 [==============================] - 0s 163us/step - loss: 0.9124 - acc: 0.5414 - val_loss: 0.9603 - val_acc: 0.4889\n",
      "Epoch 224/1200\n",
      "133/133 [==============================] - 0s 145us/step - loss: 0.9110 - acc: 0.5489 - val_loss: 0.9594 - val_acc: 0.5111\n",
      "Epoch 225/1200\n",
      "133/133 [==============================] - 0s 110us/step - loss: 0.9100 - acc: 0.5414 - val_loss: 0.9584 - val_acc: 0.5111\n",
      "Epoch 226/1200\n",
      "133/133 [==============================] - 0s 118us/step - loss: 0.9086 - acc: 0.5414 - val_loss: 0.9573 - val_acc: 0.5111\n",
      "Epoch 227/1200\n",
      "133/133 [==============================] - 0s 144us/step - loss: 0.9075 - acc: 0.5414 - val_loss: 0.9563 - val_acc: 0.5556\n",
      "Epoch 228/1200\n",
      "133/133 [==============================] - 0s 113us/step - loss: 0.9063 - acc: 0.5489 - val_loss: 0.9552 - val_acc: 0.5556\n",
      "Epoch 229/1200\n",
      "133/133 [==============================] - 0s 213us/step - loss: 0.9051 - acc: 0.5489 - val_loss: 0.9542 - val_acc: 0.5556\n",
      "Epoch 230/1200\n",
      "133/133 [==============================] - 0s 257us/step - loss: 0.9040 - acc: 0.5489 - val_loss: 0.9532 - val_acc: 0.5556\n",
      "Epoch 231/1200\n",
      "133/133 [==============================] - 0s 177us/step - loss: 0.9028 - acc: 0.5489 - val_loss: 0.9521 - val_acc: 0.5778\n",
      "Epoch 232/1200\n",
      "133/133 [==============================] - 0s 243us/step - loss: 0.9015 - acc: 0.5489 - val_loss: 0.9511 - val_acc: 0.5778\n",
      "Epoch 233/1200\n",
      "133/133 [==============================] - 0s 245us/step - loss: 0.9004 - acc: 0.5489 - val_loss: 0.9500 - val_acc: 0.5778\n",
      "Epoch 234/1200\n",
      "133/133 [==============================] - 0s 185us/step - loss: 0.8992 - acc: 0.5714 - val_loss: 0.9489 - val_acc: 0.5778\n",
      "Epoch 235/1200\n",
      "133/133 [==============================] - 0s 153us/step - loss: 0.8978 - acc: 0.5714 - val_loss: 0.9478 - val_acc: 0.5778\n",
      "Epoch 236/1200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133/133 [==============================] - 0s 149us/step - loss: 0.8967 - acc: 0.5714 - val_loss: 0.9467 - val_acc: 0.5778\n",
      "Epoch 237/1200\n",
      "133/133 [==============================] - 0s 181us/step - loss: 0.8955 - acc: 0.5789 - val_loss: 0.9457 - val_acc: 0.5778\n",
      "Epoch 238/1200\n",
      "133/133 [==============================] - 0s 191us/step - loss: 0.8943 - acc: 0.5789 - val_loss: 0.9447 - val_acc: 0.5778\n",
      "Epoch 239/1200\n",
      "133/133 [==============================] - 0s 148us/step - loss: 0.8932 - acc: 0.5714 - val_loss: 0.9437 - val_acc: 0.5778\n",
      "Epoch 240/1200\n",
      "133/133 [==============================] - 0s 113us/step - loss: 0.8920 - acc: 0.5789 - val_loss: 0.9427 - val_acc: 0.5778\n",
      "Epoch 241/1200\n",
      "133/133 [==============================] - 0s 167us/step - loss: 0.8912 - acc: 0.5789 - val_loss: 0.9418 - val_acc: 0.5778\n",
      "Epoch 242/1200\n",
      "133/133 [==============================] - 0s 142us/step - loss: 0.8899 - acc: 0.5789 - val_loss: 0.9408 - val_acc: 0.5778\n",
      "Epoch 243/1200\n",
      "133/133 [==============================] - 0s 167us/step - loss: 0.8888 - acc: 0.5789 - val_loss: 0.9398 - val_acc: 0.5778\n",
      "Epoch 244/1200\n",
      "133/133 [==============================] - 0s 153us/step - loss: 0.8878 - acc: 0.5789 - val_loss: 0.9388 - val_acc: 0.5778\n",
      "Epoch 245/1200\n",
      "133/133 [==============================] - 0s 189us/step - loss: 0.8867 - acc: 0.5789 - val_loss: 0.9378 - val_acc: 0.5778\n",
      "Epoch 246/1200\n",
      "133/133 [==============================] - 0s 178us/step - loss: 0.8856 - acc: 0.5789 - val_loss: 0.9367 - val_acc: 0.5778\n",
      "Epoch 247/1200\n",
      "133/133 [==============================] - 0s 136us/step - loss: 0.8845 - acc: 0.5789 - val_loss: 0.9358 - val_acc: 0.5778\n",
      "Epoch 248/1200\n",
      "133/133 [==============================] - 0s 139us/step - loss: 0.8835 - acc: 0.5789 - val_loss: 0.9349 - val_acc: 0.5778\n",
      "Epoch 249/1200\n",
      "133/133 [==============================] - 0s 290us/step - loss: 0.8825 - acc: 0.5789 - val_loss: 0.9339 - val_acc: 0.5778\n",
      "Epoch 250/1200\n",
      "133/133 [==============================] - 0s 149us/step - loss: 0.8813 - acc: 0.5789 - val_loss: 0.9330 - val_acc: 0.5778\n",
      "Epoch 251/1200\n",
      "133/133 [==============================] - 0s 158us/step - loss: 0.8803 - acc: 0.5789 - val_loss: 0.9320 - val_acc: 0.6000\n",
      "Epoch 252/1200\n",
      "133/133 [==============================] - 0s 172us/step - loss: 0.8792 - acc: 0.5789 - val_loss: 0.9309 - val_acc: 0.6000\n",
      "Epoch 253/1200\n",
      "133/133 [==============================] - 0s 161us/step - loss: 0.8782 - acc: 0.5789 - val_loss: 0.9299 - val_acc: 0.6000\n",
      "Epoch 254/1200\n",
      "133/133 [==============================] - 0s 251us/step - loss: 0.8771 - acc: 0.5789 - val_loss: 0.9290 - val_acc: 0.6000\n",
      "Epoch 255/1200\n",
      "133/133 [==============================] - 0s 237us/step - loss: 0.8760 - acc: 0.5789 - val_loss: 0.9281 - val_acc: 0.6000\n",
      "Epoch 256/1200\n",
      "133/133 [==============================] - 0s 219us/step - loss: 0.8749 - acc: 0.5789 - val_loss: 0.9271 - val_acc: 0.6000\n",
      "Epoch 257/1200\n",
      "133/133 [==============================] - 0s 199us/step - loss: 0.8740 - acc: 0.5865 - val_loss: 0.9262 - val_acc: 0.6000\n",
      "Epoch 258/1200\n",
      "133/133 [==============================] - 0s 198us/step - loss: 0.8729 - acc: 0.5789 - val_loss: 0.9252 - val_acc: 0.6000\n",
      "Epoch 259/1200\n",
      "133/133 [==============================] - 0s 121us/step - loss: 0.8718 - acc: 0.6015 - val_loss: 0.9243 - val_acc: 0.6000\n",
      "Epoch 260/1200\n",
      "133/133 [==============================] - 0s 145us/step - loss: 0.8707 - acc: 0.6165 - val_loss: 0.9234 - val_acc: 0.6000\n",
      "Epoch 261/1200\n",
      "133/133 [==============================] - 0s 99us/step - loss: 0.8698 - acc: 0.6165 - val_loss: 0.9225 - val_acc: 0.6000\n",
      "Epoch 262/1200\n",
      "133/133 [==============================] - 0s 140us/step - loss: 0.8687 - acc: 0.6165 - val_loss: 0.9216 - val_acc: 0.6000\n",
      "Epoch 263/1200\n",
      "133/133 [==============================] - 0s 122us/step - loss: 0.8677 - acc: 0.6165 - val_loss: 0.9208 - val_acc: 0.6000\n",
      "Epoch 264/1200\n",
      "133/133 [==============================] - 0s 128us/step - loss: 0.8665 - acc: 0.6165 - val_loss: 0.9197 - val_acc: 0.6000\n",
      "Epoch 265/1200\n",
      "133/133 [==============================] - 0s 149us/step - loss: 0.8654 - acc: 0.6165 - val_loss: 0.9187 - val_acc: 0.6000\n",
      "Epoch 266/1200\n",
      "133/133 [==============================] - 0s 173us/step - loss: 0.8646 - acc: 0.6165 - val_loss: 0.9179 - val_acc: 0.6000\n",
      "Epoch 267/1200\n",
      "133/133 [==============================] - 0s 180us/step - loss: 0.8633 - acc: 0.6090 - val_loss: 0.9171 - val_acc: 0.6000\n",
      "Epoch 268/1200\n",
      "133/133 [==============================] - 0s 277us/step - loss: 0.8624 - acc: 0.6165 - val_loss: 0.9162 - val_acc: 0.6000\n",
      "Epoch 269/1200\n",
      "133/133 [==============================] - 0s 199us/step - loss: 0.8614 - acc: 0.6316 - val_loss: 0.9155 - val_acc: 0.6000\n",
      "Epoch 270/1200\n",
      "133/133 [==============================] - 0s 242us/step - loss: 0.8604 - acc: 0.6316 - val_loss: 0.9146 - val_acc: 0.6000\n",
      "Epoch 271/1200\n",
      "133/133 [==============================] - 0s 190us/step - loss: 0.8593 - acc: 0.6316 - val_loss: 0.9138 - val_acc: 0.6000\n",
      "Epoch 272/1200\n",
      "133/133 [==============================] - 0s 159us/step - loss: 0.8583 - acc: 0.6241 - val_loss: 0.9128 - val_acc: 0.6000\n",
      "Epoch 273/1200\n",
      "133/133 [==============================] - 0s 156us/step - loss: 0.8574 - acc: 0.6316 - val_loss: 0.9118 - val_acc: 0.6000\n",
      "Epoch 274/1200\n",
      "133/133 [==============================] - 0s 107us/step - loss: 0.8564 - acc: 0.6316 - val_loss: 0.9110 - val_acc: 0.6000\n",
      "Epoch 275/1200\n",
      "133/133 [==============================] - 0s 85us/step - loss: 0.8556 - acc: 0.6391 - val_loss: 0.9103 - val_acc: 0.6000\n",
      "Epoch 276/1200\n",
      "133/133 [==============================] - 0s 100us/step - loss: 0.8544 - acc: 0.6316 - val_loss: 0.9095 - val_acc: 0.6000\n",
      "Epoch 277/1200\n",
      "133/133 [==============================] - 0s 118us/step - loss: 0.8535 - acc: 0.6316 - val_loss: 0.9087 - val_acc: 0.6000\n",
      "Epoch 278/1200\n",
      "133/133 [==============================] - 0s 105us/step - loss: 0.8527 - acc: 0.6241 - val_loss: 0.9080 - val_acc: 0.6000\n",
      "Epoch 279/1200\n",
      "133/133 [==============================] - 0s 130us/step - loss: 0.8516 - acc: 0.6391 - val_loss: 0.9073 - val_acc: 0.6000\n",
      "Epoch 280/1200\n",
      "133/133 [==============================] - 0s 145us/step - loss: 0.8509 - acc: 0.6316 - val_loss: 0.9066 - val_acc: 0.6000\n",
      "Epoch 281/1200\n",
      "133/133 [==============================] - 0s 172us/step - loss: 0.8499 - acc: 0.6316 - val_loss: 0.9058 - val_acc: 0.6000\n",
      "Epoch 282/1200\n",
      "133/133 [==============================] - 0s 191us/step - loss: 0.8490 - acc: 0.6391 - val_loss: 0.9051 - val_acc: 0.6000\n",
      "Epoch 283/1200\n",
      "133/133 [==============================] - 0s 166us/step - loss: 0.8478 - acc: 0.6316 - val_loss: 0.9044 - val_acc: 0.6000\n",
      "Epoch 284/1200\n",
      "133/133 [==============================] - 0s 116us/step - loss: 0.8469 - acc: 0.6391 - val_loss: 0.9035 - val_acc: 0.6000\n",
      "Epoch 285/1200\n",
      "133/133 [==============================] - 0s 107us/step - loss: 0.8459 - acc: 0.6391 - val_loss: 0.9026 - val_acc: 0.6000\n",
      "Epoch 286/1200\n",
      "133/133 [==============================] - 0s 97us/step - loss: 0.8451 - acc: 0.6391 - val_loss: 0.9020 - val_acc: 0.6000\n",
      "Epoch 287/1200\n",
      "133/133 [==============================] - 0s 94us/step - loss: 0.8442 - acc: 0.6391 - val_loss: 0.9012 - val_acc: 0.6000\n",
      "Epoch 288/1200\n",
      "133/133 [==============================] - 0s 98us/step - loss: 0.8431 - acc: 0.6391 - val_loss: 0.9005 - val_acc: 0.6000\n",
      "Epoch 289/1200\n",
      "133/133 [==============================] - 0s 113us/step - loss: 0.8421 - acc: 0.6391 - val_loss: 0.8996 - val_acc: 0.6000\n",
      "Epoch 290/1200\n",
      "133/133 [==============================] - 0s 90us/step - loss: 0.8412 - acc: 0.6466 - val_loss: 0.8987 - val_acc: 0.6000\n",
      "Epoch 291/1200\n",
      "133/133 [==============================] - 0s 116us/step - loss: 0.8402 - acc: 0.6466 - val_loss: 0.8979 - val_acc: 0.6000\n",
      "Epoch 292/1200\n",
      "133/133 [==============================] - 0s 110us/step - loss: 0.8395 - acc: 0.6391 - val_loss: 0.8972 - val_acc: 0.6000\n",
      "Epoch 293/1200\n",
      "133/133 [==============================] - 0s 118us/step - loss: 0.8386 - acc: 0.6466 - val_loss: 0.8965 - val_acc: 0.6000\n",
      "Epoch 294/1200\n",
      "133/133 [==============================] - 0s 99us/step - loss: 0.8375 - acc: 0.6391 - val_loss: 0.8957 - val_acc: 0.6000\n",
      "Epoch 295/1200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133/133 [==============================] - 0s 115us/step - loss: 0.8367 - acc: 0.6466 - val_loss: 0.8950 - val_acc: 0.6000\n",
      "Epoch 296/1200\n",
      "133/133 [==============================] - 0s 119us/step - loss: 0.8359 - acc: 0.6466 - val_loss: 0.8943 - val_acc: 0.6000\n",
      "Epoch 297/1200\n",
      "133/133 [==============================] - 0s 152us/step - loss: 0.8349 - acc: 0.6466 - val_loss: 0.8936 - val_acc: 0.6000\n",
      "Epoch 298/1200\n",
      "133/133 [==============================] - 0s 118us/step - loss: 0.8341 - acc: 0.6466 - val_loss: 0.8929 - val_acc: 0.6000\n",
      "Epoch 299/1200\n",
      "133/133 [==============================] - 0s 104us/step - loss: 0.8332 - acc: 0.6466 - val_loss: 0.8921 - val_acc: 0.6000\n",
      "Epoch 300/1200\n",
      "133/133 [==============================] - 0s 94us/step - loss: 0.8322 - acc: 0.6466 - val_loss: 0.8913 - val_acc: 0.6000\n",
      "Epoch 301/1200\n",
      "133/133 [==============================] - 0s 98us/step - loss: 0.8312 - acc: 0.6466 - val_loss: 0.8903 - val_acc: 0.6000\n",
      "Epoch 302/1200\n",
      "133/133 [==============================] - 0s 92us/step - loss: 0.8304 - acc: 0.6466 - val_loss: 0.8896 - val_acc: 0.6000\n",
      "Epoch 303/1200\n",
      "133/133 [==============================] - 0s 113us/step - loss: 0.8293 - acc: 0.6466 - val_loss: 0.8885 - val_acc: 0.6000\n",
      "Epoch 304/1200\n",
      "133/133 [==============================] - 0s 102us/step - loss: 0.8289 - acc: 0.6466 - val_loss: 0.8878 - val_acc: 0.6000\n",
      "Epoch 305/1200\n",
      "133/133 [==============================] - 0s 137us/step - loss: 0.8278 - acc: 0.6466 - val_loss: 0.8872 - val_acc: 0.6000\n",
      "Epoch 306/1200\n",
      "133/133 [==============================] - 0s 144us/step - loss: 0.8269 - acc: 0.6466 - val_loss: 0.8864 - val_acc: 0.6000\n",
      "Epoch 307/1200\n",
      "133/133 [==============================] - 0s 121us/step - loss: 0.8261 - acc: 0.6466 - val_loss: 0.8855 - val_acc: 0.6000\n",
      "Epoch 308/1200\n",
      "133/133 [==============================] - 0s 118us/step - loss: 0.8254 - acc: 0.6466 - val_loss: 0.8848 - val_acc: 0.6000\n",
      "Epoch 309/1200\n",
      "133/133 [==============================] - 0s 116us/step - loss: 0.8243 - acc: 0.6466 - val_loss: 0.8841 - val_acc: 0.6000\n",
      "Epoch 310/1200\n",
      "133/133 [==============================] - 0s 136us/step - loss: 0.8237 - acc: 0.6466 - val_loss: 0.8834 - val_acc: 0.6000\n",
      "Epoch 311/1200\n",
      "133/133 [==============================] - 0s 123us/step - loss: 0.8230 - acc: 0.6466 - val_loss: 0.8827 - val_acc: 0.6000\n",
      "Epoch 312/1200\n",
      "133/133 [==============================] - 0s 175us/step - loss: 0.8218 - acc: 0.6466 - val_loss: 0.8819 - val_acc: 0.6000\n",
      "Epoch 313/1200\n",
      "133/133 [==============================] - 0s 333us/step - loss: 0.8212 - acc: 0.6466 - val_loss: 0.8812 - val_acc: 0.6000\n",
      "Epoch 314/1200\n",
      "133/133 [==============================] - 0s 259us/step - loss: 0.8203 - acc: 0.6466 - val_loss: 0.8806 - val_acc: 0.6000\n",
      "Epoch 315/1200\n",
      "133/133 [==============================] - 0s 205us/step - loss: 0.8193 - acc: 0.6466 - val_loss: 0.8798 - val_acc: 0.6000\n",
      "Epoch 316/1200\n",
      "133/133 [==============================] - 0s 313us/step - loss: 0.8186 - acc: 0.6466 - val_loss: 0.8789 - val_acc: 0.6000\n",
      "Epoch 317/1200\n",
      "133/133 [==============================] - 0s 206us/step - loss: 0.8176 - acc: 0.6466 - val_loss: 0.8779 - val_acc: 0.6000\n",
      "Epoch 318/1200\n",
      "133/133 [==============================] - 0s 164us/step - loss: 0.8167 - acc: 0.6466 - val_loss: 0.8769 - val_acc: 0.6000\n",
      "Epoch 319/1200\n",
      "133/133 [==============================] - 0s 119us/step - loss: 0.8159 - acc: 0.6466 - val_loss: 0.8761 - val_acc: 0.6000\n",
      "Epoch 320/1200\n",
      "133/133 [==============================] - 0s 127us/step - loss: 0.8149 - acc: 0.6466 - val_loss: 0.8753 - val_acc: 0.6000\n",
      "Epoch 321/1200\n",
      "133/133 [==============================] - 0s 145us/step - loss: 0.8141 - acc: 0.6466 - val_loss: 0.8744 - val_acc: 0.6000\n",
      "Epoch 322/1200\n",
      "133/133 [==============================] - 0s 143us/step - loss: 0.8132 - acc: 0.6466 - val_loss: 0.8735 - val_acc: 0.6000\n",
      "Epoch 323/1200\n",
      "133/133 [==============================] - 0s 173us/step - loss: 0.8125 - acc: 0.6466 - val_loss: 0.8728 - val_acc: 0.6000\n",
      "Epoch 324/1200\n",
      "133/133 [==============================] - 0s 166us/step - loss: 0.8114 - acc: 0.6466 - val_loss: 0.8718 - val_acc: 0.6000\n",
      "Epoch 325/1200\n",
      "133/133 [==============================] - 0s 141us/step - loss: 0.8105 - acc: 0.6466 - val_loss: 0.8707 - val_acc: 0.6000\n",
      "Epoch 326/1200\n",
      "133/133 [==============================] - 0s 152us/step - loss: 0.8096 - acc: 0.6466 - val_loss: 0.8698 - val_acc: 0.6000\n",
      "Epoch 327/1200\n",
      "133/133 [==============================] - 0s 149us/step - loss: 0.8089 - acc: 0.6617 - val_loss: 0.8690 - val_acc: 0.6000\n",
      "Epoch 328/1200\n",
      "133/133 [==============================] - 0s 130us/step - loss: 0.8079 - acc: 0.6466 - val_loss: 0.8682 - val_acc: 0.6000\n",
      "Epoch 329/1200\n",
      "133/133 [==============================] - 0s 115us/step - loss: 0.8071 - acc: 0.6466 - val_loss: 0.8672 - val_acc: 0.6000\n",
      "Epoch 330/1200\n",
      "133/133 [==============================] - 0s 113us/step - loss: 0.8065 - acc: 0.6541 - val_loss: 0.8663 - val_acc: 0.6000\n",
      "Epoch 331/1200\n",
      "133/133 [==============================] - 0s 124us/step - loss: 0.8053 - acc: 0.6617 - val_loss: 0.8653 - val_acc: 0.6000\n",
      "Epoch 332/1200\n",
      "133/133 [==============================] - 0s 119us/step - loss: 0.8045 - acc: 0.6617 - val_loss: 0.8643 - val_acc: 0.6000\n",
      "Epoch 333/1200\n",
      "133/133 [==============================] - 0s 115us/step - loss: 0.8034 - acc: 0.6541 - val_loss: 0.8632 - val_acc: 0.6000\n",
      "Epoch 334/1200\n",
      "133/133 [==============================] - 0s 116us/step - loss: 0.8024 - acc: 0.6617 - val_loss: 0.8623 - val_acc: 0.6000\n",
      "Epoch 335/1200\n",
      "133/133 [==============================] - 0s 109us/step - loss: 0.8014 - acc: 0.6617 - val_loss: 0.8615 - val_acc: 0.6000\n",
      "Epoch 336/1200\n",
      "133/133 [==============================] - 0s 116us/step - loss: 0.8004 - acc: 0.6617 - val_loss: 0.8604 - val_acc: 0.6000\n",
      "Epoch 337/1200\n",
      "133/133 [==============================] - 0s 121us/step - loss: 0.7993 - acc: 0.6692 - val_loss: 0.8595 - val_acc: 0.6000\n",
      "Epoch 338/1200\n",
      "133/133 [==============================] - 0s 131us/step - loss: 0.7984 - acc: 0.6692 - val_loss: 0.8584 - val_acc: 0.6000\n",
      "Epoch 339/1200\n",
      "133/133 [==============================] - 0s 111us/step - loss: 0.7974 - acc: 0.6617 - val_loss: 0.8573 - val_acc: 0.6000\n",
      "Epoch 340/1200\n",
      "133/133 [==============================] - 0s 145us/step - loss: 0.7965 - acc: 0.6617 - val_loss: 0.8563 - val_acc: 0.6000\n",
      "Epoch 341/1200\n",
      "133/133 [==============================] - 0s 107us/step - loss: 0.7951 - acc: 0.6617 - val_loss: 0.8551 - val_acc: 0.6222\n",
      "Epoch 342/1200\n",
      "133/133 [==============================] - 0s 102us/step - loss: 0.7941 - acc: 0.6617 - val_loss: 0.8538 - val_acc: 0.6222\n",
      "Epoch 343/1200\n",
      "133/133 [==============================] - 0s 97us/step - loss: 0.7931 - acc: 0.6692 - val_loss: 0.8528 - val_acc: 0.6222\n",
      "Epoch 344/1200\n",
      "133/133 [==============================] - 0s 125us/step - loss: 0.7919 - acc: 0.6692 - val_loss: 0.8518 - val_acc: 0.6222\n",
      "Epoch 345/1200\n",
      "133/133 [==============================] - 0s 107us/step - loss: 0.7910 - acc: 0.6692 - val_loss: 0.8506 - val_acc: 0.6222\n",
      "Epoch 346/1200\n",
      "133/133 [==============================] - 0s 110us/step - loss: 0.7899 - acc: 0.6692 - val_loss: 0.8494 - val_acc: 0.6222\n",
      "Epoch 347/1200\n",
      "133/133 [==============================] - 0s 102us/step - loss: 0.7887 - acc: 0.6692 - val_loss: 0.8481 - val_acc: 0.6222\n",
      "Epoch 348/1200\n",
      "133/133 [==============================] - 0s 121us/step - loss: 0.7876 - acc: 0.6692 - val_loss: 0.8469 - val_acc: 0.6222\n",
      "Epoch 349/1200\n",
      "133/133 [==============================] - 0s 141us/step - loss: 0.7864 - acc: 0.6692 - val_loss: 0.8457 - val_acc: 0.6222\n",
      "Epoch 350/1200\n",
      "133/133 [==============================] - 0s 128us/step - loss: 0.7853 - acc: 0.6692 - val_loss: 0.8444 - val_acc: 0.6222\n",
      "Epoch 351/1200\n",
      "133/133 [==============================] - 0s 104us/step - loss: 0.7840 - acc: 0.6692 - val_loss: 0.8430 - val_acc: 0.6444\n",
      "Epoch 352/1200\n",
      "133/133 [==============================] - 0s 112us/step - loss: 0.7828 - acc: 0.6692 - val_loss: 0.8417 - val_acc: 0.6444\n",
      "Epoch 353/1200\n",
      "133/133 [==============================] - 0s 97us/step - loss: 0.7816 - acc: 0.6692 - val_loss: 0.8401 - val_acc: 0.6444\n",
      "Epoch 354/1200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133/133 [==============================] - 0s 101us/step - loss: 0.7806 - acc: 0.6692 - val_loss: 0.8390 - val_acc: 0.6444\n",
      "Epoch 355/1200\n",
      "133/133 [==============================] - 0s 97us/step - loss: 0.7790 - acc: 0.6692 - val_loss: 0.8376 - val_acc: 0.6444\n",
      "Epoch 356/1200\n",
      "133/133 [==============================] - 0s 131us/step - loss: 0.7781 - acc: 0.6767 - val_loss: 0.8365 - val_acc: 0.6444\n",
      "Epoch 357/1200\n",
      "133/133 [==============================] - 0s 102us/step - loss: 0.7767 - acc: 0.6692 - val_loss: 0.8351 - val_acc: 0.6444\n",
      "Epoch 358/1200\n",
      "133/133 [==============================] - 0s 104us/step - loss: 0.7755 - acc: 0.6767 - val_loss: 0.8337 - val_acc: 0.6444\n",
      "Epoch 359/1200\n",
      "133/133 [==============================] - 0s 100us/step - loss: 0.7742 - acc: 0.6767 - val_loss: 0.8324 - val_acc: 0.6444\n",
      "Epoch 360/1200\n",
      "133/133 [==============================] - 0s 100us/step - loss: 0.7732 - acc: 0.6767 - val_loss: 0.8311 - val_acc: 0.6444\n",
      "Epoch 361/1200\n",
      "133/133 [==============================] - 0s 114us/step - loss: 0.7717 - acc: 0.6767 - val_loss: 0.8299 - val_acc: 0.6444\n",
      "Epoch 362/1200\n",
      "133/133 [==============================] - 0s 105us/step - loss: 0.7707 - acc: 0.6842 - val_loss: 0.8285 - val_acc: 0.6444\n",
      "Epoch 363/1200\n",
      "133/133 [==============================] - 0s 117us/step - loss: 0.7692 - acc: 0.6767 - val_loss: 0.8271 - val_acc: 0.6444\n",
      "Epoch 364/1200\n",
      "133/133 [==============================] - 0s 111us/step - loss: 0.7678 - acc: 0.6842 - val_loss: 0.8257 - val_acc: 0.6444\n",
      "Epoch 365/1200\n",
      "133/133 [==============================] - 0s 106us/step - loss: 0.7665 - acc: 0.6842 - val_loss: 0.8241 - val_acc: 0.6667\n",
      "Epoch 366/1200\n",
      "133/133 [==============================] - 0s 109us/step - loss: 0.7652 - acc: 0.6842 - val_loss: 0.8227 - val_acc: 0.6667\n",
      "Epoch 367/1200\n",
      "133/133 [==============================] - 0s 108us/step - loss: 0.7642 - acc: 0.6842 - val_loss: 0.8215 - val_acc: 0.6667\n",
      "Epoch 368/1200\n",
      "133/133 [==============================] - 0s 108us/step - loss: 0.7627 - acc: 0.6842 - val_loss: 0.8202 - val_acc: 0.6667\n",
      "Epoch 369/1200\n",
      "133/133 [==============================] - 0s 137us/step - loss: 0.7614 - acc: 0.6842 - val_loss: 0.8187 - val_acc: 0.6667\n",
      "Epoch 370/1200\n",
      "133/133 [==============================] - 0s 125us/step - loss: 0.7601 - acc: 0.6842 - val_loss: 0.8172 - val_acc: 0.6667\n",
      "Epoch 371/1200\n",
      "133/133 [==============================] - 0s 115us/step - loss: 0.7588 - acc: 0.6842 - val_loss: 0.8159 - val_acc: 0.6667\n",
      "Epoch 372/1200\n",
      "133/133 [==============================] - 0s 106us/step - loss: 0.7574 - acc: 0.6842 - val_loss: 0.8145 - val_acc: 0.6667\n",
      "Epoch 373/1200\n",
      "133/133 [==============================] - 0s 138us/step - loss: 0.7559 - acc: 0.6842 - val_loss: 0.8132 - val_acc: 0.6667\n",
      "Epoch 374/1200\n",
      "133/133 [==============================] - 0s 133us/step - loss: 0.7545 - acc: 0.6842 - val_loss: 0.8118 - val_acc: 0.6667\n",
      "Epoch 375/1200\n",
      "133/133 [==============================] - 0s 600us/step - loss: 0.7531 - acc: 0.6842 - val_loss: 0.8104 - val_acc: 0.6667\n",
      "Epoch 376/1200\n",
      "133/133 [==============================] - 0s 218us/step - loss: 0.7518 - acc: 0.6842 - val_loss: 0.8090 - val_acc: 0.6667\n",
      "Epoch 377/1200\n",
      "133/133 [==============================] - 0s 136us/step - loss: 0.7501 - acc: 0.6917 - val_loss: 0.8074 - val_acc: 0.6667\n",
      "Epoch 378/1200\n",
      "133/133 [==============================] - 0s 124us/step - loss: 0.7488 - acc: 0.6917 - val_loss: 0.8059 - val_acc: 0.6889\n",
      "Epoch 379/1200\n",
      "133/133 [==============================] - 0s 115us/step - loss: 0.7472 - acc: 0.6917 - val_loss: 0.8044 - val_acc: 0.6889\n",
      "Epoch 380/1200\n",
      "133/133 [==============================] - 0s 137us/step - loss: 0.7459 - acc: 0.6917 - val_loss: 0.8029 - val_acc: 0.6889\n",
      "Epoch 381/1200\n",
      "133/133 [==============================] - 0s 109us/step - loss: 0.7446 - acc: 0.6917 - val_loss: 0.8014 - val_acc: 0.6889\n",
      "Epoch 382/1200\n",
      "133/133 [==============================] - 0s 120us/step - loss: 0.7432 - acc: 0.6917 - val_loss: 0.8001 - val_acc: 0.6889\n",
      "Epoch 383/1200\n",
      "133/133 [==============================] - 0s 107us/step - loss: 0.7419 - acc: 0.6917 - val_loss: 0.7989 - val_acc: 0.6889\n",
      "Epoch 384/1200\n",
      "133/133 [==============================] - 0s 100us/step - loss: 0.7406 - acc: 0.6917 - val_loss: 0.7976 - val_acc: 0.6889\n",
      "Epoch 385/1200\n",
      "133/133 [==============================] - 0s 100us/step - loss: 0.7390 - acc: 0.6917 - val_loss: 0.7962 - val_acc: 0.6889\n",
      "Epoch 386/1200\n",
      "133/133 [==============================] - 0s 107us/step - loss: 0.7378 - acc: 0.6917 - val_loss: 0.7947 - val_acc: 0.6889\n",
      "Epoch 387/1200\n",
      "133/133 [==============================] - 0s 106us/step - loss: 0.7362 - acc: 0.6917 - val_loss: 0.7932 - val_acc: 0.6889\n",
      "Epoch 388/1200\n",
      "133/133 [==============================] - 0s 107us/step - loss: 0.7349 - acc: 0.6917 - val_loss: 0.7918 - val_acc: 0.6889\n",
      "Epoch 389/1200\n",
      "133/133 [==============================] - 0s 107us/step - loss: 0.7333 - acc: 0.6917 - val_loss: 0.7904 - val_acc: 0.6889\n",
      "Epoch 390/1200\n",
      "133/133 [==============================] - 0s 112us/step - loss: 0.7320 - acc: 0.6917 - val_loss: 0.7889 - val_acc: 0.6889\n",
      "Epoch 391/1200\n",
      "133/133 [==============================] - 0s 106us/step - loss: 0.7307 - acc: 0.6917 - val_loss: 0.7874 - val_acc: 0.6889\n",
      "Epoch 392/1200\n",
      "133/133 [==============================] - 0s 113us/step - loss: 0.7294 - acc: 0.6917 - val_loss: 0.7861 - val_acc: 0.6889\n",
      "Epoch 393/1200\n",
      "133/133 [==============================] - 0s 120us/step - loss: 0.7277 - acc: 0.6917 - val_loss: 0.7846 - val_acc: 0.6889\n",
      "Epoch 394/1200\n",
      "133/133 [==============================] - 0s 132us/step - loss: 0.7266 - acc: 0.6917 - val_loss: 0.7833 - val_acc: 0.6889\n",
      "Epoch 395/1200\n",
      "133/133 [==============================] - 0s 94us/step - loss: 0.7249 - acc: 0.6917 - val_loss: 0.7816 - val_acc: 0.6889\n",
      "Epoch 396/1200\n",
      "133/133 [==============================] - 0s 113us/step - loss: 0.7236 - acc: 0.6917 - val_loss: 0.7802 - val_acc: 0.6889\n",
      "Epoch 397/1200\n",
      "133/133 [==============================] - 0s 108us/step - loss: 0.7221 - acc: 0.6917 - val_loss: 0.7787 - val_acc: 0.6889\n",
      "Epoch 398/1200\n",
      "133/133 [==============================] - 0s 97us/step - loss: 0.7207 - acc: 0.6992 - val_loss: 0.7769 - val_acc: 0.6889\n",
      "Epoch 399/1200\n",
      "133/133 [==============================] - 0s 114us/step - loss: 0.7192 - acc: 0.6992 - val_loss: 0.7752 - val_acc: 0.6889\n",
      "Epoch 400/1200\n",
      "133/133 [==============================] - 0s 116us/step - loss: 0.7178 - acc: 0.6992 - val_loss: 0.7737 - val_acc: 0.6889\n",
      "Epoch 401/1200\n",
      "133/133 [==============================] - 0s 107us/step - loss: 0.7162 - acc: 0.6992 - val_loss: 0.7723 - val_acc: 0.6889\n",
      "Epoch 402/1200\n",
      "133/133 [==============================] - 0s 128us/step - loss: 0.7153 - acc: 0.6992 - val_loss: 0.7710 - val_acc: 0.6889\n",
      "Epoch 403/1200\n",
      "133/133 [==============================] - 0s 127us/step - loss: 0.7134 - acc: 0.6992 - val_loss: 0.7695 - val_acc: 0.6889\n",
      "Epoch 404/1200\n",
      "133/133 [==============================] - 0s 135us/step - loss: 0.7121 - acc: 0.6992 - val_loss: 0.7681 - val_acc: 0.6889\n",
      "Epoch 405/1200\n",
      "133/133 [==============================] - 0s 137us/step - loss: 0.7104 - acc: 0.6992 - val_loss: 0.7666 - val_acc: 0.6889\n",
      "Epoch 406/1200\n",
      "133/133 [==============================] - 0s 124us/step - loss: 0.7092 - acc: 0.6992 - val_loss: 0.7651 - val_acc: 0.6889\n",
      "Epoch 407/1200\n",
      "133/133 [==============================] - 0s 180us/step - loss: 0.7077 - acc: 0.6992 - val_loss: 0.7635 - val_acc: 0.6889\n",
      "Epoch 408/1200\n",
      "133/133 [==============================] - 0s 97us/step - loss: 0.7063 - acc: 0.6992 - val_loss: 0.7621 - val_acc: 0.6889\n",
      "Epoch 409/1200\n",
      "133/133 [==============================] - 0s 104us/step - loss: 0.7045 - acc: 0.7068 - val_loss: 0.7607 - val_acc: 0.6889\n",
      "Epoch 410/1200\n",
      "133/133 [==============================] - 0s 105us/step - loss: 0.7033 - acc: 0.7068 - val_loss: 0.7591 - val_acc: 0.6889\n",
      "Epoch 411/1200\n",
      "133/133 [==============================] - 0s 161us/step - loss: 0.7014 - acc: 0.7068 - val_loss: 0.7576 - val_acc: 0.6889\n",
      "Epoch 412/1200\n",
      "133/133 [==============================] - 0s 124us/step - loss: 0.7010 - acc: 0.7068 - val_loss: 0.7564 - val_acc: 0.6889\n",
      "Epoch 413/1200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133/133 [==============================] - 0s 129us/step - loss: 0.6985 - acc: 0.7143 - val_loss: 0.7550 - val_acc: 0.6889\n",
      "Epoch 414/1200\n",
      "133/133 [==============================] - 0s 116us/step - loss: 0.6970 - acc: 0.7143 - val_loss: 0.7535 - val_acc: 0.6889\n",
      "Epoch 415/1200\n",
      "133/133 [==============================] - 0s 124us/step - loss: 0.6957 - acc: 0.7143 - val_loss: 0.7519 - val_acc: 0.6889\n",
      "Epoch 416/1200\n",
      "133/133 [==============================] - 0s 112us/step - loss: 0.6940 - acc: 0.7143 - val_loss: 0.7506 - val_acc: 0.6889\n",
      "Epoch 417/1200\n",
      "133/133 [==============================] - 0s 108us/step - loss: 0.6924 - acc: 0.7143 - val_loss: 0.7491 - val_acc: 0.6889\n",
      "Epoch 418/1200\n",
      "133/133 [==============================] - 0s 116us/step - loss: 0.6907 - acc: 0.7143 - val_loss: 0.7475 - val_acc: 0.6889\n",
      "Epoch 419/1200\n",
      "133/133 [==============================] - 0s 112us/step - loss: 0.6895 - acc: 0.7218 - val_loss: 0.7460 - val_acc: 0.6889\n",
      "Epoch 420/1200\n",
      "133/133 [==============================] - 0s 127us/step - loss: 0.6877 - acc: 0.7218 - val_loss: 0.7445 - val_acc: 0.6889\n",
      "Epoch 421/1200\n",
      "133/133 [==============================] - 0s 108us/step - loss: 0.6860 - acc: 0.7218 - val_loss: 0.7431 - val_acc: 0.6889\n",
      "Epoch 422/1200\n",
      "133/133 [==============================] - 0s 119us/step - loss: 0.6845 - acc: 0.7218 - val_loss: 0.7415 - val_acc: 0.6889\n",
      "Epoch 423/1200\n",
      "133/133 [==============================] - 0s 111us/step - loss: 0.6832 - acc: 0.7218 - val_loss: 0.7400 - val_acc: 0.6889\n",
      "Epoch 424/1200\n",
      "133/133 [==============================] - 0s 112us/step - loss: 0.6814 - acc: 0.7293 - val_loss: 0.7386 - val_acc: 0.6889\n",
      "Epoch 425/1200\n",
      "133/133 [==============================] - 0s 104us/step - loss: 0.6799 - acc: 0.7293 - val_loss: 0.7372 - val_acc: 0.6889\n",
      "Epoch 426/1200\n",
      "133/133 [==============================] - 0s 106us/step - loss: 0.6786 - acc: 0.7293 - val_loss: 0.7358 - val_acc: 0.6889\n",
      "Epoch 427/1200\n",
      "133/133 [==============================] - 0s 100us/step - loss: 0.6769 - acc: 0.7293 - val_loss: 0.7345 - val_acc: 0.6889\n",
      "Epoch 428/1200\n",
      "133/133 [==============================] - 0s 102us/step - loss: 0.6752 - acc: 0.7368 - val_loss: 0.7330 - val_acc: 0.7111\n",
      "Epoch 429/1200\n",
      "133/133 [==============================] - 0s 109us/step - loss: 0.6738 - acc: 0.7368 - val_loss: 0.7316 - val_acc: 0.7111\n",
      "Epoch 430/1200\n",
      "133/133 [==============================] - 0s 104us/step - loss: 0.6724 - acc: 0.7293 - val_loss: 0.7302 - val_acc: 0.7111\n",
      "Epoch 431/1200\n",
      "133/133 [==============================] - 0s 110us/step - loss: 0.6706 - acc: 0.7293 - val_loss: 0.7289 - val_acc: 0.7111\n",
      "Epoch 432/1200\n",
      "133/133 [==============================] - 0s 111us/step - loss: 0.6690 - acc: 0.7293 - val_loss: 0.7274 - val_acc: 0.7111\n",
      "Epoch 433/1200\n",
      "133/133 [==============================] - 0s 107us/step - loss: 0.6677 - acc: 0.7293 - val_loss: 0.7259 - val_acc: 0.7111\n",
      "Epoch 434/1200\n",
      "133/133 [==============================] - 0s 106us/step - loss: 0.6657 - acc: 0.7669 - val_loss: 0.7246 - val_acc: 0.7111\n",
      "Epoch 435/1200\n",
      "133/133 [==============================] - 0s 120us/step - loss: 0.6645 - acc: 0.7669 - val_loss: 0.7232 - val_acc: 0.7333\n",
      "Epoch 436/1200\n",
      "133/133 [==============================] - 0s 107us/step - loss: 0.6629 - acc: 0.7669 - val_loss: 0.7218 - val_acc: 0.7333\n",
      "Epoch 437/1200\n",
      "133/133 [==============================] - 0s 118us/step - loss: 0.6613 - acc: 0.7820 - val_loss: 0.7204 - val_acc: 0.7333\n",
      "Epoch 438/1200\n",
      "133/133 [==============================] - 0s 112us/step - loss: 0.6598 - acc: 0.7820 - val_loss: 0.7190 - val_acc: 0.7333\n",
      "Epoch 439/1200\n",
      "133/133 [==============================] - 0s 113us/step - loss: 0.6580 - acc: 0.7820 - val_loss: 0.7176 - val_acc: 0.7333\n",
      "Epoch 440/1200\n",
      "133/133 [==============================] - 0s 155us/step - loss: 0.6564 - acc: 0.7895 - val_loss: 0.7160 - val_acc: 0.7333\n",
      "Epoch 441/1200\n",
      "133/133 [==============================] - 0s 174us/step - loss: 0.6550 - acc: 0.7895 - val_loss: 0.7146 - val_acc: 0.7333\n",
      "Epoch 442/1200\n",
      "133/133 [==============================] - 0s 123us/step - loss: 0.6533 - acc: 0.7895 - val_loss: 0.7131 - val_acc: 0.7333\n",
      "Epoch 443/1200\n",
      "133/133 [==============================] - 0s 114us/step - loss: 0.6515 - acc: 0.7970 - val_loss: 0.7117 - val_acc: 0.7333\n",
      "Epoch 444/1200\n",
      "133/133 [==============================] - 0s 99us/step - loss: 0.6501 - acc: 0.7970 - val_loss: 0.7102 - val_acc: 0.7333\n",
      "Epoch 445/1200\n",
      "133/133 [==============================] - 0s 110us/step - loss: 0.6483 - acc: 0.8120 - val_loss: 0.7087 - val_acc: 0.7333\n",
      "Epoch 446/1200\n",
      "133/133 [==============================] - 0s 114us/step - loss: 0.6467 - acc: 0.8195 - val_loss: 0.7071 - val_acc: 0.7333\n",
      "Epoch 447/1200\n",
      "133/133 [==============================] - 0s 112us/step - loss: 0.6450 - acc: 0.8195 - val_loss: 0.7055 - val_acc: 0.7333\n",
      "Epoch 448/1200\n",
      "133/133 [==============================] - 0s 112us/step - loss: 0.6436 - acc: 0.8195 - val_loss: 0.7040 - val_acc: 0.7333\n",
      "Epoch 449/1200\n",
      "133/133 [==============================] - 0s 112us/step - loss: 0.6418 - acc: 0.8271 - val_loss: 0.7024 - val_acc: 0.7778\n",
      "Epoch 450/1200\n",
      "133/133 [==============================] - 0s 127us/step - loss: 0.6402 - acc: 0.8271 - val_loss: 0.7009 - val_acc: 0.7778\n",
      "Epoch 451/1200\n",
      "133/133 [==============================] - 0s 163us/step - loss: 0.6385 - acc: 0.8346 - val_loss: 0.6996 - val_acc: 0.8000\n",
      "Epoch 452/1200\n",
      "133/133 [==============================] - 0s 101us/step - loss: 0.6369 - acc: 0.8571 - val_loss: 0.6982 - val_acc: 0.8000\n",
      "Epoch 453/1200\n",
      "133/133 [==============================] - 0s 103us/step - loss: 0.6351 - acc: 0.8797 - val_loss: 0.6966 - val_acc: 0.8222\n",
      "Epoch 454/1200\n",
      "133/133 [==============================] - 0s 103us/step - loss: 0.6336 - acc: 0.8872 - val_loss: 0.6951 - val_acc: 0.8222\n",
      "Epoch 455/1200\n",
      "133/133 [==============================] - 0s 97us/step - loss: 0.6318 - acc: 0.8947 - val_loss: 0.6936 - val_acc: 0.8444\n",
      "Epoch 456/1200\n",
      "133/133 [==============================] - 0s 107us/step - loss: 0.6306 - acc: 0.9023 - val_loss: 0.6922 - val_acc: 0.8444\n",
      "Epoch 457/1200\n",
      "133/133 [==============================] - 0s 106us/step - loss: 0.6285 - acc: 0.9098 - val_loss: 0.6907 - val_acc: 0.8889\n",
      "Epoch 458/1200\n",
      "133/133 [==============================] - 0s 110us/step - loss: 0.6270 - acc: 0.9173 - val_loss: 0.6892 - val_acc: 0.9333\n",
      "Epoch 459/1200\n",
      "133/133 [==============================] - 0s 113us/step - loss: 0.6253 - acc: 0.9173 - val_loss: 0.6876 - val_acc: 0.9333\n",
      "Epoch 460/1200\n",
      "133/133 [==============================] - 0s 108us/step - loss: 0.6237 - acc: 0.9248 - val_loss: 0.6860 - val_acc: 0.9556\n",
      "Epoch 461/1200\n",
      "133/133 [==============================] - 0s 128us/step - loss: 0.6219 - acc: 0.9248 - val_loss: 0.6846 - val_acc: 0.9556\n",
      "Epoch 462/1200\n",
      "133/133 [==============================] - 0s 100us/step - loss: 0.6204 - acc: 0.9248 - val_loss: 0.6830 - val_acc: 0.9556\n",
      "Epoch 463/1200\n",
      "133/133 [==============================] - 0s 115us/step - loss: 0.6184 - acc: 0.9248 - val_loss: 0.6815 - val_acc: 0.9556\n",
      "Epoch 464/1200\n",
      "133/133 [==============================] - 0s 104us/step - loss: 0.6169 - acc: 0.9323 - val_loss: 0.6800 - val_acc: 0.9556\n",
      "Epoch 465/1200\n",
      "133/133 [==============================] - 0s 111us/step - loss: 0.6156 - acc: 0.9323 - val_loss: 0.6786 - val_acc: 0.9556\n",
      "Epoch 466/1200\n",
      "133/133 [==============================] - 0s 106us/step - loss: 0.6139 - acc: 0.9323 - val_loss: 0.6773 - val_acc: 0.9556\n",
      "Epoch 467/1200\n",
      "133/133 [==============================] - 0s 116us/step - loss: 0.6122 - acc: 0.9323 - val_loss: 0.6759 - val_acc: 0.9556\n",
      "Epoch 468/1200\n",
      "133/133 [==============================] - 0s 109us/step - loss: 0.6103 - acc: 0.9323 - val_loss: 0.6744 - val_acc: 0.9556\n",
      "Epoch 469/1200\n",
      "133/133 [==============================] - 0s 110us/step - loss: 0.6088 - acc: 0.9323 - val_loss: 0.6729 - val_acc: 0.9556\n",
      "Epoch 470/1200\n",
      "133/133 [==============================] - 0s 113us/step - loss: 0.6072 - acc: 0.9323 - val_loss: 0.6715 - val_acc: 0.9556\n",
      "Epoch 471/1200\n",
      "133/133 [==============================] - 0s 105us/step - loss: 0.6055 - acc: 0.9323 - val_loss: 0.6699 - val_acc: 0.9556\n",
      "Epoch 472/1200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133/133 [==============================] - 0s 121us/step - loss: 0.6042 - acc: 0.9323 - val_loss: 0.6685 - val_acc: 0.9556\n",
      "Epoch 473/1200\n",
      "133/133 [==============================] - 0s 111us/step - loss: 0.6023 - acc: 0.9323 - val_loss: 0.6670 - val_acc: 0.9556\n",
      "Epoch 474/1200\n",
      "133/133 [==============================] - 0s 107us/step - loss: 0.6005 - acc: 0.9323 - val_loss: 0.6655 - val_acc: 0.9556\n",
      "Epoch 475/1200\n",
      "133/133 [==============================] - 0s 185us/step - loss: 0.5989 - acc: 0.9323 - val_loss: 0.6640 - val_acc: 0.9556\n",
      "Epoch 476/1200\n",
      "133/133 [==============================] - 0s 118us/step - loss: 0.5974 - acc: 0.9398 - val_loss: 0.6624 - val_acc: 0.9556\n",
      "Epoch 477/1200\n",
      "133/133 [==============================] - 0s 114us/step - loss: 0.5955 - acc: 0.9398 - val_loss: 0.6609 - val_acc: 0.9556\n",
      "Epoch 478/1200\n",
      "133/133 [==============================] - 0s 107us/step - loss: 0.5937 - acc: 0.9398 - val_loss: 0.6594 - val_acc: 0.9556\n",
      "Epoch 479/1200\n",
      "133/133 [==============================] - 0s 120us/step - loss: 0.5919 - acc: 0.9398 - val_loss: 0.6578 - val_acc: 0.9556\n",
      "Epoch 480/1200\n",
      "133/133 [==============================] - 0s 109us/step - loss: 0.5906 - acc: 0.9474 - val_loss: 0.6564 - val_acc: 0.9556\n",
      "Epoch 481/1200\n",
      "133/133 [==============================] - 0s 121us/step - loss: 0.5887 - acc: 0.9474 - val_loss: 0.6549 - val_acc: 0.9556\n",
      "Epoch 482/1200\n",
      "133/133 [==============================] - 0s 120us/step - loss: 0.5869 - acc: 0.9474 - val_loss: 0.6534 - val_acc: 0.9556\n",
      "Epoch 483/1200\n",
      "133/133 [==============================] - 0s 121us/step - loss: 0.5855 - acc: 0.9474 - val_loss: 0.6519 - val_acc: 0.9556\n",
      "Epoch 484/1200\n",
      "133/133 [==============================] - 0s 120us/step - loss: 0.5835 - acc: 0.9549 - val_loss: 0.6504 - val_acc: 0.9556\n",
      "Epoch 485/1200\n",
      "133/133 [==============================] - 0s 116us/step - loss: 0.5818 - acc: 0.9549 - val_loss: 0.6489 - val_acc: 0.9556\n",
      "Epoch 486/1200\n",
      "133/133 [==============================] - 0s 109us/step - loss: 0.5800 - acc: 0.9549 - val_loss: 0.6473 - val_acc: 0.9778\n",
      "Epoch 487/1200\n",
      "133/133 [==============================] - 0s 97us/step - loss: 0.5786 - acc: 0.9549 - val_loss: 0.6457 - val_acc: 0.9778\n",
      "Epoch 488/1200\n",
      "133/133 [==============================] - 0s 105us/step - loss: 0.5768 - acc: 0.9549 - val_loss: 0.6443 - val_acc: 0.9778\n",
      "Epoch 489/1200\n",
      "133/133 [==============================] - 0s 108us/step - loss: 0.5752 - acc: 0.9549 - val_loss: 0.6427 - val_acc: 0.9556\n",
      "Epoch 490/1200\n",
      "133/133 [==============================] - 0s 115us/step - loss: 0.5734 - acc: 0.9549 - val_loss: 0.6413 - val_acc: 0.9556\n",
      "Epoch 491/1200\n",
      "133/133 [==============================] - 0s 132us/step - loss: 0.5718 - acc: 0.9549 - val_loss: 0.6400 - val_acc: 0.9556\n",
      "Epoch 492/1200\n",
      "133/133 [==============================] - 0s 123us/step - loss: 0.5703 - acc: 0.9549 - val_loss: 0.6385 - val_acc: 0.9556\n",
      "Epoch 493/1200\n",
      "133/133 [==============================] - 0s 96us/step - loss: 0.5685 - acc: 0.9549 - val_loss: 0.6371 - val_acc: 0.9556\n",
      "Epoch 494/1200\n",
      "133/133 [==============================] - 0s 98us/step - loss: 0.5668 - acc: 0.9549 - val_loss: 0.6358 - val_acc: 0.9333\n",
      "Epoch 495/1200\n",
      "133/133 [==============================] - 0s 106us/step - loss: 0.5651 - acc: 0.9624 - val_loss: 0.6341 - val_acc: 0.9333\n",
      "Epoch 496/1200\n",
      "133/133 [==============================] - 0s 212us/step - loss: 0.5636 - acc: 0.9624 - val_loss: 0.6326 - val_acc: 0.9333\n",
      "Epoch 497/1200\n",
      "133/133 [==============================] - 0s 141us/step - loss: 0.5620 - acc: 0.9549 - val_loss: 0.6314 - val_acc: 0.9333\n",
      "Epoch 498/1200\n",
      "133/133 [==============================] - 0s 108us/step - loss: 0.5602 - acc: 0.9624 - val_loss: 0.6300 - val_acc: 0.9333\n",
      "Epoch 499/1200\n",
      "133/133 [==============================] - 0s 108us/step - loss: 0.5590 - acc: 0.9549 - val_loss: 0.6286 - val_acc: 0.9333\n",
      "Epoch 500/1200\n",
      "133/133 [==============================] - 0s 123us/step - loss: 0.5572 - acc: 0.9549 - val_loss: 0.6272 - val_acc: 0.9333\n",
      "Epoch 501/1200\n",
      "133/133 [==============================] - 0s 110us/step - loss: 0.5554 - acc: 0.9624 - val_loss: 0.6257 - val_acc: 0.9333\n",
      "Epoch 502/1200\n",
      "133/133 [==============================] - 0s 112us/step - loss: 0.5539 - acc: 0.9624 - val_loss: 0.6244 - val_acc: 0.9333\n",
      "Epoch 503/1200\n",
      "133/133 [==============================] - 0s 117us/step - loss: 0.5523 - acc: 0.9624 - val_loss: 0.6230 - val_acc: 0.9333\n",
      "Epoch 504/1200\n",
      "133/133 [==============================] - 0s 105us/step - loss: 0.5503 - acc: 0.9624 - val_loss: 0.6214 - val_acc: 0.9333\n",
      "Epoch 505/1200\n",
      "133/133 [==============================] - 0s 121us/step - loss: 0.5488 - acc: 0.9624 - val_loss: 0.6200 - val_acc: 0.9333\n",
      "Epoch 506/1200\n",
      "133/133 [==============================] - 0s 140us/step - loss: 0.5470 - acc: 0.9624 - val_loss: 0.6185 - val_acc: 0.9333\n",
      "Epoch 507/1200\n",
      "133/133 [==============================] - 0s 156us/step - loss: 0.5457 - acc: 0.9624 - val_loss: 0.6171 - val_acc: 0.9333\n",
      "Epoch 508/1200\n",
      "133/133 [==============================] - 0s 144us/step - loss: 0.5439 - acc: 0.9624 - val_loss: 0.6156 - val_acc: 0.9333\n",
      "Epoch 509/1200\n",
      "133/133 [==============================] - 0s 101us/step - loss: 0.5425 - acc: 0.9624 - val_loss: 0.6141 - val_acc: 0.9333\n",
      "Epoch 510/1200\n",
      "133/133 [==============================] - 0s 124us/step - loss: 0.5405 - acc: 0.9624 - val_loss: 0.6126 - val_acc: 0.9333\n",
      "Epoch 511/1200\n",
      "133/133 [==============================] - 0s 112us/step - loss: 0.5388 - acc: 0.9624 - val_loss: 0.6111 - val_acc: 0.9333\n",
      "Epoch 512/1200\n",
      "133/133 [==============================] - 0s 109us/step - loss: 0.5372 - acc: 0.9624 - val_loss: 0.6096 - val_acc: 0.9333\n",
      "Epoch 513/1200\n",
      "133/133 [==============================] - 0s 99us/step - loss: 0.5355 - acc: 0.9624 - val_loss: 0.6081 - val_acc: 0.9333\n",
      "Epoch 514/1200\n",
      "133/133 [==============================] - 0s 96us/step - loss: 0.5340 - acc: 0.9624 - val_loss: 0.6066 - val_acc: 0.9333\n",
      "Epoch 515/1200\n",
      "133/133 [==============================] - 0s 118us/step - loss: 0.5322 - acc: 0.9624 - val_loss: 0.6051 - val_acc: 0.9333\n",
      "Epoch 516/1200\n",
      "133/133 [==============================] - 0s 146us/step - loss: 0.5303 - acc: 0.9624 - val_loss: 0.6037 - val_acc: 0.9333\n",
      "Epoch 517/1200\n",
      "133/133 [==============================] - 0s 240us/step - loss: 0.5286 - acc: 0.9624 - val_loss: 0.6022 - val_acc: 0.9333\n",
      "Epoch 518/1200\n",
      "133/133 [==============================] - 0s 128us/step - loss: 0.5269 - acc: 0.9624 - val_loss: 0.6005 - val_acc: 0.9333\n",
      "Epoch 519/1200\n",
      "133/133 [==============================] - 0s 116us/step - loss: 0.5251 - acc: 0.9699 - val_loss: 0.5990 - val_acc: 0.9333\n",
      "Epoch 520/1200\n",
      "133/133 [==============================] - 0s 124us/step - loss: 0.5232 - acc: 0.9699 - val_loss: 0.5972 - val_acc: 0.9333\n",
      "Epoch 521/1200\n",
      "133/133 [==============================] - 0s 138us/step - loss: 0.5215 - acc: 0.9699 - val_loss: 0.5956 - val_acc: 0.9333\n",
      "Epoch 522/1200\n",
      "133/133 [==============================] - 0s 114us/step - loss: 0.5197 - acc: 0.9699 - val_loss: 0.5941 - val_acc: 0.9333\n",
      "Epoch 523/1200\n",
      "133/133 [==============================] - 0s 112us/step - loss: 0.5182 - acc: 0.9699 - val_loss: 0.5925 - val_acc: 0.9333\n",
      "Epoch 524/1200\n",
      "133/133 [==============================] - 0s 114us/step - loss: 0.5162 - acc: 0.9699 - val_loss: 0.5910 - val_acc: 0.9333\n",
      "Epoch 525/1200\n",
      "133/133 [==============================] - 0s 119us/step - loss: 0.5145 - acc: 0.9699 - val_loss: 0.5895 - val_acc: 0.9333\n",
      "Epoch 526/1200\n",
      "133/133 [==============================] - 0s 105us/step - loss: 0.5129 - acc: 0.9699 - val_loss: 0.5878 - val_acc: 0.9333\n",
      "Epoch 527/1200\n",
      "133/133 [==============================] - 0s 179us/step - loss: 0.5114 - acc: 0.9699 - val_loss: 0.5863 - val_acc: 0.9333\n",
      "Epoch 528/1200\n",
      "133/133 [==============================] - 0s 150us/step - loss: 0.5096 - acc: 0.9699 - val_loss: 0.5846 - val_acc: 0.9333\n",
      "Epoch 529/1200\n",
      "133/133 [==============================] - 0s 113us/step - loss: 0.5079 - acc: 0.9699 - val_loss: 0.5830 - val_acc: 0.9333\n",
      "Epoch 530/1200\n",
      "133/133 [==============================] - 0s 125us/step - loss: 0.5062 - acc: 0.9699 - val_loss: 0.5815 - val_acc: 0.9333\n",
      "Epoch 531/1200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133/133 [==============================] - 0s 105us/step - loss: 0.5045 - acc: 0.9699 - val_loss: 0.5800 - val_acc: 0.9333\n",
      "Epoch 532/1200\n",
      "133/133 [==============================] - 0s 115us/step - loss: 0.5027 - acc: 0.9699 - val_loss: 0.5784 - val_acc: 0.9333\n",
      "Epoch 533/1200\n",
      "133/133 [==============================] - 0s 130us/step - loss: 0.5011 - acc: 0.9699 - val_loss: 0.5769 - val_acc: 0.9333\n",
      "Epoch 534/1200\n",
      "133/133 [==============================] - 0s 108us/step - loss: 0.4994 - acc: 0.9699 - val_loss: 0.5752 - val_acc: 0.9333\n",
      "Epoch 535/1200\n",
      "133/133 [==============================] - 0s 119us/step - loss: 0.4974 - acc: 0.9699 - val_loss: 0.5735 - val_acc: 0.9333\n",
      "Epoch 536/1200\n",
      "133/133 [==============================] - 0s 120us/step - loss: 0.4960 - acc: 0.9699 - val_loss: 0.5719 - val_acc: 0.9333\n",
      "Epoch 537/1200\n",
      "133/133 [==============================] - 0s 170us/step - loss: 0.4940 - acc: 0.9699 - val_loss: 0.5703 - val_acc: 0.9333\n",
      "Epoch 538/1200\n",
      "133/133 [==============================] - 0s 143us/step - loss: 0.4922 - acc: 0.9699 - val_loss: 0.5687 - val_acc: 0.9333\n",
      "Epoch 539/1200\n",
      "133/133 [==============================] - 0s 158us/step - loss: 0.4907 - acc: 0.9699 - val_loss: 0.5673 - val_acc: 0.9333\n",
      "Epoch 540/1200\n",
      "133/133 [==============================] - 0s 133us/step - loss: 0.4887 - acc: 0.9699 - val_loss: 0.5659 - val_acc: 0.9333\n",
      "Epoch 541/1200\n",
      "133/133 [==============================] - 0s 139us/step - loss: 0.4872 - acc: 0.9699 - val_loss: 0.5644 - val_acc: 0.9333\n",
      "Epoch 542/1200\n",
      "133/133 [==============================] - 0s 168us/step - loss: 0.4854 - acc: 0.9699 - val_loss: 0.5630 - val_acc: 0.9333\n",
      "Epoch 543/1200\n",
      "133/133 [==============================] - 0s 133us/step - loss: 0.4837 - acc: 0.9699 - val_loss: 0.5613 - val_acc: 0.9333\n",
      "Epoch 544/1200\n",
      "133/133 [==============================] - 0s 186us/step - loss: 0.4818 - acc: 0.9699 - val_loss: 0.5598 - val_acc: 0.9333\n",
      "Epoch 545/1200\n",
      "133/133 [==============================] - 0s 242us/step - loss: 0.4801 - acc: 0.9699 - val_loss: 0.5581 - val_acc: 0.9333\n",
      "Epoch 546/1200\n",
      "133/133 [==============================] - 0s 140us/step - loss: 0.4784 - acc: 0.9699 - val_loss: 0.5564 - val_acc: 0.9333\n",
      "Epoch 547/1200\n",
      "133/133 [==============================] - 0s 168us/step - loss: 0.4766 - acc: 0.9699 - val_loss: 0.5547 - val_acc: 0.9333\n",
      "Epoch 548/1200\n",
      "133/133 [==============================] - 0s 154us/step - loss: 0.4748 - acc: 0.9699 - val_loss: 0.5531 - val_acc: 0.9333\n",
      "Epoch 549/1200\n",
      "133/133 [==============================] - 0s 149us/step - loss: 0.4730 - acc: 0.9699 - val_loss: 0.5515 - val_acc: 0.9333\n",
      "Epoch 550/1200\n",
      "133/133 [==============================] - 0s 132us/step - loss: 0.4710 - acc: 0.9699 - val_loss: 0.5501 - val_acc: 0.9333\n",
      "Epoch 551/1200\n",
      "133/133 [==============================] - 0s 148us/step - loss: 0.4695 - acc: 0.9774 - val_loss: 0.5483 - val_acc: 0.9333\n",
      "Epoch 552/1200\n",
      "133/133 [==============================] - 0s 132us/step - loss: 0.4677 - acc: 0.9699 - val_loss: 0.5469 - val_acc: 0.9333\n",
      "Epoch 553/1200\n",
      "133/133 [==============================] - 0s 133us/step - loss: 0.4661 - acc: 0.9699 - val_loss: 0.5452 - val_acc: 0.9333\n",
      "Epoch 554/1200\n",
      "133/133 [==============================] - 0s 114us/step - loss: 0.4642 - acc: 0.9699 - val_loss: 0.5437 - val_acc: 0.9333\n",
      "Epoch 555/1200\n",
      "133/133 [==============================] - 0s 129us/step - loss: 0.4624 - acc: 0.9699 - val_loss: 0.5420 - val_acc: 0.9333\n",
      "Epoch 556/1200\n",
      "133/133 [==============================] - 0s 167us/step - loss: 0.4604 - acc: 0.9774 - val_loss: 0.5404 - val_acc: 0.9333\n",
      "Epoch 557/1200\n",
      "133/133 [==============================] - 0s 163us/step - loss: 0.4587 - acc: 0.9774 - val_loss: 0.5387 - val_acc: 0.9333\n",
      "Epoch 558/1200\n",
      "133/133 [==============================] - 0s 142us/step - loss: 0.4568 - acc: 0.9774 - val_loss: 0.5371 - val_acc: 0.9333\n",
      "Epoch 559/1200\n",
      "133/133 [==============================] - 0s 165us/step - loss: 0.4549 - acc: 0.9774 - val_loss: 0.5353 - val_acc: 0.9333\n",
      "Epoch 560/1200\n",
      "133/133 [==============================] - 0s 149us/step - loss: 0.4533 - acc: 0.9774 - val_loss: 0.5336 - val_acc: 0.9333\n",
      "Epoch 561/1200\n",
      "133/133 [==============================] - 0s 169us/step - loss: 0.4514 - acc: 0.9774 - val_loss: 0.5319 - val_acc: 0.9333\n",
      "Epoch 562/1200\n",
      "133/133 [==============================] - 0s 148us/step - loss: 0.4495 - acc: 0.9774 - val_loss: 0.5300 - val_acc: 0.9333\n",
      "Epoch 563/1200\n",
      "133/133 [==============================] - 0s 146us/step - loss: 0.4478 - acc: 0.9774 - val_loss: 0.5284 - val_acc: 0.9333\n",
      "Epoch 564/1200\n",
      "133/133 [==============================] - 0s 113us/step - loss: 0.4458 - acc: 0.9774 - val_loss: 0.5268 - val_acc: 0.9333\n",
      "Epoch 565/1200\n",
      "133/133 [==============================] - 0s 152us/step - loss: 0.4441 - acc: 0.9774 - val_loss: 0.5252 - val_acc: 0.9333\n",
      "Epoch 566/1200\n",
      "133/133 [==============================] - 0s 109us/step - loss: 0.4422 - acc: 0.9774 - val_loss: 0.5235 - val_acc: 0.9333\n",
      "Epoch 567/1200\n",
      "133/133 [==============================] - 0s 160us/step - loss: 0.4406 - acc: 0.9774 - val_loss: 0.5217 - val_acc: 0.9333\n",
      "Epoch 568/1200\n",
      "133/133 [==============================] - 0s 127us/step - loss: 0.4386 - acc: 0.9774 - val_loss: 0.5199 - val_acc: 0.9333\n",
      "Epoch 569/1200\n",
      "133/133 [==============================] - 0s 165us/step - loss: 0.4366 - acc: 0.9774 - val_loss: 0.5180 - val_acc: 0.9333\n",
      "Epoch 570/1200\n",
      "133/133 [==============================] - 0s 139us/step - loss: 0.4349 - acc: 0.9774 - val_loss: 0.5161 - val_acc: 0.9333\n",
      "Epoch 571/1200\n",
      "133/133 [==============================] - 0s 135us/step - loss: 0.4330 - acc: 0.9774 - val_loss: 0.5144 - val_acc: 0.9333\n",
      "Epoch 572/1200\n",
      "133/133 [==============================] - 0s 164us/step - loss: 0.4310 - acc: 0.9774 - val_loss: 0.5124 - val_acc: 0.9333\n",
      "Epoch 573/1200\n",
      "133/133 [==============================] - 0s 112us/step - loss: 0.4295 - acc: 0.9774 - val_loss: 0.5107 - val_acc: 0.9333\n",
      "Epoch 574/1200\n",
      "133/133 [==============================] - 0s 137us/step - loss: 0.4276 - acc: 0.9774 - val_loss: 0.5090 - val_acc: 0.9333\n",
      "Epoch 575/1200\n",
      "133/133 [==============================] - 0s 148us/step - loss: 0.4258 - acc: 0.9774 - val_loss: 0.5073 - val_acc: 0.9333\n",
      "Epoch 576/1200\n",
      "133/133 [==============================] - 0s 180us/step - loss: 0.4240 - acc: 0.9774 - val_loss: 0.5057 - val_acc: 0.9333\n",
      "Epoch 577/1200\n",
      "133/133 [==============================] - 0s 131us/step - loss: 0.4222 - acc: 0.9774 - val_loss: 0.5042 - val_acc: 0.9333\n",
      "Epoch 578/1200\n",
      "133/133 [==============================] - 0s 173us/step - loss: 0.4202 - acc: 0.9774 - val_loss: 0.5024 - val_acc: 0.9333\n",
      "Epoch 579/1200\n",
      "133/133 [==============================] - 0s 201us/step - loss: 0.4186 - acc: 0.9774 - val_loss: 0.5006 - val_acc: 0.9333\n",
      "Epoch 580/1200\n",
      "133/133 [==============================] - 0s 178us/step - loss: 0.4171 - acc: 0.9774 - val_loss: 0.4990 - val_acc: 0.9333\n",
      "Epoch 581/1200\n",
      "133/133 [==============================] - 0s 166us/step - loss: 0.4151 - acc: 0.9774 - val_loss: 0.4973 - val_acc: 0.9333\n",
      "Epoch 582/1200\n",
      "133/133 [==============================] - 0s 164us/step - loss: 0.4132 - acc: 0.9774 - val_loss: 0.4955 - val_acc: 0.9333\n",
      "Epoch 583/1200\n",
      "133/133 [==============================] - 0s 160us/step - loss: 0.4115 - acc: 0.9774 - val_loss: 0.4936 - val_acc: 0.9333\n",
      "Epoch 584/1200\n",
      "133/133 [==============================] - 0s 151us/step - loss: 0.4096 - acc: 0.9774 - val_loss: 0.4920 - val_acc: 0.9333\n",
      "Epoch 585/1200\n",
      "133/133 [==============================] - 0s 126us/step - loss: 0.4077 - acc: 0.9774 - val_loss: 0.4903 - val_acc: 0.9333\n",
      "Epoch 586/1200\n",
      "133/133 [==============================] - 0s 139us/step - loss: 0.4060 - acc: 0.9774 - val_loss: 0.4886 - val_acc: 0.9333\n",
      "Epoch 587/1200\n",
      "133/133 [==============================] - 0s 180us/step - loss: 0.4042 - acc: 0.9774 - val_loss: 0.4871 - val_acc: 0.9333\n",
      "Epoch 588/1200\n",
      "133/133 [==============================] - 0s 159us/step - loss: 0.4022 - acc: 0.9774 - val_loss: 0.4855 - val_acc: 0.9333\n",
      "Epoch 589/1200\n",
      "133/133 [==============================] - 0s 163us/step - loss: 0.4004 - acc: 0.9774 - val_loss: 0.4837 - val_acc: 0.9333\n",
      "Epoch 590/1200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133/133 [==============================] - 0s 122us/step - loss: 0.3986 - acc: 0.9774 - val_loss: 0.4820 - val_acc: 0.9333\n",
      "Epoch 591/1200\n",
      "133/133 [==============================] - 0s 143us/step - loss: 0.3965 - acc: 0.9774 - val_loss: 0.4802 - val_acc: 0.9333\n",
      "Epoch 592/1200\n",
      "133/133 [==============================] - 0s 136us/step - loss: 0.3951 - acc: 0.9774 - val_loss: 0.4785 - val_acc: 0.9333\n",
      "Epoch 593/1200\n",
      "133/133 [==============================] - 0s 153us/step - loss: 0.3928 - acc: 0.9774 - val_loss: 0.4767 - val_acc: 0.9333\n",
      "Epoch 594/1200\n",
      "133/133 [==============================] - 0s 141us/step - loss: 0.3913 - acc: 0.9774 - val_loss: 0.4750 - val_acc: 0.9333\n",
      "Epoch 595/1200\n",
      "133/133 [==============================] - 0s 136us/step - loss: 0.3895 - acc: 0.9774 - val_loss: 0.4733 - val_acc: 0.9333\n",
      "Epoch 596/1200\n",
      "133/133 [==============================] - 0s 134us/step - loss: 0.3876 - acc: 0.9774 - val_loss: 0.4718 - val_acc: 0.9333\n",
      "Epoch 597/1200\n",
      "133/133 [==============================] - 0s 151us/step - loss: 0.3856 - acc: 0.9774 - val_loss: 0.4701 - val_acc: 0.9333\n",
      "Epoch 598/1200\n",
      "133/133 [==============================] - 0s 133us/step - loss: 0.3840 - acc: 0.9774 - val_loss: 0.4684 - val_acc: 0.9333\n",
      "Epoch 599/1200\n",
      "133/133 [==============================] - 0s 132us/step - loss: 0.3823 - acc: 0.9774 - val_loss: 0.4666 - val_acc: 0.9333\n",
      "Epoch 600/1200\n",
      "133/133 [==============================] - 0s 151us/step - loss: 0.3802 - acc: 0.9774 - val_loss: 0.4651 - val_acc: 0.9333\n",
      "Epoch 601/1200\n",
      "133/133 [==============================] - 0s 160us/step - loss: 0.3785 - acc: 0.9774 - val_loss: 0.4633 - val_acc: 0.9333\n",
      "Epoch 602/1200\n",
      "133/133 [==============================] - 0s 150us/step - loss: 0.3766 - acc: 0.9774 - val_loss: 0.4616 - val_acc: 0.9333\n",
      "Epoch 603/1200\n",
      "133/133 [==============================] - 0s 145us/step - loss: 0.3747 - acc: 0.9774 - val_loss: 0.4600 - val_acc: 0.9333\n",
      "Epoch 604/1200\n",
      "133/133 [==============================] - 0s 125us/step - loss: 0.3728 - acc: 0.9774 - val_loss: 0.4581 - val_acc: 0.9333\n",
      "Epoch 605/1200\n",
      "133/133 [==============================] - 0s 149us/step - loss: 0.3710 - acc: 0.9774 - val_loss: 0.4564 - val_acc: 0.9333\n",
      "Epoch 606/1200\n",
      "133/133 [==============================] - 0s 106us/step - loss: 0.3693 - acc: 0.9774 - val_loss: 0.4546 - val_acc: 0.9333\n",
      "Epoch 607/1200\n",
      "133/133 [==============================] - 0s 137us/step - loss: 0.3676 - acc: 0.9774 - val_loss: 0.4530 - val_acc: 0.9333\n",
      "Epoch 608/1200\n",
      "133/133 [==============================] - 0s 125us/step - loss: 0.3660 - acc: 0.9774 - val_loss: 0.4513 - val_acc: 0.9333\n",
      "Epoch 609/1200\n",
      "133/133 [==============================] - 0s 133us/step - loss: 0.3642 - acc: 0.9774 - val_loss: 0.4498 - val_acc: 0.9333\n",
      "Epoch 610/1200\n",
      "133/133 [==============================] - 0s 142us/step - loss: 0.3624 - acc: 0.9774 - val_loss: 0.4483 - val_acc: 0.9333\n",
      "Epoch 611/1200\n",
      "133/133 [==============================] - 0s 178us/step - loss: 0.3607 - acc: 0.9774 - val_loss: 0.4466 - val_acc: 0.9333\n",
      "Epoch 612/1200\n",
      "133/133 [==============================] - 0s 143us/step - loss: 0.3592 - acc: 0.9774 - val_loss: 0.4448 - val_acc: 0.9333\n",
      "Epoch 613/1200\n",
      "133/133 [==============================] - 0s 153us/step - loss: 0.3571 - acc: 0.9774 - val_loss: 0.4432 - val_acc: 0.9333\n",
      "Epoch 614/1200\n",
      "133/133 [==============================] - 0s 113us/step - loss: 0.3557 - acc: 0.9774 - val_loss: 0.4415 - val_acc: 0.9333\n",
      "Epoch 615/1200\n",
      "133/133 [==============================] - 0s 108us/step - loss: 0.3535 - acc: 0.9774 - val_loss: 0.4397 - val_acc: 0.9333\n",
      "Epoch 616/1200\n",
      "133/133 [==============================] - 0s 105us/step - loss: 0.3520 - acc: 0.9774 - val_loss: 0.4383 - val_acc: 0.9333\n",
      "Epoch 617/1200\n",
      "133/133 [==============================] - 0s 126us/step - loss: 0.3503 - acc: 0.9774 - val_loss: 0.4371 - val_acc: 0.9333\n",
      "Epoch 618/1200\n",
      "133/133 [==============================] - 0s 109us/step - loss: 0.3483 - acc: 0.9774 - val_loss: 0.4353 - val_acc: 0.9333\n",
      "Epoch 619/1200\n",
      "133/133 [==============================] - 0s 109us/step - loss: 0.3467 - acc: 0.9774 - val_loss: 0.4338 - val_acc: 0.9333\n",
      "Epoch 620/1200\n",
      "133/133 [==============================] - 0s 111us/step - loss: 0.3451 - acc: 0.9774 - val_loss: 0.4325 - val_acc: 0.9333\n",
      "Epoch 621/1200\n",
      "133/133 [==============================] - 0s 104us/step - loss: 0.3433 - acc: 0.9774 - val_loss: 0.4310 - val_acc: 0.9333\n",
      "Epoch 622/1200\n",
      "133/133 [==============================] - 0s 107us/step - loss: 0.3416 - acc: 0.9774 - val_loss: 0.4292 - val_acc: 0.9333\n",
      "Epoch 623/1200\n",
      "133/133 [==============================] - 0s 101us/step - loss: 0.3398 - acc: 0.9774 - val_loss: 0.4276 - val_acc: 0.9333\n",
      "Epoch 624/1200\n",
      "133/133 [==============================] - 0s 105us/step - loss: 0.3383 - acc: 0.9774 - val_loss: 0.4260 - val_acc: 0.9333\n",
      "Epoch 625/1200\n",
      "133/133 [==============================] - 0s 106us/step - loss: 0.3365 - acc: 0.9774 - val_loss: 0.4245 - val_acc: 0.9333\n",
      "Epoch 626/1200\n",
      "133/133 [==============================] - 0s 125us/step - loss: 0.3351 - acc: 0.9774 - val_loss: 0.4232 - val_acc: 0.9333\n",
      "Epoch 627/1200\n",
      "133/133 [==============================] - 0s 108us/step - loss: 0.3331 - acc: 0.9774 - val_loss: 0.4217 - val_acc: 0.9333\n",
      "Epoch 628/1200\n",
      "133/133 [==============================] - 0s 126us/step - loss: 0.3317 - acc: 0.9774 - val_loss: 0.4200 - val_acc: 0.9333\n",
      "Epoch 629/1200\n",
      "133/133 [==============================] - 0s 116us/step - loss: 0.3301 - acc: 0.9774 - val_loss: 0.4182 - val_acc: 0.9333\n",
      "Epoch 630/1200\n",
      "133/133 [==============================] - 0s 119us/step - loss: 0.3284 - acc: 0.9774 - val_loss: 0.4171 - val_acc: 0.9333\n",
      "Epoch 631/1200\n",
      "133/133 [==============================] - 0s 120us/step - loss: 0.3267 - acc: 0.9774 - val_loss: 0.4157 - val_acc: 0.9333\n",
      "Epoch 632/1200\n",
      "133/133 [==============================] - 0s 115us/step - loss: 0.3253 - acc: 0.9774 - val_loss: 0.4142 - val_acc: 0.9333\n",
      "Epoch 633/1200\n",
      "133/133 [==============================] - 0s 122us/step - loss: 0.3235 - acc: 0.9774 - val_loss: 0.4123 - val_acc: 0.9333\n",
      "Epoch 634/1200\n",
      "133/133 [==============================] - 0s 150us/step - loss: 0.3218 - acc: 0.9774 - val_loss: 0.4108 - val_acc: 0.9333\n",
      "Epoch 635/1200\n",
      "133/133 [==============================] - 0s 114us/step - loss: 0.3203 - acc: 0.9774 - val_loss: 0.4091 - val_acc: 0.9333\n",
      "Epoch 636/1200\n",
      "133/133 [==============================] - 0s 131us/step - loss: 0.3188 - acc: 0.9774 - val_loss: 0.4077 - val_acc: 0.9333\n",
      "Epoch 637/1200\n",
      "133/133 [==============================] - 0s 119us/step - loss: 0.3172 - acc: 0.9774 - val_loss: 0.4063 - val_acc: 0.9333\n",
      "Epoch 638/1200\n",
      "133/133 [==============================] - 0s 104us/step - loss: 0.3156 - acc: 0.9774 - val_loss: 0.4050 - val_acc: 0.9333\n",
      "Epoch 639/1200\n",
      "133/133 [==============================] - 0s 107us/step - loss: 0.3141 - acc: 0.9774 - val_loss: 0.4037 - val_acc: 0.9333\n",
      "Epoch 640/1200\n",
      "133/133 [==============================] - 0s 135us/step - loss: 0.3124 - acc: 0.9850 - val_loss: 0.4020 - val_acc: 0.9333\n",
      "Epoch 641/1200\n",
      "133/133 [==============================] - 0s 131us/step - loss: 0.3108 - acc: 0.9774 - val_loss: 0.4007 - val_acc: 0.9333\n",
      "Epoch 642/1200\n",
      "133/133 [==============================] - 0s 105us/step - loss: 0.3093 - acc: 0.9850 - val_loss: 0.3989 - val_acc: 0.9333\n",
      "Epoch 643/1200\n",
      "133/133 [==============================] - 0s 114us/step - loss: 0.3080 - acc: 0.9850 - val_loss: 0.3972 - val_acc: 0.9333\n",
      "Epoch 644/1200\n",
      "133/133 [==============================] - 0s 105us/step - loss: 0.3061 - acc: 0.9850 - val_loss: 0.3962 - val_acc: 0.9333\n",
      "Epoch 645/1200\n",
      "133/133 [==============================] - 0s 119us/step - loss: 0.3048 - acc: 0.9850 - val_loss: 0.3946 - val_acc: 0.9333\n",
      "Epoch 646/1200\n",
      "133/133 [==============================] - 0s 103us/step - loss: 0.3032 - acc: 0.9850 - val_loss: 0.3934 - val_acc: 0.9333\n",
      "Epoch 647/1200\n",
      "133/133 [==============================] - 0s 96us/step - loss: 0.3019 - acc: 0.9850 - val_loss: 0.3919 - val_acc: 0.9333\n",
      "Epoch 648/1200\n",
      "133/133 [==============================] - 0s 109us/step - loss: 0.3003 - acc: 0.9850 - val_loss: 0.3907 - val_acc: 0.9333\n",
      "Epoch 649/1200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133/133 [==============================] - 0s 96us/step - loss: 0.2989 - acc: 0.9850 - val_loss: 0.3893 - val_acc: 0.9333\n",
      "Epoch 650/1200\n",
      "133/133 [==============================] - 0s 107us/step - loss: 0.2971 - acc: 0.9850 - val_loss: 0.3881 - val_acc: 0.9333\n",
      "Epoch 651/1200\n",
      "133/133 [==============================] - 0s 100us/step - loss: 0.2956 - acc: 0.9850 - val_loss: 0.3864 - val_acc: 0.9333\n",
      "Epoch 652/1200\n",
      "133/133 [==============================] - 0s 96us/step - loss: 0.2941 - acc: 0.9850 - val_loss: 0.3847 - val_acc: 0.9333\n",
      "Epoch 653/1200\n",
      "133/133 [==============================] - 0s 93us/step - loss: 0.2926 - acc: 0.9850 - val_loss: 0.3835 - val_acc: 0.9333\n",
      "Epoch 654/1200\n",
      "133/133 [==============================] - 0s 89us/step - loss: 0.2913 - acc: 0.9850 - val_loss: 0.3823 - val_acc: 0.9333\n",
      "Epoch 655/1200\n",
      "133/133 [==============================] - 0s 96us/step - loss: 0.2900 - acc: 0.9850 - val_loss: 0.3810 - val_acc: 0.9333\n",
      "Epoch 656/1200\n",
      "133/133 [==============================] - 0s 142us/step - loss: 0.2886 - acc: 0.9850 - val_loss: 0.3794 - val_acc: 0.9333\n",
      "Epoch 657/1200\n",
      "133/133 [==============================] - 0s 102us/step - loss: 0.2871 - acc: 0.9850 - val_loss: 0.3780 - val_acc: 0.9333\n",
      "Epoch 658/1200\n",
      "133/133 [==============================] - 0s 110us/step - loss: 0.2856 - acc: 0.9850 - val_loss: 0.3766 - val_acc: 0.9333\n",
      "Epoch 659/1200\n",
      "133/133 [==============================] - 0s 104us/step - loss: 0.2848 - acc: 0.9850 - val_loss: 0.3752 - val_acc: 0.9333\n",
      "Epoch 660/1200\n",
      "133/133 [==============================] - 0s 100us/step - loss: 0.2831 - acc: 0.9850 - val_loss: 0.3740 - val_acc: 0.9333\n",
      "Epoch 661/1200\n",
      "133/133 [==============================] - 0s 99us/step - loss: 0.2816 - acc: 0.9850 - val_loss: 0.3728 - val_acc: 0.9333\n",
      "Epoch 662/1200\n",
      "133/133 [==============================] - 0s 106us/step - loss: 0.2803 - acc: 0.9850 - val_loss: 0.3717 - val_acc: 0.9333\n",
      "Epoch 663/1200\n",
      "133/133 [==============================] - 0s 98us/step - loss: 0.2790 - acc: 0.9850 - val_loss: 0.3706 - val_acc: 0.9333\n",
      "Epoch 664/1200\n",
      "133/133 [==============================] - 0s 104us/step - loss: 0.2777 - acc: 0.9850 - val_loss: 0.3698 - val_acc: 0.9333\n",
      "Epoch 665/1200\n",
      "133/133 [==============================] - 0s 102us/step - loss: 0.2760 - acc: 0.9850 - val_loss: 0.3684 - val_acc: 0.9333\n",
      "Epoch 666/1200\n",
      "133/133 [==============================] - 0s 114us/step - loss: 0.2749 - acc: 0.9850 - val_loss: 0.3674 - val_acc: 0.9333\n",
      "Epoch 667/1200\n",
      "133/133 [==============================] - 0s 117us/step - loss: 0.2734 - acc: 0.9850 - val_loss: 0.3663 - val_acc: 0.9333\n",
      "Epoch 668/1200\n",
      "133/133 [==============================] - 0s 116us/step - loss: 0.2720 - acc: 0.9850 - val_loss: 0.3649 - val_acc: 0.9333\n",
      "Epoch 669/1200\n",
      "133/133 [==============================] - 0s 114us/step - loss: 0.2708 - acc: 0.9850 - val_loss: 0.3635 - val_acc: 0.9333\n",
      "Epoch 670/1200\n",
      "133/133 [==============================] - 0s 117us/step - loss: 0.2693 - acc: 0.9850 - val_loss: 0.3622 - val_acc: 0.9333\n",
      "Epoch 671/1200\n",
      "133/133 [==============================] - 0s 104us/step - loss: 0.2682 - acc: 0.9850 - val_loss: 0.3613 - val_acc: 0.9333\n",
      "Epoch 672/1200\n",
      "133/133 [==============================] - 0s 115us/step - loss: 0.2666 - acc: 0.9850 - val_loss: 0.3601 - val_acc: 0.9333\n",
      "Epoch 673/1200\n",
      "133/133 [==============================] - 0s 110us/step - loss: 0.2656 - acc: 0.9850 - val_loss: 0.3589 - val_acc: 0.9333\n",
      "Epoch 674/1200\n",
      "133/133 [==============================] - 0s 113us/step - loss: 0.2639 - acc: 0.9850 - val_loss: 0.3576 - val_acc: 0.9333\n",
      "Epoch 675/1200\n",
      "133/133 [==============================] - 0s 113us/step - loss: 0.2626 - acc: 0.9850 - val_loss: 0.3568 - val_acc: 0.9333\n",
      "Epoch 676/1200\n",
      "133/133 [==============================] - 0s 119us/step - loss: 0.2616 - acc: 0.9850 - val_loss: 0.3556 - val_acc: 0.9333\n",
      "Epoch 677/1200\n",
      "133/133 [==============================] - 0s 109us/step - loss: 0.2603 - acc: 0.9850 - val_loss: 0.3542 - val_acc: 0.9333\n",
      "Epoch 678/1200\n",
      "133/133 [==============================] - 0s 115us/step - loss: 0.2592 - acc: 0.9850 - val_loss: 0.3530 - val_acc: 0.9333\n",
      "Epoch 679/1200\n",
      "133/133 [==============================] - 0s 114us/step - loss: 0.2578 - acc: 0.9850 - val_loss: 0.3519 - val_acc: 0.9333\n",
      "Epoch 680/1200\n",
      "133/133 [==============================] - 0s 113us/step - loss: 0.2568 - acc: 0.9850 - val_loss: 0.3509 - val_acc: 0.9333\n",
      "Epoch 681/1200\n",
      "133/133 [==============================] - 0s 106us/step - loss: 0.2554 - acc: 0.9850 - val_loss: 0.3497 - val_acc: 0.9333\n",
      "Epoch 682/1200\n",
      "133/133 [==============================] - 0s 118us/step - loss: 0.2543 - acc: 0.9850 - val_loss: 0.3485 - val_acc: 0.9333\n",
      "Epoch 683/1200\n",
      "133/133 [==============================] - 0s 115us/step - loss: 0.2537 - acc: 0.9850 - val_loss: 0.3470 - val_acc: 0.9333\n",
      "Epoch 684/1200\n",
      "133/133 [==============================] - 0s 114us/step - loss: 0.2519 - acc: 0.9850 - val_loss: 0.3461 - val_acc: 0.9333\n",
      "Epoch 685/1200\n",
      "133/133 [==============================] - 0s 130us/step - loss: 0.2508 - acc: 0.9850 - val_loss: 0.3448 - val_acc: 0.9333\n",
      "Epoch 686/1200\n",
      "133/133 [==============================] - 0s 115us/step - loss: 0.2498 - acc: 0.9850 - val_loss: 0.3440 - val_acc: 0.9333\n",
      "Epoch 687/1200\n",
      "133/133 [==============================] - 0s 125us/step - loss: 0.2485 - acc: 0.9850 - val_loss: 0.3432 - val_acc: 0.9333\n",
      "Epoch 688/1200\n",
      "133/133 [==============================] - 0s 120us/step - loss: 0.2471 - acc: 0.9850 - val_loss: 0.3421 - val_acc: 0.9333\n",
      "Epoch 689/1200\n",
      "133/133 [==============================] - 0s 116us/step - loss: 0.2462 - acc: 0.9850 - val_loss: 0.3413 - val_acc: 0.9333\n",
      "Epoch 690/1200\n",
      "133/133 [==============================] - 0s 109us/step - loss: 0.2452 - acc: 0.9850 - val_loss: 0.3399 - val_acc: 0.9333\n",
      "Epoch 691/1200\n",
      "133/133 [==============================] - 0s 112us/step - loss: 0.2439 - acc: 0.9850 - val_loss: 0.3388 - val_acc: 0.9333\n",
      "Epoch 692/1200\n",
      "133/133 [==============================] - 0s 112us/step - loss: 0.2425 - acc: 0.9850 - val_loss: 0.3379 - val_acc: 0.9333\n",
      "Epoch 693/1200\n",
      "133/133 [==============================] - 0s 107us/step - loss: 0.2416 - acc: 0.9850 - val_loss: 0.3364 - val_acc: 0.9333\n",
      "Epoch 694/1200\n",
      "133/133 [==============================] - 0s 111us/step - loss: 0.2405 - acc: 0.9850 - val_loss: 0.3352 - val_acc: 0.9333\n",
      "Epoch 695/1200\n",
      "133/133 [==============================] - 0s 116us/step - loss: 0.2393 - acc: 0.9850 - val_loss: 0.3340 - val_acc: 0.9333\n",
      "Epoch 696/1200\n",
      "133/133 [==============================] - 0s 123us/step - loss: 0.2382 - acc: 0.9850 - val_loss: 0.3328 - val_acc: 0.9333\n",
      "Epoch 697/1200\n",
      "133/133 [==============================] - 0s 109us/step - loss: 0.2371 - acc: 0.9850 - val_loss: 0.3318 - val_acc: 0.9333\n",
      "Epoch 698/1200\n",
      "133/133 [==============================] - 0s 109us/step - loss: 0.2361 - acc: 0.9850 - val_loss: 0.3305 - val_acc: 0.9333\n",
      "Epoch 699/1200\n",
      "133/133 [==============================] - 0s 120us/step - loss: 0.2350 - acc: 0.9850 - val_loss: 0.3296 - val_acc: 0.9333\n",
      "Epoch 700/1200\n",
      "133/133 [==============================] - 0s 121us/step - loss: 0.2339 - acc: 0.9850 - val_loss: 0.3286 - val_acc: 0.9333\n",
      "Epoch 701/1200\n",
      "133/133 [==============================] - 0s 115us/step - loss: 0.2327 - acc: 0.9850 - val_loss: 0.3279 - val_acc: 0.9333\n",
      "Epoch 702/1200\n",
      "133/133 [==============================] - 0s 123us/step - loss: 0.2315 - acc: 0.9850 - val_loss: 0.3267 - val_acc: 0.9333\n",
      "Epoch 703/1200\n",
      "133/133 [==============================] - 0s 118us/step - loss: 0.2303 - acc: 0.9850 - val_loss: 0.3259 - val_acc: 0.9333\n",
      "Epoch 704/1200\n",
      "133/133 [==============================] - 0s 125us/step - loss: 0.2294 - acc: 0.9850 - val_loss: 0.3247 - val_acc: 0.9333\n",
      "Epoch 705/1200\n",
      "133/133 [==============================] - 0s 111us/step - loss: 0.2282 - acc: 0.9850 - val_loss: 0.3240 - val_acc: 0.9333\n",
      "Epoch 706/1200\n",
      "133/133 [==============================] - 0s 111us/step - loss: 0.2270 - acc: 0.9850 - val_loss: 0.3225 - val_acc: 0.9333\n",
      "Epoch 707/1200\n",
      "133/133 [==============================] - 0s 122us/step - loss: 0.2257 - acc: 0.9850 - val_loss: 0.3219 - val_acc: 0.9333\n",
      "Epoch 708/1200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133/133 [==============================] - 0s 117us/step - loss: 0.2247 - acc: 0.9850 - val_loss: 0.3206 - val_acc: 0.9333\n",
      "Epoch 709/1200\n",
      "133/133 [==============================] - 0s 109us/step - loss: 0.2238 - acc: 0.9850 - val_loss: 0.3196 - val_acc: 0.9333\n",
      "Epoch 710/1200\n",
      "133/133 [==============================] - 0s 104us/step - loss: 0.2226 - acc: 0.9850 - val_loss: 0.3190 - val_acc: 0.9111\n",
      "Epoch 711/1200\n",
      "133/133 [==============================] - 0s 99us/step - loss: 0.2216 - acc: 0.9850 - val_loss: 0.3177 - val_acc: 0.9333\n",
      "Epoch 712/1200\n",
      "133/133 [==============================] - 0s 102us/step - loss: 0.2207 - acc: 0.9850 - val_loss: 0.3168 - val_acc: 0.9111\n",
      "Epoch 713/1200\n",
      "133/133 [==============================] - 0s 100us/step - loss: 0.2195 - acc: 0.9850 - val_loss: 0.3157 - val_acc: 0.9111\n",
      "Epoch 714/1200\n",
      "133/133 [==============================] - 0s 108us/step - loss: 0.2186 - acc: 0.9850 - val_loss: 0.3146 - val_acc: 0.9111\n",
      "Epoch 715/1200\n",
      "133/133 [==============================] - 0s 125us/step - loss: 0.2175 - acc: 0.9850 - val_loss: 0.3139 - val_acc: 0.9111\n",
      "Epoch 716/1200\n",
      "133/133 [==============================] - ETA: 0s - loss: 0.2222 - acc: 1.000 - 0s 100us/step - loss: 0.2164 - acc: 0.9850 - val_loss: 0.3129 - val_acc: 0.9111\n",
      "Epoch 717/1200\n",
      "133/133 [==============================] - 0s 121us/step - loss: 0.2156 - acc: 0.9850 - val_loss: 0.3117 - val_acc: 0.9111\n",
      "Epoch 718/1200\n",
      "133/133 [==============================] - 0s 91us/step - loss: 0.2145 - acc: 0.9850 - val_loss: 0.3111 - val_acc: 0.9111\n",
      "Epoch 719/1200\n",
      "133/133 [==============================] - 0s 105us/step - loss: 0.2134 - acc: 0.9850 - val_loss: 0.3105 - val_acc: 0.9111\n",
      "Epoch 720/1200\n",
      "133/133 [==============================] - 0s 102us/step - loss: 0.2123 - acc: 0.9850 - val_loss: 0.3098 - val_acc: 0.9111\n",
      "Epoch 721/1200\n",
      "133/133 [==============================] - 0s 106us/step - loss: 0.2112 - acc: 0.9850 - val_loss: 0.3092 - val_acc: 0.9111\n",
      "Epoch 722/1200\n",
      "133/133 [==============================] - 0s 115us/step - loss: 0.2099 - acc: 0.9850 - val_loss: 0.3077 - val_acc: 0.9111\n",
      "Epoch 723/1200\n",
      "133/133 [==============================] - 0s 108us/step - loss: 0.2089 - acc: 0.9850 - val_loss: 0.3067 - val_acc: 0.9111\n",
      "Epoch 724/1200\n",
      "133/133 [==============================] - 0s 111us/step - loss: 0.2077 - acc: 0.9850 - val_loss: 0.3051 - val_acc: 0.9111\n",
      "Epoch 725/1200\n",
      "133/133 [==============================] - 0s 108us/step - loss: 0.2067 - acc: 0.9850 - val_loss: 0.3039 - val_acc: 0.9111\n",
      "Epoch 726/1200\n",
      "133/133 [==============================] - 0s 101us/step - loss: 0.2057 - acc: 0.9850 - val_loss: 0.3032 - val_acc: 0.9111\n",
      "Epoch 727/1200\n",
      "133/133 [==============================] - 0s 103us/step - loss: 0.2046 - acc: 0.9850 - val_loss: 0.3023 - val_acc: 0.9111\n",
      "Epoch 728/1200\n",
      "133/133 [==============================] - 0s 112us/step - loss: 0.2035 - acc: 0.9850 - val_loss: 0.3016 - val_acc: 0.9111\n",
      "Epoch 729/1200\n",
      "133/133 [==============================] - 0s 107us/step - loss: 0.2026 - acc: 0.9850 - val_loss: 0.3006 - val_acc: 0.9111\n",
      "Epoch 730/1200\n",
      "133/133 [==============================] - 0s 107us/step - loss: 0.2017 - acc: 0.9850 - val_loss: 0.2999 - val_acc: 0.9111\n",
      "Epoch 731/1200\n",
      "133/133 [==============================] - 0s 113us/step - loss: 0.2001 - acc: 0.9850 - val_loss: 0.2992 - val_acc: 0.9111\n",
      "Epoch 732/1200\n",
      "133/133 [==============================] - 0s 120us/step - loss: 0.1992 - acc: 0.9850 - val_loss: 0.2982 - val_acc: 0.9111\n",
      "Epoch 733/1200\n",
      "133/133 [==============================] - 0s 126us/step - loss: 0.1981 - acc: 0.9850 - val_loss: 0.2972 - val_acc: 0.9111\n",
      "Epoch 734/1200\n",
      "133/133 [==============================] - 0s 125us/step - loss: 0.1968 - acc: 0.9850 - val_loss: 0.2966 - val_acc: 0.9111\n",
      "Epoch 735/1200\n",
      "133/133 [==============================] - 0s 103us/step - loss: 0.1956 - acc: 0.9850 - val_loss: 0.2958 - val_acc: 0.9111\n",
      "Epoch 736/1200\n",
      "133/133 [==============================] - 0s 107us/step - loss: 0.1947 - acc: 0.9850 - val_loss: 0.2939 - val_acc: 0.9111\n",
      "Epoch 737/1200\n",
      "133/133 [==============================] - 0s 116us/step - loss: 0.1936 - acc: 0.9850 - val_loss: 0.2933 - val_acc: 0.9111\n",
      "Epoch 738/1200\n",
      "133/133 [==============================] - 0s 119us/step - loss: 0.1922 - acc: 0.9850 - val_loss: 0.2921 - val_acc: 0.9111\n",
      "Epoch 739/1200\n",
      "133/133 [==============================] - 0s 114us/step - loss: 0.1915 - acc: 0.9850 - val_loss: 0.2913 - val_acc: 0.9111\n",
      "Epoch 740/1200\n",
      "133/133 [==============================] - 0s 118us/step - loss: 0.1902 - acc: 0.9850 - val_loss: 0.2902 - val_acc: 0.9111\n",
      "Epoch 741/1200\n",
      "133/133 [==============================] - 0s 103us/step - loss: 0.1890 - acc: 0.9850 - val_loss: 0.2895 - val_acc: 0.9111\n",
      "Epoch 742/1200\n",
      "133/133 [==============================] - 0s 104us/step - loss: 0.1880 - acc: 0.9850 - val_loss: 0.2888 - val_acc: 0.9111\n",
      "Epoch 743/1200\n",
      "133/133 [==============================] - 0s 109us/step - loss: 0.1870 - acc: 0.9850 - val_loss: 0.2880 - val_acc: 0.9111\n",
      "Epoch 744/1200\n",
      "133/133 [==============================] - 0s 133us/step - loss: 0.1857 - acc: 0.9850 - val_loss: 0.2871 - val_acc: 0.9111\n",
      "Epoch 745/1200\n",
      "133/133 [==============================] - 0s 104us/step - loss: 0.1844 - acc: 0.9850 - val_loss: 0.2860 - val_acc: 0.9111\n",
      "Epoch 746/1200\n",
      "133/133 [==============================] - 0s 125us/step - loss: 0.1839 - acc: 0.9850 - val_loss: 0.2842 - val_acc: 0.9111\n",
      "Epoch 747/1200\n",
      "133/133 [==============================] - 0s 122us/step - loss: 0.1821 - acc: 0.9850 - val_loss: 0.2839 - val_acc: 0.9111\n",
      "Epoch 748/1200\n",
      "133/133 [==============================] - 0s 110us/step - loss: 0.1811 - acc: 0.9850 - val_loss: 0.2825 - val_acc: 0.9111\n",
      "Epoch 749/1200\n",
      "133/133 [==============================] - 0s 114us/step - loss: 0.1802 - acc: 0.9850 - val_loss: 0.2813 - val_acc: 0.9111\n",
      "Epoch 750/1200\n",
      "133/133 [==============================] - 0s 114us/step - loss: 0.1789 - acc: 0.9850 - val_loss: 0.2805 - val_acc: 0.9111\n",
      "Epoch 751/1200\n",
      "133/133 [==============================] - 0s 112us/step - loss: 0.1779 - acc: 0.9850 - val_loss: 0.2795 - val_acc: 0.9111\n",
      "Epoch 752/1200\n",
      "133/133 [==============================] - 0s 108us/step - loss: 0.1769 - acc: 0.9850 - val_loss: 0.2780 - val_acc: 0.9111\n",
      "Epoch 753/1200\n",
      "133/133 [==============================] - 0s 113us/step - loss: 0.1755 - acc: 0.9850 - val_loss: 0.2765 - val_acc: 0.9111\n",
      "Epoch 754/1200\n",
      "133/133 [==============================] - 0s 109us/step - loss: 0.1747 - acc: 0.9850 - val_loss: 0.2756 - val_acc: 0.9111\n",
      "Epoch 755/1200\n",
      "133/133 [==============================] - 0s 102us/step - loss: 0.1734 - acc: 0.9850 - val_loss: 0.2745 - val_acc: 0.9111\n",
      "Epoch 756/1200\n",
      "133/133 [==============================] - 0s 117us/step - loss: 0.1721 - acc: 0.9850 - val_loss: 0.2726 - val_acc: 0.9111\n",
      "Epoch 757/1200\n",
      "133/133 [==============================] - 0s 103us/step - loss: 0.1715 - acc: 0.9850 - val_loss: 0.2715 - val_acc: 0.9111\n",
      "Epoch 758/1200\n",
      "133/133 [==============================] - 0s 104us/step - loss: 0.1702 - acc: 0.9850 - val_loss: 0.2705 - val_acc: 0.9111\n",
      "Epoch 759/1200\n",
      "133/133 [==============================] - 0s 120us/step - loss: 0.1690 - acc: 0.9850 - val_loss: 0.2693 - val_acc: 0.9111\n",
      "Epoch 760/1200\n",
      "133/133 [==============================] - 0s 108us/step - loss: 0.1679 - acc: 0.9850 - val_loss: 0.2687 - val_acc: 0.9111\n",
      "Epoch 761/1200\n",
      "133/133 [==============================] - 0s 99us/step - loss: 0.1667 - acc: 0.9850 - val_loss: 0.2676 - val_acc: 0.9111\n",
      "Epoch 762/1200\n",
      "133/133 [==============================] - 0s 118us/step - loss: 0.1653 - acc: 0.9850 - val_loss: 0.2664 - val_acc: 0.9111\n",
      "Epoch 763/1200\n",
      "133/133 [==============================] - 0s 116us/step - loss: 0.1644 - acc: 0.9850 - val_loss: 0.2659 - val_acc: 0.9111\n",
      "Epoch 764/1200\n",
      "133/133 [==============================] - 0s 106us/step - loss: 0.1632 - acc: 0.9850 - val_loss: 0.2655 - val_acc: 0.9111\n",
      "Epoch 765/1200\n",
      "133/133 [==============================] - 0s 116us/step - loss: 0.1618 - acc: 0.9925 - val_loss: 0.2640 - val_acc: 0.9111\n",
      "Epoch 766/1200\n",
      "133/133 [==============================] - 0s 109us/step - loss: 0.1609 - acc: 0.9850 - val_loss: 0.2627 - val_acc: 0.9111\n",
      "Epoch 767/1200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133/133 [==============================] - 0s 115us/step - loss: 0.1597 - acc: 0.9925 - val_loss: 0.2614 - val_acc: 0.9111\n",
      "Epoch 768/1200\n",
      "133/133 [==============================] - 0s 162us/step - loss: 0.1584 - acc: 0.9925 - val_loss: 0.2604 - val_acc: 0.9111\n",
      "Epoch 769/1200\n",
      "133/133 [==============================] - 0s 111us/step - loss: 0.1575 - acc: 0.9850 - val_loss: 0.2601 - val_acc: 0.9111\n",
      "Epoch 770/1200\n",
      "133/133 [==============================] - 0s 113us/step - loss: 0.1564 - acc: 0.9850 - val_loss: 0.2590 - val_acc: 0.9111\n",
      "Epoch 771/1200\n",
      "133/133 [==============================] - 0s 110us/step - loss: 0.1550 - acc: 0.9925 - val_loss: 0.2578 - val_acc: 0.9111\n",
      "Epoch 772/1200\n",
      "133/133 [==============================] - 0s 111us/step - loss: 0.1540 - acc: 0.9925 - val_loss: 0.2563 - val_acc: 0.9111\n",
      "Epoch 773/1200\n",
      "133/133 [==============================] - 0s 110us/step - loss: 0.1530 - acc: 0.9925 - val_loss: 0.2556 - val_acc: 0.9111\n",
      "Epoch 774/1200\n",
      "133/133 [==============================] - 0s 113us/step - loss: 0.1519 - acc: 0.9925 - val_loss: 0.2547 - val_acc: 0.9111\n",
      "Epoch 775/1200\n",
      "133/133 [==============================] - 0s 112us/step - loss: 0.1506 - acc: 0.9925 - val_loss: 0.2531 - val_acc: 0.9111\n",
      "Epoch 776/1200\n",
      "133/133 [==============================] - 0s 120us/step - loss: 0.1496 - acc: 0.9850 - val_loss: 0.2527 - val_acc: 0.9111\n",
      "Epoch 777/1200\n",
      "133/133 [==============================] - 0s 106us/step - loss: 0.1484 - acc: 0.9925 - val_loss: 0.2517 - val_acc: 0.9111\n",
      "Epoch 778/1200\n",
      "133/133 [==============================] - 0s 111us/step - loss: 0.1472 - acc: 0.9925 - val_loss: 0.2508 - val_acc: 0.9333\n",
      "Epoch 779/1200\n",
      "133/133 [==============================] - 0s 105us/step - loss: 0.1462 - acc: 0.9925 - val_loss: 0.2501 - val_acc: 0.9333\n",
      "Epoch 780/1200\n",
      "133/133 [==============================] - 0s 104us/step - loss: 0.1452 - acc: 0.9925 - val_loss: 0.2491 - val_acc: 0.9333\n",
      "Epoch 781/1200\n",
      "133/133 [==============================] - 0s 98us/step - loss: 0.1437 - acc: 0.9925 - val_loss: 0.2480 - val_acc: 0.9333\n",
      "Epoch 782/1200\n",
      "133/133 [==============================] - 0s 104us/step - loss: 0.1426 - acc: 0.9925 - val_loss: 0.2469 - val_acc: 0.9333\n",
      "Epoch 783/1200\n",
      "133/133 [==============================] - 0s 106us/step - loss: 0.1418 - acc: 0.9925 - val_loss: 0.2460 - val_acc: 0.9333\n",
      "Epoch 784/1200\n",
      "133/133 [==============================] - 0s 109us/step - loss: 0.1405 - acc: 0.9925 - val_loss: 0.2452 - val_acc: 0.9333\n",
      "Epoch 785/1200\n",
      "133/133 [==============================] - 0s 103us/step - loss: 0.1394 - acc: 0.9925 - val_loss: 0.2440 - val_acc: 0.9333\n",
      "Epoch 786/1200\n",
      "133/133 [==============================] - 0s 99us/step - loss: 0.1381 - acc: 0.9925 - val_loss: 0.2438 - val_acc: 0.9333\n",
      "Epoch 787/1200\n",
      "133/133 [==============================] - 0s 113us/step - loss: 0.1371 - acc: 0.9925 - val_loss: 0.2429 - val_acc: 0.9333\n",
      "Epoch 788/1200\n",
      "133/133 [==============================] - 0s 96us/step - loss: 0.1360 - acc: 0.9925 - val_loss: 0.2418 - val_acc: 0.9333\n",
      "Epoch 789/1200\n",
      "133/133 [==============================] - 0s 101us/step - loss: 0.1350 - acc: 0.9925 - val_loss: 0.2409 - val_acc: 0.9333\n",
      "Epoch 790/1200\n",
      "133/133 [==============================] - 0s 116us/step - loss: 0.1339 - acc: 0.9925 - val_loss: 0.2398 - val_acc: 0.9333\n",
      "Epoch 791/1200\n",
      "133/133 [==============================] - 0s 103us/step - loss: 0.1326 - acc: 0.9925 - val_loss: 0.2394 - val_acc: 0.9333\n",
      "Epoch 792/1200\n",
      "133/133 [==============================] - 0s 112us/step - loss: 0.1318 - acc: 0.9925 - val_loss: 0.2374 - val_acc: 0.9333\n",
      "Epoch 793/1200\n",
      "133/133 [==============================] - 0s 112us/step - loss: 0.1306 - acc: 0.9925 - val_loss: 0.2367 - val_acc: 0.9333\n",
      "Epoch 794/1200\n",
      "133/133 [==============================] - 0s 105us/step - loss: 0.1295 - acc: 0.9925 - val_loss: 0.2355 - val_acc: 0.9333\n",
      "Epoch 795/1200\n",
      "133/133 [==============================] - 0s 109us/step - loss: 0.1287 - acc: 0.9925 - val_loss: 0.2351 - val_acc: 0.9333\n",
      "Epoch 796/1200\n",
      "133/133 [==============================] - 0s 100us/step - loss: 0.1274 - acc: 0.9925 - val_loss: 0.2333 - val_acc: 0.9333\n",
      "Epoch 797/1200\n",
      "133/133 [==============================] - 0s 117us/step - loss: 0.1265 - acc: 0.9925 - val_loss: 0.2319 - val_acc: 0.9333\n",
      "Epoch 798/1200\n",
      "133/133 [==============================] - 0s 106us/step - loss: 0.1253 - acc: 0.9925 - val_loss: 0.2313 - val_acc: 0.9333\n",
      "Epoch 799/1200\n",
      "133/133 [==============================] - 0s 114us/step - loss: 0.1244 - acc: 0.9925 - val_loss: 0.2307 - val_acc: 0.9333\n",
      "Epoch 800/1200\n",
      "133/133 [==============================] - 0s 104us/step - loss: 0.1230 - acc: 0.9925 - val_loss: 0.2298 - val_acc: 0.9333\n",
      "Epoch 801/1200\n",
      "133/133 [==============================] - 0s 108us/step - loss: 0.1221 - acc: 0.9925 - val_loss: 0.2293 - val_acc: 0.9333\n",
      "Epoch 802/1200\n",
      "133/133 [==============================] - 0s 108us/step - loss: 0.1210 - acc: 0.9925 - val_loss: 0.2280 - val_acc: 0.9333\n",
      "Epoch 803/1200\n",
      "133/133 [==============================] - 0s 119us/step - loss: 0.1198 - acc: 0.9925 - val_loss: 0.2273 - val_acc: 0.9333\n",
      "Epoch 804/1200\n",
      "133/133 [==============================] - 0s 113us/step - loss: 0.1190 - acc: 0.9925 - val_loss: 0.2265 - val_acc: 0.9333\n",
      "Epoch 805/1200\n",
      "133/133 [==============================] - 0s 112us/step - loss: 0.1178 - acc: 0.9925 - val_loss: 0.2257 - val_acc: 0.9333\n",
      "Epoch 806/1200\n",
      "133/133 [==============================] - 0s 125us/step - loss: 0.1166 - acc: 0.9925 - val_loss: 0.2258 - val_acc: 0.9333\n",
      "Epoch 807/1200\n",
      "133/133 [==============================] - 0s 111us/step - loss: 0.1159 - acc: 0.9925 - val_loss: 0.2243 - val_acc: 0.9333\n",
      "Epoch 808/1200\n",
      "133/133 [==============================] - 0s 117us/step - loss: 0.1146 - acc: 0.9925 - val_loss: 0.2237 - val_acc: 0.9333\n",
      "Epoch 809/1200\n",
      "133/133 [==============================] - 0s 109us/step - loss: 0.1138 - acc: 0.9925 - val_loss: 0.2224 - val_acc: 0.9333\n",
      "Epoch 810/1200\n",
      "133/133 [==============================] - 0s 109us/step - loss: 0.1126 - acc: 0.9925 - val_loss: 0.2216 - val_acc: 0.9333\n",
      "Epoch 811/1200\n",
      "133/133 [==============================] - 0s 111us/step - loss: 0.1118 - acc: 0.9925 - val_loss: 0.2207 - val_acc: 0.9333\n",
      "Epoch 812/1200\n",
      "133/133 [==============================] - 0s 106us/step - loss: 0.1109 - acc: 0.9925 - val_loss: 0.2203 - val_acc: 0.9333\n",
      "Epoch 813/1200\n",
      "133/133 [==============================] - 0s 113us/step - loss: 0.1098 - acc: 0.9925 - val_loss: 0.2190 - val_acc: 0.9333\n",
      "Epoch 814/1200\n",
      "133/133 [==============================] - 0s 108us/step - loss: 0.1087 - acc: 0.9925 - val_loss: 0.2187 - val_acc: 0.9333\n",
      "Epoch 815/1200\n",
      "133/133 [==============================] - 0s 105us/step - loss: 0.1078 - acc: 0.9925 - val_loss: 0.2178 - val_acc: 0.9333\n",
      "Epoch 816/1200\n",
      "133/133 [==============================] - 0s 114us/step - loss: 0.1069 - acc: 0.9925 - val_loss: 0.2163 - val_acc: 0.9333\n",
      "Epoch 817/1200\n",
      "133/133 [==============================] - 0s 111us/step - loss: 0.1057 - acc: 0.9925 - val_loss: 0.2165 - val_acc: 0.9333\n",
      "Epoch 818/1200\n",
      "133/133 [==============================] - 0s 115us/step - loss: 0.1049 - acc: 0.9925 - val_loss: 0.2158 - val_acc: 0.9333\n",
      "Epoch 819/1200\n",
      "133/133 [==============================] - 0s 106us/step - loss: 0.1038 - acc: 0.9925 - val_loss: 0.2158 - val_acc: 0.9333\n",
      "Epoch 820/1200\n",
      "133/133 [==============================] - 0s 111us/step - loss: 0.1028 - acc: 0.9925 - val_loss: 0.2145 - val_acc: 0.9333\n",
      "Epoch 821/1200\n",
      "133/133 [==============================] - 0s 117us/step - loss: 0.1017 - acc: 0.9925 - val_loss: 0.2142 - val_acc: 0.9333\n",
      "Epoch 822/1200\n",
      "133/133 [==============================] - 0s 94us/step - loss: 0.1008 - acc: 0.9925 - val_loss: 0.2132 - val_acc: 0.9333\n",
      "Epoch 823/1200\n",
      "133/133 [==============================] - 0s 107us/step - loss: 0.0997 - acc: 0.9925 - val_loss: 0.2117 - val_acc: 0.9333\n",
      "Epoch 824/1200\n",
      "133/133 [==============================] - 0s 106us/step - loss: 0.0986 - acc: 0.9925 - val_loss: 0.2108 - val_acc: 0.9333\n",
      "Epoch 825/1200\n",
      "133/133 [==============================] - 0s 112us/step - loss: 0.0978 - acc: 0.9925 - val_loss: 0.2108 - val_acc: 0.9333\n",
      "Epoch 826/1200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133/133 [==============================] - 0s 106us/step - loss: 0.0969 - acc: 0.9925 - val_loss: 0.2098 - val_acc: 0.9333\n",
      "Epoch 827/1200\n",
      "133/133 [==============================] - 0s 113us/step - loss: 0.0959 - acc: 0.9925 - val_loss: 0.2099 - val_acc: 0.9333\n",
      "Epoch 828/1200\n",
      "133/133 [==============================] - 0s 113us/step - loss: 0.0950 - acc: 0.9925 - val_loss: 0.2106 - val_acc: 0.9333\n",
      "Epoch 829/1200\n",
      "133/133 [==============================] - 0s 99us/step - loss: 0.0940 - acc: 0.9925 - val_loss: 0.2097 - val_acc: 0.9333\n",
      "Epoch 830/1200\n",
      "133/133 [==============================] - 0s 104us/step - loss: 0.0933 - acc: 0.9925 - val_loss: 0.2092 - val_acc: 0.9333\n",
      "Epoch 831/1200\n",
      "133/133 [==============================] - 0s 104us/step - loss: 0.0924 - acc: 0.9925 - val_loss: 0.2076 - val_acc: 0.9333\n",
      "Epoch 832/1200\n",
      "133/133 [==============================] - 0s 102us/step - loss: 0.0919 - acc: 0.9925 - val_loss: 0.2067 - val_acc: 0.9333\n",
      "Epoch 833/1200\n",
      "133/133 [==============================] - 0s 115us/step - loss: 0.0908 - acc: 0.9925 - val_loss: 0.2067 - val_acc: 0.9333\n",
      "Epoch 834/1200\n",
      "133/133 [==============================] - 0s 114us/step - loss: 0.0898 - acc: 0.9925 - val_loss: 0.2055 - val_acc: 0.9333\n",
      "Epoch 835/1200\n",
      "133/133 [==============================] - 0s 127us/step - loss: 0.0890 - acc: 0.9925 - val_loss: 0.2049 - val_acc: 0.9333\n",
      "Epoch 836/1200\n",
      "133/133 [==============================] - 0s 126us/step - loss: 0.0883 - acc: 0.9925 - val_loss: 0.2036 - val_acc: 0.9333\n",
      "Epoch 837/1200\n",
      "133/133 [==============================] - 0s 128us/step - loss: 0.0875 - acc: 0.9925 - val_loss: 0.2034 - val_acc: 0.9333\n",
      "Epoch 838/1200\n",
      "133/133 [==============================] - 0s 112us/step - loss: 0.0864 - acc: 0.9925 - val_loss: 0.2028 - val_acc: 0.9333\n",
      "Epoch 839/1200\n",
      "133/133 [==============================] - 0s 111us/step - loss: 0.0857 - acc: 0.9925 - val_loss: 0.2026 - val_acc: 0.9333\n",
      "Epoch 840/1200\n",
      "133/133 [==============================] - 0s 119us/step - loss: 0.0849 - acc: 0.9925 - val_loss: 0.2033 - val_acc: 0.9333\n",
      "Epoch 841/1200\n",
      "133/133 [==============================] - 0s 107us/step - loss: 0.0841 - acc: 0.9925 - val_loss: 0.2026 - val_acc: 0.9333\n",
      "Epoch 842/1200\n",
      "133/133 [==============================] - 0s 108us/step - loss: 0.0832 - acc: 0.9925 - val_loss: 0.2028 - val_acc: 0.9111\n",
      "Epoch 843/1200\n",
      "133/133 [==============================] - 0s 113us/step - loss: 0.0826 - acc: 0.9925 - val_loss: 0.2022 - val_acc: 0.9111\n",
      "Epoch 844/1200\n",
      "133/133 [==============================] - 0s 96us/step - loss: 0.0815 - acc: 1.0000 - val_loss: 0.2011 - val_acc: 0.9111\n",
      "Epoch 845/1200\n",
      "133/133 [==============================] - 0s 113us/step - loss: 0.0812 - acc: 0.9925 - val_loss: 0.2004 - val_acc: 0.9111\n",
      "Epoch 846/1200\n",
      "133/133 [==============================] - 0s 95us/step - loss: 0.0800 - acc: 0.9925 - val_loss: 0.2008 - val_acc: 0.9111\n",
      "Epoch 847/1200\n",
      "133/133 [==============================] - 0s 111us/step - loss: 0.0793 - acc: 1.0000 - val_loss: 0.1999 - val_acc: 0.9111\n",
      "Epoch 848/1200\n",
      "133/133 [==============================] - 0s 98us/step - loss: 0.0786 - acc: 1.0000 - val_loss: 0.1988 - val_acc: 0.9111\n",
      "Epoch 849/1200\n",
      "133/133 [==============================] - 0s 103us/step - loss: 0.0779 - acc: 1.0000 - val_loss: 0.1983 - val_acc: 0.9111\n",
      "Epoch 850/1200\n",
      "133/133 [==============================] - 0s 93us/step - loss: 0.0771 - acc: 0.9925 - val_loss: 0.1988 - val_acc: 0.9111\n",
      "Epoch 851/1200\n",
      "133/133 [==============================] - 0s 101us/step - loss: 0.0764 - acc: 0.9925 - val_loss: 0.1989 - val_acc: 0.9111\n",
      "Epoch 852/1200\n",
      "133/133 [==============================] - 0s 108us/step - loss: 0.0756 - acc: 0.9925 - val_loss: 0.1983 - val_acc: 0.9111\n",
      "Epoch 853/1200\n",
      "133/133 [==============================] - 0s 108us/step - loss: 0.0747 - acc: 1.0000 - val_loss: 0.1964 - val_acc: 0.9111\n",
      "Epoch 854/1200\n",
      "133/133 [==============================] - 0s 104us/step - loss: 0.0741 - acc: 1.0000 - val_loss: 0.1953 - val_acc: 0.9111\n",
      "Epoch 855/1200\n",
      "133/133 [==============================] - 0s 109us/step - loss: 0.0735 - acc: 1.0000 - val_loss: 0.1955 - val_acc: 0.9111\n",
      "Epoch 856/1200\n",
      "133/133 [==============================] - 0s 105us/step - loss: 0.0728 - acc: 1.0000 - val_loss: 0.1953 - val_acc: 0.9111\n",
      "Epoch 857/1200\n",
      "133/133 [==============================] - 0s 115us/step - loss: 0.0720 - acc: 1.0000 - val_loss: 0.1954 - val_acc: 0.9111\n",
      "Epoch 858/1200\n",
      "133/133 [==============================] - 0s 108us/step - loss: 0.0713 - acc: 1.0000 - val_loss: 0.1945 - val_acc: 0.9111\n",
      "Epoch 859/1200\n",
      "133/133 [==============================] - 0s 110us/step - loss: 0.0706 - acc: 1.0000 - val_loss: 0.1955 - val_acc: 0.9111\n",
      "Epoch 860/1200\n",
      "133/133 [==============================] - 0s 117us/step - loss: 0.0700 - acc: 1.0000 - val_loss: 0.1959 - val_acc: 0.9111\n",
      "Epoch 861/1200\n",
      "133/133 [==============================] - 0s 106us/step - loss: 0.0693 - acc: 1.0000 - val_loss: 0.1951 - val_acc: 0.9111\n",
      "Epoch 862/1200\n",
      "133/133 [==============================] - 0s 107us/step - loss: 0.0686 - acc: 1.0000 - val_loss: 0.1953 - val_acc: 0.9111\n",
      "Epoch 863/1200\n",
      "133/133 [==============================] - 0s 107us/step - loss: 0.0681 - acc: 1.0000 - val_loss: 0.1940 - val_acc: 0.9111\n",
      "Epoch 864/1200\n",
      "133/133 [==============================] - 0s 123us/step - loss: 0.0674 - acc: 1.0000 - val_loss: 0.1936 - val_acc: 0.9111\n",
      "Epoch 865/1200\n",
      "133/133 [==============================] - 0s 120us/step - loss: 0.0667 - acc: 1.0000 - val_loss: 0.1921 - val_acc: 0.9111\n",
      "Epoch 866/1200\n",
      "133/133 [==============================] - 0s 103us/step - loss: 0.0661 - acc: 1.0000 - val_loss: 0.1916 - val_acc: 0.9111\n",
      "Epoch 867/1200\n",
      "133/133 [==============================] - 0s 121us/step - loss: 0.0657 - acc: 1.0000 - val_loss: 0.1918 - val_acc: 0.9111\n",
      "Epoch 868/1200\n",
      "133/133 [==============================] - 0s 110us/step - loss: 0.0649 - acc: 1.0000 - val_loss: 0.1910 - val_acc: 0.9111\n",
      "Epoch 869/1200\n",
      "133/133 [==============================] - 0s 107us/step - loss: 0.0641 - acc: 1.0000 - val_loss: 0.1914 - val_acc: 0.9111\n",
      "Epoch 870/1200\n",
      "133/133 [==============================] - 0s 112us/step - loss: 0.0636 - acc: 1.0000 - val_loss: 0.1898 - val_acc: 0.9111\n",
      "Epoch 871/1200\n",
      "133/133 [==============================] - 0s 134us/step - loss: 0.0628 - acc: 1.0000 - val_loss: 0.1892 - val_acc: 0.9111\n",
      "Epoch 872/1200\n",
      "133/133 [==============================] - 0s 120us/step - loss: 0.0625 - acc: 1.0000 - val_loss: 0.1895 - val_acc: 0.9111\n",
      "Epoch 873/1200\n",
      "133/133 [==============================] - 0s 106us/step - loss: 0.0617 - acc: 1.0000 - val_loss: 0.1895 - val_acc: 0.9111\n",
      "Epoch 874/1200\n",
      "133/133 [==============================] - 0s 111us/step - loss: 0.0612 - acc: 1.0000 - val_loss: 0.1910 - val_acc: 0.9111\n",
      "Epoch 875/1200\n",
      "133/133 [==============================] - 0s 106us/step - loss: 0.0609 - acc: 1.0000 - val_loss: 0.1903 - val_acc: 0.9111\n",
      "Epoch 876/1200\n",
      "133/133 [==============================] - 0s 109us/step - loss: 0.0601 - acc: 1.0000 - val_loss: 0.1902 - val_acc: 0.9111\n",
      "Epoch 877/1200\n",
      "133/133 [==============================] - 0s 118us/step - loss: 0.0596 - acc: 1.0000 - val_loss: 0.1899 - val_acc: 0.9111\n",
      "Epoch 878/1200\n",
      "133/133 [==============================] - 0s 106us/step - loss: 0.0590 - acc: 1.0000 - val_loss: 0.1899 - val_acc: 0.9111\n",
      "Epoch 879/1200\n",
      "133/133 [==============================] - 0s 117us/step - loss: 0.0585 - acc: 1.0000 - val_loss: 0.1896 - val_acc: 0.9111\n",
      "Epoch 880/1200\n",
      "133/133 [==============================] - 0s 120us/step - loss: 0.0579 - acc: 1.0000 - val_loss: 0.1889 - val_acc: 0.9111\n",
      "Epoch 881/1200\n",
      "133/133 [==============================] - 0s 173us/step - loss: 0.0576 - acc: 1.0000 - val_loss: 0.1879 - val_acc: 0.9111\n",
      "Epoch 882/1200\n",
      "133/133 [==============================] - 0s 122us/step - loss: 0.0570 - acc: 1.0000 - val_loss: 0.1885 - val_acc: 0.9111\n",
      "Epoch 883/1200\n",
      "133/133 [==============================] - 0s 130us/step - loss: 0.0563 - acc: 1.0000 - val_loss: 0.1871 - val_acc: 0.9111\n",
      "Epoch 884/1200\n",
      "133/133 [==============================] - 0s 105us/step - loss: 0.0560 - acc: 1.0000 - val_loss: 0.1881 - val_acc: 0.9111\n",
      "Epoch 885/1200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133/133 [==============================] - 0s 117us/step - loss: 0.0552 - acc: 1.0000 - val_loss: 0.1881 - val_acc: 0.9111\n",
      "Epoch 886/1200\n",
      "133/133 [==============================] - 0s 101us/step - loss: 0.0546 - acc: 1.0000 - val_loss: 0.1867 - val_acc: 0.9111\n",
      "Epoch 887/1200\n",
      "133/133 [==============================] - 0s 120us/step - loss: 0.0544 - acc: 1.0000 - val_loss: 0.1861 - val_acc: 0.9111\n",
      "Epoch 888/1200\n",
      "133/133 [==============================] - 0s 107us/step - loss: 0.0539 - acc: 1.0000 - val_loss: 0.1857 - val_acc: 0.9111\n",
      "Epoch 889/1200\n",
      "133/133 [==============================] - 0s 107us/step - loss: 0.0532 - acc: 1.0000 - val_loss: 0.1850 - val_acc: 0.9111\n",
      "Epoch 890/1200\n",
      "133/133 [==============================] - 0s 107us/step - loss: 0.0529 - acc: 1.0000 - val_loss: 0.1850 - val_acc: 0.9111\n",
      "Epoch 891/1200\n",
      "133/133 [==============================] - 0s 104us/step - loss: 0.0522 - acc: 1.0000 - val_loss: 0.1862 - val_acc: 0.9111\n",
      "Epoch 892/1200\n",
      "133/133 [==============================] - 0s 103us/step - loss: 0.0517 - acc: 1.0000 - val_loss: 0.1866 - val_acc: 0.9111\n",
      "Epoch 893/1200\n",
      "133/133 [==============================] - 0s 114us/step - loss: 0.0513 - acc: 1.0000 - val_loss: 0.1851 - val_acc: 0.9111\n",
      "Epoch 894/1200\n",
      "133/133 [==============================] - 0s 136us/step - loss: 0.0510 - acc: 1.0000 - val_loss: 0.1851 - val_acc: 0.9111\n",
      "Epoch 895/1200\n",
      "133/133 [==============================] - 0s 115us/step - loss: 0.0505 - acc: 1.0000 - val_loss: 0.1852 - val_acc: 0.9111\n",
      "Epoch 896/1200\n",
      "133/133 [==============================] - 0s 107us/step - loss: 0.0498 - acc: 1.0000 - val_loss: 0.1854 - val_acc: 0.9111\n",
      "Epoch 897/1200\n",
      "133/133 [==============================] - 0s 100us/step - loss: 0.0497 - acc: 1.0000 - val_loss: 0.1854 - val_acc: 0.9111\n",
      "Epoch 898/1200\n",
      "133/133 [==============================] - 0s 106us/step - loss: 0.0490 - acc: 1.0000 - val_loss: 0.1863 - val_acc: 0.9111\n",
      "Epoch 899/1200\n",
      "133/133 [==============================] - 0s 111us/step - loss: 0.0488 - acc: 1.0000 - val_loss: 0.1861 - val_acc: 0.9111\n",
      "Epoch 900/1200\n",
      "133/133 [==============================] - 0s 102us/step - loss: 0.0480 - acc: 1.0000 - val_loss: 0.1844 - val_acc: 0.9111\n",
      "Epoch 901/1200\n",
      "133/133 [==============================] - 0s 113us/step - loss: 0.0478 - acc: 1.0000 - val_loss: 0.1851 - val_acc: 0.9111\n",
      "Epoch 902/1200\n",
      "133/133 [==============================] - 0s 113us/step - loss: 0.0472 - acc: 1.0000 - val_loss: 0.1848 - val_acc: 0.9111\n",
      "Epoch 903/1200\n",
      "133/133 [==============================] - 0s 118us/step - loss: 0.0470 - acc: 1.0000 - val_loss: 0.1835 - val_acc: 0.9111\n",
      "Epoch 904/1200\n",
      "133/133 [==============================] - 0s 158us/step - loss: 0.0465 - acc: 1.0000 - val_loss: 0.1842 - val_acc: 0.9111\n",
      "Epoch 905/1200\n",
      "133/133 [==============================] - 0s 117us/step - loss: 0.0463 - acc: 1.0000 - val_loss: 0.1844 - val_acc: 0.9111\n",
      "Epoch 906/1200\n",
      "133/133 [==============================] - 0s 124us/step - loss: 0.0458 - acc: 1.0000 - val_loss: 0.1837 - val_acc: 0.9111\n",
      "Epoch 907/1200\n",
      "133/133 [==============================] - 0s 120us/step - loss: 0.0454 - acc: 1.0000 - val_loss: 0.1832 - val_acc: 0.9111\n",
      "Epoch 908/1200\n",
      "133/133 [==============================] - 0s 116us/step - loss: 0.0450 - acc: 1.0000 - val_loss: 0.1837 - val_acc: 0.9111\n",
      "Epoch 909/1200\n",
      "133/133 [==============================] - 0s 122us/step - loss: 0.0446 - acc: 1.0000 - val_loss: 0.1843 - val_acc: 0.9111\n",
      "Epoch 910/1200\n",
      "133/133 [==============================] - 0s 115us/step - loss: 0.0442 - acc: 1.0000 - val_loss: 0.1841 - val_acc: 0.9111\n",
      "Epoch 911/1200\n",
      "133/133 [==============================] - 0s 122us/step - loss: 0.0439 - acc: 1.0000 - val_loss: 0.1847 - val_acc: 0.9111\n",
      "Epoch 912/1200\n",
      "133/133 [==============================] - 0s 108us/step - loss: 0.0434 - acc: 1.0000 - val_loss: 0.1854 - val_acc: 0.9111\n",
      "Epoch 913/1200\n",
      "133/133 [==============================] - 0s 118us/step - loss: 0.0430 - acc: 1.0000 - val_loss: 0.1852 - val_acc: 0.9111\n",
      "Epoch 914/1200\n",
      "133/133 [==============================] - 0s 119us/step - loss: 0.0427 - acc: 1.0000 - val_loss: 0.1851 - val_acc: 0.9111\n",
      "Epoch 915/1200\n",
      "133/133 [==============================] - 0s 110us/step - loss: 0.0422 - acc: 1.0000 - val_loss: 0.1845 - val_acc: 0.9111\n",
      "Epoch 916/1200\n",
      "133/133 [==============================] - 0s 102us/step - loss: 0.0421 - acc: 1.0000 - val_loss: 0.1843 - val_acc: 0.9111\n",
      "Epoch 917/1200\n",
      "133/133 [==============================] - 0s 115us/step - loss: 0.0416 - acc: 1.0000 - val_loss: 0.1845 - val_acc: 0.9111\n",
      "Epoch 918/1200\n",
      "133/133 [==============================] - 0s 111us/step - loss: 0.0412 - acc: 1.0000 - val_loss: 0.1853 - val_acc: 0.9111\n",
      "Epoch 919/1200\n",
      "133/133 [==============================] - 0s 111us/step - loss: 0.0408 - acc: 1.0000 - val_loss: 0.1858 - val_acc: 0.9111\n",
      "Epoch 920/1200\n",
      "133/133 [==============================] - 0s 112us/step - loss: 0.0406 - acc: 1.0000 - val_loss: 0.1861 - val_acc: 0.9111\n",
      "Epoch 921/1200\n",
      "133/133 [==============================] - 0s 107us/step - loss: 0.0401 - acc: 1.0000 - val_loss: 0.1857 - val_acc: 0.9111\n",
      "Epoch 922/1200\n",
      "133/133 [==============================] - 0s 114us/step - loss: 0.0399 - acc: 1.0000 - val_loss: 0.1851 - val_acc: 0.9111\n",
      "Epoch 923/1200\n",
      "133/133 [==============================] - 0s 130us/step - loss: 0.0395 - acc: 1.0000 - val_loss: 0.1832 - val_acc: 0.9111\n",
      "Epoch 924/1200\n",
      "133/133 [==============================] - 0s 127us/step - loss: 0.0390 - acc: 1.0000 - val_loss: 0.1836 - val_acc: 0.9111\n",
      "Epoch 925/1200\n",
      "133/133 [==============================] - 0s 114us/step - loss: 0.0387 - acc: 1.0000 - val_loss: 0.1841 - val_acc: 0.9111\n",
      "Epoch 926/1200\n",
      "133/133 [==============================] - 0s 97us/step - loss: 0.0382 - acc: 1.0000 - val_loss: 0.1852 - val_acc: 0.9111\n",
      "Epoch 927/1200\n",
      "133/133 [==============================] - 0s 102us/step - loss: 0.0382 - acc: 1.0000 - val_loss: 0.1842 - val_acc: 0.9111\n",
      "Epoch 928/1200\n",
      "133/133 [==============================] - 0s 113us/step - loss: 0.0378 - acc: 1.0000 - val_loss: 0.1844 - val_acc: 0.9111\n",
      "Epoch 929/1200\n",
      "133/133 [==============================] - 0s 111us/step - loss: 0.0374 - acc: 1.0000 - val_loss: 0.1844 - val_acc: 0.9111\n",
      "Epoch 930/1200\n",
      "133/133 [==============================] - 0s 109us/step - loss: 0.0370 - acc: 1.0000 - val_loss: 0.1834 - val_acc: 0.9111\n",
      "Epoch 931/1200\n",
      "133/133 [==============================] - 0s 114us/step - loss: 0.0366 - acc: 1.0000 - val_loss: 0.1822 - val_acc: 0.9111\n",
      "Epoch 932/1200\n",
      "133/133 [==============================] - 0s 117us/step - loss: 0.0365 - acc: 1.0000 - val_loss: 0.1829 - val_acc: 0.9111\n",
      "Epoch 933/1200\n",
      "133/133 [==============================] - 0s 112us/step - loss: 0.0362 - acc: 1.0000 - val_loss: 0.1845 - val_acc: 0.9111\n",
      "Epoch 934/1200\n",
      "133/133 [==============================] - 0s 131us/step - loss: 0.0359 - acc: 1.0000 - val_loss: 0.1837 - val_acc: 0.9111\n",
      "Epoch 935/1200\n",
      "133/133 [==============================] - 0s 123us/step - loss: 0.0356 - acc: 1.0000 - val_loss: 0.1842 - val_acc: 0.9111\n",
      "Epoch 936/1200\n",
      "133/133 [==============================] - 0s 112us/step - loss: 0.0352 - acc: 1.0000 - val_loss: 0.1858 - val_acc: 0.9111\n",
      "Epoch 937/1200\n",
      "133/133 [==============================] - 0s 109us/step - loss: 0.0350 - acc: 1.0000 - val_loss: 0.1859 - val_acc: 0.9111\n",
      "Epoch 938/1200\n",
      "133/133 [==============================] - 0s 156us/step - loss: 0.0346 - acc: 1.0000 - val_loss: 0.1842 - val_acc: 0.9111\n",
      "Epoch 939/1200\n",
      "133/133 [==============================] - 0s 113us/step - loss: 0.0343 - acc: 1.0000 - val_loss: 0.1825 - val_acc: 0.9111\n",
      "Epoch 940/1200\n",
      "133/133 [==============================] - 0s 115us/step - loss: 0.0342 - acc: 1.0000 - val_loss: 0.1838 - val_acc: 0.9111\n",
      "Epoch 941/1200\n",
      "133/133 [==============================] - 0s 117us/step - loss: 0.0339 - acc: 1.0000 - val_loss: 0.1855 - val_acc: 0.9111\n",
      "Epoch 942/1200\n",
      "133/133 [==============================] - 0s 104us/step - loss: 0.0333 - acc: 1.0000 - val_loss: 0.1850 - val_acc: 0.9111\n",
      "Epoch 943/1200\n",
      "133/133 [==============================] - 0s 102us/step - loss: 0.0333 - acc: 1.0000 - val_loss: 0.1844 - val_acc: 0.9111\n",
      "Epoch 944/1200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133/133 [==============================] - 0s 99us/step - loss: 0.0331 - acc: 1.0000 - val_loss: 0.1842 - val_acc: 0.9111\n",
      "Epoch 945/1200\n",
      "133/133 [==============================] - 0s 95us/step - loss: 0.0326 - acc: 1.0000 - val_loss: 0.1858 - val_acc: 0.9111\n",
      "Epoch 946/1200\n",
      "133/133 [==============================] - 0s 101us/step - loss: 0.0326 - acc: 1.0000 - val_loss: 0.1860 - val_acc: 0.9111\n",
      "Epoch 947/1200\n",
      "133/133 [==============================] - 0s 98us/step - loss: 0.0321 - acc: 1.0000 - val_loss: 0.1846 - val_acc: 0.9111\n",
      "Epoch 948/1200\n",
      "133/133 [==============================] - 0s 98us/step - loss: 0.0318 - acc: 1.0000 - val_loss: 0.1850 - val_acc: 0.9111\n",
      "Epoch 949/1200\n",
      "133/133 [==============================] - 0s 108us/step - loss: 0.0316 - acc: 1.0000 - val_loss: 0.1857 - val_acc: 0.9111\n",
      "Epoch 950/1200\n",
      "133/133 [==============================] - 0s 95us/step - loss: 0.0311 - acc: 1.0000 - val_loss: 0.1829 - val_acc: 0.9111\n",
      "Epoch 951/1200\n",
      "133/133 [==============================] - 0s 97us/step - loss: 0.0310 - acc: 1.0000 - val_loss: 0.1830 - val_acc: 0.9111\n",
      "Epoch 952/1200\n",
      "133/133 [==============================] - 0s 106us/step - loss: 0.0308 - acc: 1.0000 - val_loss: 0.1832 - val_acc: 0.9111\n",
      "Epoch 953/1200\n",
      "133/133 [==============================] - 0s 125us/step - loss: 0.0305 - acc: 1.0000 - val_loss: 0.1851 - val_acc: 0.9111\n",
      "Epoch 954/1200\n",
      "133/133 [==============================] - 0s 115us/step - loss: 0.0301 - acc: 1.0000 - val_loss: 0.1870 - val_acc: 0.9111\n",
      "Epoch 955/1200\n",
      "133/133 [==============================] - 0s 121us/step - loss: 0.0300 - acc: 1.0000 - val_loss: 0.1860 - val_acc: 0.9111\n",
      "Epoch 956/1200\n",
      "133/133 [==============================] - 0s 105us/step - loss: 0.0297 - acc: 1.0000 - val_loss: 0.1858 - val_acc: 0.9111\n",
      "Epoch 957/1200\n",
      "133/133 [==============================] - 0s 98us/step - loss: 0.0295 - acc: 1.0000 - val_loss: 0.1870 - val_acc: 0.9111\n",
      "Epoch 958/1200\n",
      "133/133 [==============================] - 0s 99us/step - loss: 0.0292 - acc: 1.0000 - val_loss: 0.1852 - val_acc: 0.9111\n",
      "Epoch 959/1200\n",
      "133/133 [==============================] - 0s 101us/step - loss: 0.0290 - acc: 1.0000 - val_loss: 0.1852 - val_acc: 0.9111\n",
      "Epoch 960/1200\n",
      "133/133 [==============================] - 0s 104us/step - loss: 0.0286 - acc: 1.0000 - val_loss: 0.1851 - val_acc: 0.9111\n",
      "Epoch 961/1200\n",
      "133/133 [==============================] - 0s 106us/step - loss: 0.0284 - acc: 1.0000 - val_loss: 0.1854 - val_acc: 0.9111\n",
      "Epoch 962/1200\n",
      "133/133 [==============================] - 0s 95us/step - loss: 0.0281 - acc: 1.0000 - val_loss: 0.1844 - val_acc: 0.9111\n",
      "Epoch 963/1200\n",
      "133/133 [==============================] - 0s 98us/step - loss: 0.0280 - acc: 1.0000 - val_loss: 0.1851 - val_acc: 0.9111\n",
      "Epoch 964/1200\n",
      "133/133 [==============================] - 0s 91us/step - loss: 0.0277 - acc: 1.0000 - val_loss: 0.1862 - val_acc: 0.9111\n",
      "Epoch 965/1200\n",
      "133/133 [==============================] - 0s 97us/step - loss: 0.0275 - acc: 1.0000 - val_loss: 0.1851 - val_acc: 0.9111\n",
      "Epoch 966/1200\n",
      "133/133 [==============================] - 0s 104us/step - loss: 0.0272 - acc: 1.0000 - val_loss: 0.1846 - val_acc: 0.9111\n",
      "Epoch 967/1200\n",
      "133/133 [==============================] - 0s 105us/step - loss: 0.0271 - acc: 1.0000 - val_loss: 0.1853 - val_acc: 0.9111\n",
      "Epoch 968/1200\n",
      "133/133 [==============================] - 0s 103us/step - loss: 0.0270 - acc: 1.0000 - val_loss: 0.1865 - val_acc: 0.9111\n",
      "Epoch 969/1200\n",
      "133/133 [==============================] - 0s 114us/step - loss: 0.0265 - acc: 1.0000 - val_loss: 0.1859 - val_acc: 0.9111\n",
      "Epoch 970/1200\n",
      "133/133 [==============================] - 0s 95us/step - loss: 0.0264 - acc: 1.0000 - val_loss: 0.1872 - val_acc: 0.9111\n",
      "Epoch 971/1200\n",
      "133/133 [==============================] - 0s 104us/step - loss: 0.0263 - acc: 1.0000 - val_loss: 0.1873 - val_acc: 0.9111\n",
      "Epoch 972/1200\n",
      "133/133 [==============================] - 0s 111us/step - loss: 0.0260 - acc: 1.0000 - val_loss: 0.1877 - val_acc: 0.9111\n",
      "Epoch 973/1200\n",
      "133/133 [==============================] - 0s 105us/step - loss: 0.0258 - acc: 1.0000 - val_loss: 0.1870 - val_acc: 0.9111\n",
      "Epoch 974/1200\n",
      "133/133 [==============================] - 0s 98us/step - loss: 0.0257 - acc: 1.0000 - val_loss: 0.1859 - val_acc: 0.9111\n",
      "Epoch 975/1200\n",
      "133/133 [==============================] - 0s 103us/step - loss: 0.0254 - acc: 1.0000 - val_loss: 0.1855 - val_acc: 0.9111\n",
      "Epoch 976/1200\n",
      "133/133 [==============================] - 0s 110us/step - loss: 0.0251 - acc: 1.0000 - val_loss: 0.1882 - val_acc: 0.9111\n",
      "Epoch 977/1200\n",
      "133/133 [==============================] - 0s 109us/step - loss: 0.0248 - acc: 1.0000 - val_loss: 0.1857 - val_acc: 0.9111\n",
      "Epoch 978/1200\n",
      "133/133 [==============================] - 0s 97us/step - loss: 0.0249 - acc: 1.0000 - val_loss: 0.1864 - val_acc: 0.9111\n",
      "Epoch 979/1200\n",
      "133/133 [==============================] - 0s 102us/step - loss: 0.0246 - acc: 1.0000 - val_loss: 0.1887 - val_acc: 0.9111\n",
      "Epoch 980/1200\n",
      "133/133 [==============================] - 0s 94us/step - loss: 0.0242 - acc: 1.0000 - val_loss: 0.1874 - val_acc: 0.9111\n",
      "Epoch 981/1200\n",
      "133/133 [==============================] - 0s 98us/step - loss: 0.0244 - acc: 1.0000 - val_loss: 0.1883 - val_acc: 0.9111\n",
      "Epoch 982/1200\n",
      "133/133 [==============================] - 0s 97us/step - loss: 0.0240 - acc: 1.0000 - val_loss: 0.1890 - val_acc: 0.9111\n",
      "Epoch 983/1200\n",
      "133/133 [==============================] - 0s 106us/step - loss: 0.0238 - acc: 1.0000 - val_loss: 0.1906 - val_acc: 0.9111\n",
      "Epoch 984/1200\n",
      "133/133 [==============================] - 0s 99us/step - loss: 0.0236 - acc: 1.0000 - val_loss: 0.1885 - val_acc: 0.9111\n",
      "Epoch 985/1200\n",
      "133/133 [==============================] - 0s 129us/step - loss: 0.0233 - acc: 1.0000 - val_loss: 0.1884 - val_acc: 0.9111\n",
      "Epoch 986/1200\n",
      "133/133 [==============================] - 0s 107us/step - loss: 0.0232 - acc: 1.0000 - val_loss: 0.1882 - val_acc: 0.9111\n",
      "Epoch 987/1200\n",
      "133/133 [==============================] - 0s 105us/step - loss: 0.0230 - acc: 1.0000 - val_loss: 0.1900 - val_acc: 0.9111\n",
      "Epoch 988/1200\n",
      "133/133 [==============================] - 0s 102us/step - loss: 0.0227 - acc: 1.0000 - val_loss: 0.1901 - val_acc: 0.9111\n",
      "Epoch 989/1200\n",
      "133/133 [==============================] - 0s 103us/step - loss: 0.0227 - acc: 1.0000 - val_loss: 0.1883 - val_acc: 0.9111\n",
      "Epoch 990/1200\n",
      "133/133 [==============================] - 0s 113us/step - loss: 0.0224 - acc: 1.0000 - val_loss: 0.1877 - val_acc: 0.9111\n",
      "Epoch 991/1200\n",
      "133/133 [==============================] - 0s 102us/step - loss: 0.0222 - acc: 1.0000 - val_loss: 0.1899 - val_acc: 0.9111\n",
      "Epoch 992/1200\n",
      "133/133 [==============================] - 0s 98us/step - loss: 0.0220 - acc: 1.0000 - val_loss: 0.1906 - val_acc: 0.9111\n",
      "Epoch 993/1200\n",
      "133/133 [==============================] - 0s 111us/step - loss: 0.0221 - acc: 1.0000 - val_loss: 0.1898 - val_acc: 0.9111\n",
      "Epoch 994/1200\n",
      "133/133 [==============================] - 0s 103us/step - loss: 0.0218 - acc: 1.0000 - val_loss: 0.1891 - val_acc: 0.9111\n",
      "Epoch 995/1200\n",
      "133/133 [==============================] - 0s 135us/step - loss: 0.0215 - acc: 1.0000 - val_loss: 0.1879 - val_acc: 0.9111\n",
      "Epoch 996/1200\n",
      "133/133 [==============================] - 0s 121us/step - loss: 0.0213 - acc: 1.0000 - val_loss: 0.1906 - val_acc: 0.9111\n",
      "Epoch 997/1200\n",
      "133/133 [==============================] - 0s 115us/step - loss: 0.0212 - acc: 1.0000 - val_loss: 0.1897 - val_acc: 0.9111\n",
      "Epoch 998/1200\n",
      "133/133 [==============================] - 0s 128us/step - loss: 0.0209 - acc: 1.0000 - val_loss: 0.1909 - val_acc: 0.9111\n",
      "Epoch 999/1200\n",
      "133/133 [==============================] - 0s 135us/step - loss: 0.0207 - acc: 1.0000 - val_loss: 0.1903 - val_acc: 0.9111\n",
      "Epoch 1000/1200\n",
      "133/133 [==============================] - 0s 127us/step - loss: 0.0207 - acc: 1.0000 - val_loss: 0.1893 - val_acc: 0.9111\n",
      "Epoch 1001/1200\n",
      "133/133 [==============================] - 0s 125us/step - loss: 0.0205 - acc: 1.0000 - val_loss: 0.1887 - val_acc: 0.9111\n",
      "Epoch 1002/1200\n",
      "133/133 [==============================] - 0s 102us/step - loss: 0.0203 - acc: 1.0000 - val_loss: 0.1907 - val_acc: 0.9111\n",
      "Epoch 1003/1200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133/133 [==============================] - 0s 103us/step - loss: 0.0201 - acc: 1.0000 - val_loss: 0.1908 - val_acc: 0.9111\n",
      "Epoch 1004/1200\n",
      "133/133 [==============================] - 0s 106us/step - loss: 0.0199 - acc: 1.0000 - val_loss: 0.1916 - val_acc: 0.9111\n",
      "Epoch 1005/1200\n",
      "133/133 [==============================] - 0s 123us/step - loss: 0.0198 - acc: 1.0000 - val_loss: 0.1900 - val_acc: 0.9111\n",
      "Epoch 1006/1200\n",
      "133/133 [==============================] - 0s 106us/step - loss: 0.0196 - acc: 1.0000 - val_loss: 0.1911 - val_acc: 0.9111\n",
      "Epoch 1007/1200\n",
      "133/133 [==============================] - 0s 98us/step - loss: 0.0195 - acc: 1.0000 - val_loss: 0.1907 - val_acc: 0.9111\n",
      "Epoch 1008/1200\n",
      "133/133 [==============================] - 0s 101us/step - loss: 0.0193 - acc: 1.0000 - val_loss: 0.1934 - val_acc: 0.9111\n",
      "Epoch 1009/1200\n",
      "133/133 [==============================] - 0s 106us/step - loss: 0.0192 - acc: 1.0000 - val_loss: 0.1911 - val_acc: 0.9111\n",
      "Epoch 1010/1200\n",
      "133/133 [==============================] - 0s 104us/step - loss: 0.0192 - acc: 1.0000 - val_loss: 0.1919 - val_acc: 0.9111\n",
      "Epoch 1011/1200\n",
      "133/133 [==============================] - 0s 100us/step - loss: 0.0187 - acc: 1.0000 - val_loss: 0.1913 - val_acc: 0.9111\n",
      "Epoch 1012/1200\n",
      "133/133 [==============================] - 0s 115us/step - loss: 0.0188 - acc: 1.0000 - val_loss: 0.1922 - val_acc: 0.9111\n",
      "Epoch 1013/1200\n",
      "133/133 [==============================] - 0s 114us/step - loss: 0.0185 - acc: 1.0000 - val_loss: 0.1930 - val_acc: 0.9111\n",
      "Epoch 1014/1200\n",
      "133/133 [==============================] - 0s 111us/step - loss: 0.0183 - acc: 1.0000 - val_loss: 0.1900 - val_acc: 0.9111\n",
      "Epoch 1015/1200\n",
      "133/133 [==============================] - 0s 119us/step - loss: 0.0185 - acc: 1.0000 - val_loss: 0.1903 - val_acc: 0.9111\n",
      "Epoch 1016/1200\n",
      "133/133 [==============================] - 0s 118us/step - loss: 0.0181 - acc: 1.0000 - val_loss: 0.1895 - val_acc: 0.9111\n",
      "Epoch 1017/1200\n",
      "133/133 [==============================] - 0s 115us/step - loss: 0.0179 - acc: 1.0000 - val_loss: 0.1909 - val_acc: 0.9111\n",
      "Epoch 1018/1200\n",
      "133/133 [==============================] - 0s 126us/step - loss: 0.0179 - acc: 1.0000 - val_loss: 0.1908 - val_acc: 0.9111\n",
      "Epoch 1019/1200\n",
      "133/133 [==============================] - 0s 124us/step - loss: 0.0178 - acc: 1.0000 - val_loss: 0.1922 - val_acc: 0.9111\n",
      "Epoch 1020/1200\n",
      "133/133 [==============================] - 0s 124us/step - loss: 0.0176 - acc: 1.0000 - val_loss: 0.1926 - val_acc: 0.9111\n",
      "Epoch 1021/1200\n",
      "133/133 [==============================] - 0s 124us/step - loss: 0.0175 - acc: 1.0000 - val_loss: 0.1919 - val_acc: 0.9111\n",
      "Epoch 1022/1200\n",
      "133/133 [==============================] - 0s 130us/step - loss: 0.0174 - acc: 1.0000 - val_loss: 0.1916 - val_acc: 0.9111\n",
      "Epoch 1023/1200\n",
      "133/133 [==============================] - 0s 102us/step - loss: 0.0173 - acc: 1.0000 - val_loss: 0.1909 - val_acc: 0.9111\n",
      "Epoch 1024/1200\n",
      "133/133 [==============================] - 0s 109us/step - loss: 0.0170 - acc: 1.0000 - val_loss: 0.1905 - val_acc: 0.9111\n",
      "Epoch 1025/1200\n",
      "133/133 [==============================] - 0s 110us/step - loss: 0.0169 - acc: 1.0000 - val_loss: 0.1907 - val_acc: 0.9111\n",
      "Epoch 1026/1200\n",
      "133/133 [==============================] - 0s 111us/step - loss: 0.0168 - acc: 1.0000 - val_loss: 0.1919 - val_acc: 0.9111\n",
      "Epoch 1027/1200\n",
      "133/133 [==============================] - 0s 118us/step - loss: 0.0167 - acc: 1.0000 - val_loss: 0.1932 - val_acc: 0.9111\n",
      "Epoch 1028/1200\n",
      "133/133 [==============================] - 0s 119us/step - loss: 0.0165 - acc: 1.0000 - val_loss: 0.1931 - val_acc: 0.9111\n",
      "Epoch 1029/1200\n",
      "133/133 [==============================] - 0s 123us/step - loss: 0.0164 - acc: 1.0000 - val_loss: 0.1939 - val_acc: 0.9111\n",
      "Epoch 1030/1200\n",
      "133/133 [==============================] - 0s 114us/step - loss: 0.0162 - acc: 1.0000 - val_loss: 0.1935 - val_acc: 0.9111\n",
      "Epoch 1031/1200\n",
      "133/133 [==============================] - 0s 112us/step - loss: 0.0162 - acc: 1.0000 - val_loss: 0.1948 - val_acc: 0.9111\n",
      "Epoch 1032/1200\n",
      "133/133 [==============================] - 0s 108us/step - loss: 0.0160 - acc: 1.0000 - val_loss: 0.1945 - val_acc: 0.9111\n",
      "Epoch 1033/1200\n",
      "133/133 [==============================] - 0s 102us/step - loss: 0.0158 - acc: 1.0000 - val_loss: 0.1953 - val_acc: 0.9111\n",
      "Epoch 1034/1200\n",
      "133/133 [==============================] - 0s 107us/step - loss: 0.0158 - acc: 1.0000 - val_loss: 0.1960 - val_acc: 0.9111\n",
      "Epoch 1035/1200\n",
      "133/133 [==============================] - 0s 103us/step - loss: 0.0156 - acc: 1.0000 - val_loss: 0.1974 - val_acc: 0.9111\n",
      "Epoch 1036/1200\n",
      "133/133 [==============================] - 0s 100us/step - loss: 0.0155 - acc: 1.0000 - val_loss: 0.1959 - val_acc: 0.9111\n",
      "Epoch 1037/1200\n",
      "133/133 [==============================] - 0s 96us/step - loss: 0.0152 - acc: 1.0000 - val_loss: 0.1947 - val_acc: 0.9111\n",
      "Epoch 1038/1200\n",
      "133/133 [==============================] - 0s 112us/step - loss: 0.0152 - acc: 1.0000 - val_loss: 0.1962 - val_acc: 0.9111\n",
      "Epoch 1039/1200\n",
      "133/133 [==============================] - 0s 107us/step - loss: 0.0151 - acc: 1.0000 - val_loss: 0.1937 - val_acc: 0.9111\n",
      "Epoch 1040/1200\n",
      "133/133 [==============================] - 0s 106us/step - loss: 0.0150 - acc: 1.0000 - val_loss: 0.1958 - val_acc: 0.9111\n",
      "Epoch 1041/1200\n",
      "133/133 [==============================] - 0s 103us/step - loss: 0.0148 - acc: 1.0000 - val_loss: 0.1943 - val_acc: 0.9111\n",
      "Epoch 1042/1200\n",
      "133/133 [==============================] - 0s 113us/step - loss: 0.0147 - acc: 1.0000 - val_loss: 0.1955 - val_acc: 0.9111\n",
      "Epoch 1043/1200\n",
      "133/133 [==============================] - 0s 109us/step - loss: 0.0147 - acc: 1.0000 - val_loss: 0.1938 - val_acc: 0.9111\n",
      "Epoch 1044/1200\n",
      "133/133 [==============================] - 0s 134us/step - loss: 0.0146 - acc: 1.0000 - val_loss: 0.1969 - val_acc: 0.9111\n",
      "Epoch 1045/1200\n",
      "133/133 [==============================] - 0s 105us/step - loss: 0.0144 - acc: 1.0000 - val_loss: 0.1950 - val_acc: 0.9111\n",
      "Epoch 1046/1200\n",
      "133/133 [==============================] - 0s 125us/step - loss: 0.0143 - acc: 1.0000 - val_loss: 0.1950 - val_acc: 0.9111\n",
      "Epoch 1047/1200\n",
      "133/133 [==============================] - 0s 108us/step - loss: 0.0142 - acc: 1.0000 - val_loss: 0.1961 - val_acc: 0.9111\n",
      "Epoch 1048/1200\n",
      "133/133 [==============================] - 0s 108us/step - loss: 0.0141 - acc: 1.0000 - val_loss: 0.1937 - val_acc: 0.9111\n",
      "Epoch 1049/1200\n",
      "133/133 [==============================] - 0s 107us/step - loss: 0.0140 - acc: 1.0000 - val_loss: 0.1940 - val_acc: 0.9111\n",
      "Epoch 1050/1200\n",
      "133/133 [==============================] - 0s 93us/step - loss: 0.0140 - acc: 1.0000 - val_loss: 0.1942 - val_acc: 0.9111\n",
      "Epoch 1051/1200\n",
      "133/133 [==============================] - 0s 107us/step - loss: 0.0138 - acc: 1.0000 - val_loss: 0.1942 - val_acc: 0.9111\n",
      "Epoch 1052/1200\n",
      "133/133 [==============================] - 0s 99us/step - loss: 0.0137 - acc: 1.0000 - val_loss: 0.1948 - val_acc: 0.9111\n",
      "Epoch 1053/1200\n",
      "133/133 [==============================] - 0s 113us/step - loss: 0.0137 - acc: 1.0000 - val_loss: 0.1962 - val_acc: 0.9111\n",
      "Epoch 1054/1200\n",
      "133/133 [==============================] - 0s 109us/step - loss: 0.0135 - acc: 1.0000 - val_loss: 0.1964 - val_acc: 0.9111\n",
      "Epoch 1055/1200\n",
      "133/133 [==============================] - 0s 122us/step - loss: 0.0135 - acc: 1.0000 - val_loss: 0.1970 - val_acc: 0.9111\n",
      "Epoch 1056/1200\n",
      "133/133 [==============================] - 0s 100us/step - loss: 0.0133 - acc: 1.0000 - val_loss: 0.1977 - val_acc: 0.9111\n",
      "Epoch 1057/1200\n",
      "133/133 [==============================] - 0s 101us/step - loss: 0.0132 - acc: 1.0000 - val_loss: 0.1978 - val_acc: 0.9111\n",
      "Epoch 1058/1200\n",
      "133/133 [==============================] - 0s 113us/step - loss: 0.0131 - acc: 1.0000 - val_loss: 0.1965 - val_acc: 0.9111\n",
      "Epoch 1059/1200\n",
      "133/133 [==============================] - 0s 124us/step - loss: 0.0130 - acc: 1.0000 - val_loss: 0.1978 - val_acc: 0.9111\n",
      "Epoch 1060/1200\n",
      "133/133 [==============================] - 0s 106us/step - loss: 0.0129 - acc: 1.0000 - val_loss: 0.1981 - val_acc: 0.9111\n",
      "Epoch 1061/1200\n",
      "133/133 [==============================] - 0s 107us/step - loss: 0.0127 - acc: 1.0000 - val_loss: 0.1983 - val_acc: 0.9111\n",
      "Epoch 1062/1200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133/133 [==============================] - 0s 129us/step - loss: 0.0127 - acc: 1.0000 - val_loss: 0.1992 - val_acc: 0.9111\n",
      "Epoch 1063/1200\n",
      "133/133 [==============================] - 0s 124us/step - loss: 0.0126 - acc: 1.0000 - val_loss: 0.1982 - val_acc: 0.9111\n",
      "Epoch 1064/1200\n",
      "133/133 [==============================] - 0s 110us/step - loss: 0.0125 - acc: 1.0000 - val_loss: 0.1993 - val_acc: 0.9111\n",
      "Epoch 1065/1200\n",
      "133/133 [==============================] - 0s 111us/step - loss: 0.0125 - acc: 1.0000 - val_loss: 0.1986 - val_acc: 0.9111\n",
      "Epoch 1066/1200\n",
      "133/133 [==============================] - 0s 113us/step - loss: 0.0124 - acc: 1.0000 - val_loss: 0.1982 - val_acc: 0.9111\n",
      "Epoch 1067/1200\n",
      "133/133 [==============================] - 0s 107us/step - loss: 0.0122 - acc: 1.0000 - val_loss: 0.1968 - val_acc: 0.9111\n",
      "Epoch 1068/1200\n",
      "133/133 [==============================] - 0s 110us/step - loss: 0.0121 - acc: 1.0000 - val_loss: 0.1980 - val_acc: 0.9111\n",
      "Epoch 1069/1200\n",
      "133/133 [==============================] - 0s 109us/step - loss: 0.0121 - acc: 1.0000 - val_loss: 0.2001 - val_acc: 0.9111\n",
      "Epoch 1070/1200\n",
      "133/133 [==============================] - 0s 112us/step - loss: 0.0120 - acc: 1.0000 - val_loss: 0.2002 - val_acc: 0.9111\n",
      "Epoch 1071/1200\n",
      "133/133 [==============================] - 0s 113us/step - loss: 0.0119 - acc: 1.0000 - val_loss: 0.1983 - val_acc: 0.9111\n",
      "Epoch 1072/1200\n",
      "133/133 [==============================] - 0s 112us/step - loss: 0.0117 - acc: 1.0000 - val_loss: 0.1964 - val_acc: 0.9111\n",
      "Epoch 1073/1200\n",
      "133/133 [==============================] - 0s 131us/step - loss: 0.0116 - acc: 1.0000 - val_loss: 0.1986 - val_acc: 0.9111\n",
      "Epoch 1074/1200\n",
      "133/133 [==============================] - 0s 185us/step - loss: 0.0116 - acc: 1.0000 - val_loss: 0.1965 - val_acc: 0.9111\n",
      "Epoch 1075/1200\n",
      "133/133 [==============================] - 0s 173us/step - loss: 0.0115 - acc: 1.0000 - val_loss: 0.1987 - val_acc: 0.9111\n",
      "Epoch 1076/1200\n",
      "133/133 [==============================] - 0s 105us/step - loss: 0.0115 - acc: 1.0000 - val_loss: 0.1997 - val_acc: 0.9111\n",
      "Epoch 1077/1200\n",
      "133/133 [==============================] - 0s 110us/step - loss: 0.0113 - acc: 1.0000 - val_loss: 0.2028 - val_acc: 0.9111\n",
      "Epoch 1078/1200\n",
      "133/133 [==============================] - 0s 115us/step - loss: 0.0112 - acc: 1.0000 - val_loss: 0.2000 - val_acc: 0.9111\n",
      "Epoch 1079/1200\n",
      "133/133 [==============================] - 0s 112us/step - loss: 0.0111 - acc: 1.0000 - val_loss: 0.1998 - val_acc: 0.9111\n",
      "Epoch 1080/1200\n",
      "133/133 [==============================] - 0s 105us/step - loss: 0.0109 - acc: 1.0000 - val_loss: 0.2022 - val_acc: 0.9111\n",
      "Epoch 1081/1200\n",
      "133/133 [==============================] - 0s 101us/step - loss: 0.0109 - acc: 1.0000 - val_loss: 0.2007 - val_acc: 0.9111\n",
      "Epoch 1082/1200\n",
      "133/133 [==============================] - 0s 111us/step - loss: 0.0109 - acc: 1.0000 - val_loss: 0.1998 - val_acc: 0.9111\n",
      "Epoch 1083/1200\n",
      "133/133 [==============================] - 0s 96us/step - loss: 0.0106 - acc: 1.0000 - val_loss: 0.1989 - val_acc: 0.9111\n",
      "Epoch 1084/1200\n",
      "133/133 [==============================] - 0s 116us/step - loss: 0.0106 - acc: 1.0000 - val_loss: 0.1988 - val_acc: 0.9111\n",
      "Epoch 1085/1200\n",
      "133/133 [==============================] - 0s 102us/step - loss: 0.0106 - acc: 1.0000 - val_loss: 0.2006 - val_acc: 0.9111\n",
      "Epoch 1086/1200\n",
      "133/133 [==============================] - 0s 107us/step - loss: 0.0104 - acc: 1.0000 - val_loss: 0.1994 - val_acc: 0.9111\n",
      "Epoch 1087/1200\n",
      "133/133 [==============================] - 0s 117us/step - loss: 0.0104 - acc: 1.0000 - val_loss: 0.1980 - val_acc: 0.9333\n",
      "Epoch 1088/1200\n",
      "133/133 [==============================] - 0s 98us/step - loss: 0.0103 - acc: 1.0000 - val_loss: 0.2002 - val_acc: 0.9111\n",
      "Epoch 1089/1200\n",
      "133/133 [==============================] - 0s 109us/step - loss: 0.0102 - acc: 1.0000 - val_loss: 0.2018 - val_acc: 0.9111\n",
      "Epoch 1090/1200\n",
      "133/133 [==============================] - 0s 109us/step - loss: 0.0102 - acc: 1.0000 - val_loss: 0.2041 - val_acc: 0.9111\n",
      "Epoch 1091/1200\n",
      "133/133 [==============================] - 0s 104us/step - loss: 0.0100 - acc: 1.0000 - val_loss: 0.2024 - val_acc: 0.9111\n",
      "Epoch 1092/1200\n",
      "133/133 [==============================] - 0s 110us/step - loss: 0.0099 - acc: 1.0000 - val_loss: 0.2041 - val_acc: 0.9111\n",
      "Epoch 1093/1200\n",
      "133/133 [==============================] - 0s 109us/step - loss: 0.0100 - acc: 1.0000 - val_loss: 0.2053 - val_acc: 0.9111\n",
      "Epoch 1094/1200\n",
      "133/133 [==============================] - 0s 111us/step - loss: 0.0098 - acc: 1.0000 - val_loss: 0.2044 - val_acc: 0.9111\n",
      "Epoch 1095/1200\n",
      "133/133 [==============================] - 0s 121us/step - loss: 0.0099 - acc: 1.0000 - val_loss: 0.2034 - val_acc: 0.9111\n",
      "Epoch 1096/1200\n",
      "133/133 [==============================] - 0s 119us/step - loss: 0.0096 - acc: 1.0000 - val_loss: 0.2037 - val_acc: 0.9111\n",
      "Epoch 1097/1200\n",
      "133/133 [==============================] - 0s 99us/step - loss: 0.0096 - acc: 1.0000 - val_loss: 0.2041 - val_acc: 0.9111\n",
      "Epoch 1098/1200\n",
      "133/133 [==============================] - 0s 109us/step - loss: 0.0095 - acc: 1.0000 - val_loss: 0.2054 - val_acc: 0.9111\n",
      "Epoch 1099/1200\n",
      "133/133 [==============================] - 0s 105us/step - loss: 0.0094 - acc: 1.0000 - val_loss: 0.2022 - val_acc: 0.9333\n",
      "Epoch 1100/1200\n",
      "133/133 [==============================] - 0s 107us/step - loss: 0.0094 - acc: 1.0000 - val_loss: 0.2027 - val_acc: 0.9333\n",
      "Epoch 1101/1200\n",
      "133/133 [==============================] - 0s 107us/step - loss: 0.0093 - acc: 1.0000 - val_loss: 0.2010 - val_acc: 0.9333\n",
      "Epoch 1102/1200\n",
      "133/133 [==============================] - 0s 136us/step - loss: 0.0094 - acc: 1.0000 - val_loss: 0.2016 - val_acc: 0.9333\n",
      "Epoch 1103/1200\n",
      "133/133 [==============================] - 0s 106us/step - loss: 0.0092 - acc: 1.0000 - val_loss: 0.2042 - val_acc: 0.9333\n",
      "Epoch 1104/1200\n",
      "133/133 [==============================] - 0s 128us/step - loss: 0.0091 - acc: 1.0000 - val_loss: 0.2062 - val_acc: 0.9333\n",
      "Epoch 1105/1200\n",
      "133/133 [==============================] - 0s 110us/step - loss: 0.0092 - acc: 1.0000 - val_loss: 0.2043 - val_acc: 0.9333\n",
      "Epoch 1106/1200\n",
      "133/133 [==============================] - 0s 96us/step - loss: 0.0090 - acc: 1.0000 - val_loss: 0.2065 - val_acc: 0.9333\n",
      "Epoch 1107/1200\n",
      "133/133 [==============================] - 0s 86us/step - loss: 0.0089 - acc: 1.0000 - val_loss: 0.2041 - val_acc: 0.9333\n",
      "Epoch 1108/1200\n",
      "133/133 [==============================] - 0s 96us/step - loss: 0.0088 - acc: 1.0000 - val_loss: 0.2056 - val_acc: 0.9333\n",
      "Epoch 1109/1200\n",
      "133/133 [==============================] - 0s 107us/step - loss: 0.0089 - acc: 1.0000 - val_loss: 0.2081 - val_acc: 0.9333\n",
      "Epoch 1110/1200\n",
      "133/133 [==============================] - 0s 110us/step - loss: 0.0087 - acc: 1.0000 - val_loss: 0.2077 - val_acc: 0.9333\n",
      "Epoch 1111/1200\n",
      "133/133 [==============================] - 0s 118us/step - loss: 0.0087 - acc: 1.0000 - val_loss: 0.2072 - val_acc: 0.9333\n",
      "Epoch 1112/1200\n",
      "133/133 [==============================] - 0s 117us/step - loss: 0.0087 - acc: 1.0000 - val_loss: 0.2080 - val_acc: 0.9333\n",
      "Epoch 1113/1200\n",
      "133/133 [==============================] - 0s 102us/step - loss: 0.0085 - acc: 1.0000 - val_loss: 0.2112 - val_acc: 0.9333\n",
      "Epoch 1114/1200\n",
      "133/133 [==============================] - 0s 87us/step - loss: 0.0085 - acc: 1.0000 - val_loss: 0.2132 - val_acc: 0.9111\n",
      "Epoch 1115/1200\n",
      "133/133 [==============================] - 0s 99us/step - loss: 0.0085 - acc: 1.0000 - val_loss: 0.2098 - val_acc: 0.9333\n",
      "Epoch 1116/1200\n",
      "133/133 [==============================] - 0s 107us/step - loss: 0.0083 - acc: 1.0000 - val_loss: 0.2091 - val_acc: 0.9333\n",
      "Epoch 1117/1200\n",
      "133/133 [==============================] - 0s 107us/step - loss: 0.0084 - acc: 1.0000 - val_loss: 0.2090 - val_acc: 0.9333\n",
      "Epoch 1118/1200\n",
      "133/133 [==============================] - 0s 104us/step - loss: 0.0083 - acc: 1.0000 - val_loss: 0.2099 - val_acc: 0.9333\n",
      "Epoch 1119/1200\n",
      "133/133 [==============================] - 0s 107us/step - loss: 0.0081 - acc: 1.0000 - val_loss: 0.2100 - val_acc: 0.9333\n",
      "Epoch 1120/1200\n",
      "133/133 [==============================] - 0s 114us/step - loss: 0.0082 - acc: 1.0000 - val_loss: 0.2123 - val_acc: 0.9333\n",
      "Epoch 1121/1200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133/133 [==============================] - 0s 116us/step - loss: 0.0081 - acc: 1.0000 - val_loss: 0.2130 - val_acc: 0.9333\n",
      "Epoch 1122/1200\n",
      "133/133 [==============================] - 0s 131us/step - loss: 0.0080 - acc: 1.0000 - val_loss: 0.2128 - val_acc: 0.9333\n",
      "Epoch 1123/1200\n",
      "133/133 [==============================] - 0s 126us/step - loss: 0.0080 - acc: 1.0000 - val_loss: 0.2094 - val_acc: 0.9333\n",
      "Epoch 1124/1200\n",
      "133/133 [==============================] - 0s 117us/step - loss: 0.0078 - acc: 1.0000 - val_loss: 0.2119 - val_acc: 0.9333\n",
      "Epoch 1125/1200\n",
      "133/133 [==============================] - 0s 113us/step - loss: 0.0079 - acc: 1.0000 - val_loss: 0.2117 - val_acc: 0.9333\n",
      "Epoch 1126/1200\n",
      "133/133 [==============================] - 0s 111us/step - loss: 0.0078 - acc: 1.0000 - val_loss: 0.2125 - val_acc: 0.9333\n",
      "Epoch 1127/1200\n",
      "133/133 [==============================] - 0s 105us/step - loss: 0.0077 - acc: 1.0000 - val_loss: 0.2131 - val_acc: 0.9333\n",
      "Epoch 1128/1200\n",
      "133/133 [==============================] - 0s 102us/step - loss: 0.0077 - acc: 1.0000 - val_loss: 0.2148 - val_acc: 0.9333\n",
      "Epoch 1129/1200\n",
      "133/133 [==============================] - 0s 98us/step - loss: 0.0076 - acc: 1.0000 - val_loss: 0.2128 - val_acc: 0.9333\n",
      "Epoch 1130/1200\n",
      "133/133 [==============================] - 0s 89us/step - loss: 0.0075 - acc: 1.0000 - val_loss: 0.2143 - val_acc: 0.9333\n",
      "Epoch 1131/1200\n",
      "133/133 [==============================] - 0s 104us/step - loss: 0.0074 - acc: 1.0000 - val_loss: 0.2136 - val_acc: 0.9333\n",
      "Epoch 1132/1200\n",
      "133/133 [==============================] - 0s 118us/step - loss: 0.0075 - acc: 1.0000 - val_loss: 0.2137 - val_acc: 0.9333\n",
      "Epoch 1133/1200\n",
      "133/133 [==============================] - 0s 128us/step - loss: 0.0074 - acc: 1.0000 - val_loss: 0.2157 - val_acc: 0.9333\n",
      "Epoch 1134/1200\n",
      "133/133 [==============================] - 0s 128us/step - loss: 0.0073 - acc: 1.0000 - val_loss: 0.2153 - val_acc: 0.9333\n",
      "Epoch 1135/1200\n",
      "133/133 [==============================] - 0s 97us/step - loss: 0.0073 - acc: 1.0000 - val_loss: 0.2126 - val_acc: 0.9333\n",
      "Epoch 1136/1200\n",
      "133/133 [==============================] - 0s 105us/step - loss: 0.0072 - acc: 1.0000 - val_loss: 0.2132 - val_acc: 0.9333\n",
      "Epoch 1137/1200\n",
      "133/133 [==============================] - 0s 118us/step - loss: 0.0071 - acc: 1.0000 - val_loss: 0.2120 - val_acc: 0.9333\n",
      "Epoch 1138/1200\n",
      "133/133 [==============================] - 0s 103us/step - loss: 0.0071 - acc: 1.0000 - val_loss: 0.2160 - val_acc: 0.9333\n",
      "Epoch 1139/1200\n",
      "133/133 [==============================] - 0s 98us/step - loss: 0.0071 - acc: 1.0000 - val_loss: 0.2178 - val_acc: 0.9333\n",
      "Epoch 1140/1200\n",
      "133/133 [==============================] - 0s 94us/step - loss: 0.0070 - acc: 1.0000 - val_loss: 0.2167 - val_acc: 0.9333\n",
      "Epoch 1141/1200\n",
      "133/133 [==============================] - 0s 136us/step - loss: 0.0070 - acc: 1.0000 - val_loss: 0.2187 - val_acc: 0.9333\n",
      "Epoch 1142/1200\n",
      "133/133 [==============================] - 0s 105us/step - loss: 0.0069 - acc: 1.0000 - val_loss: 0.2164 - val_acc: 0.9333\n",
      "Epoch 1143/1200\n",
      "133/133 [==============================] - 0s 94us/step - loss: 0.0068 - acc: 1.0000 - val_loss: 0.2143 - val_acc: 0.9333\n",
      "Epoch 1144/1200\n",
      "133/133 [==============================] - 0s 101us/step - loss: 0.0068 - acc: 1.0000 - val_loss: 0.2160 - val_acc: 0.9333\n",
      "Epoch 1145/1200\n",
      "133/133 [==============================] - 0s 111us/step - loss: 0.0068 - acc: 1.0000 - val_loss: 0.2174 - val_acc: 0.9333\n",
      "Epoch 1146/1200\n",
      "133/133 [==============================] - 0s 113us/step - loss: 0.0067 - acc: 1.0000 - val_loss: 0.2191 - val_acc: 0.9333\n",
      "Epoch 1147/1200\n",
      "133/133 [==============================] - 0s 92us/step - loss: 0.0066 - acc: 1.0000 - val_loss: 0.2204 - val_acc: 0.9333\n",
      "Epoch 1148/1200\n",
      "133/133 [==============================] - 0s 102us/step - loss: 0.0066 - acc: 1.0000 - val_loss: 0.2178 - val_acc: 0.9333\n",
      "Epoch 1149/1200\n",
      "133/133 [==============================] - 0s 104us/step - loss: 0.0066 - acc: 1.0000 - val_loss: 0.2171 - val_acc: 0.9333\n",
      "Epoch 1150/1200\n",
      "133/133 [==============================] - 0s 103us/step - loss: 0.0065 - acc: 1.0000 - val_loss: 0.2158 - val_acc: 0.9333\n",
      "Epoch 1151/1200\n",
      "133/133 [==============================] - 0s 111us/step - loss: 0.0065 - acc: 1.0000 - val_loss: 0.2164 - val_acc: 0.9333\n",
      "Epoch 1152/1200\n",
      "133/133 [==============================] - 0s 108us/step - loss: 0.0064 - acc: 1.0000 - val_loss: 0.2180 - val_acc: 0.9333\n",
      "Epoch 1153/1200\n",
      "133/133 [==============================] - 0s 104us/step - loss: 0.0063 - acc: 1.0000 - val_loss: 0.2192 - val_acc: 0.9333\n",
      "Epoch 1154/1200\n",
      "133/133 [==============================] - 0s 107us/step - loss: 0.0063 - acc: 1.0000 - val_loss: 0.2190 - val_acc: 0.9333\n",
      "Epoch 1155/1200\n",
      "133/133 [==============================] - 0s 112us/step - loss: 0.0063 - acc: 1.0000 - val_loss: 0.2198 - val_acc: 0.9333\n",
      "Epoch 1156/1200\n",
      "133/133 [==============================] - 0s 110us/step - loss: 0.0061 - acc: 1.0000 - val_loss: 0.2204 - val_acc: 0.9333\n",
      "Epoch 1157/1200\n",
      "133/133 [==============================] - 0s 117us/step - loss: 0.0061 - acc: 1.0000 - val_loss: 0.2195 - val_acc: 0.9333\n",
      "Epoch 1158/1200\n",
      "133/133 [==============================] - 0s 122us/step - loss: 0.0061 - acc: 1.0000 - val_loss: 0.2180 - val_acc: 0.9333\n",
      "Epoch 1159/1200\n",
      "133/133 [==============================] - 0s 117us/step - loss: 0.0060 - acc: 1.0000 - val_loss: 0.2219 - val_acc: 0.9333\n",
      "Epoch 1160/1200\n",
      "133/133 [==============================] - 0s 105us/step - loss: 0.0060 - acc: 1.0000 - val_loss: 0.2218 - val_acc: 0.9333\n",
      "Epoch 1161/1200\n",
      "133/133 [==============================] - 0s 106us/step - loss: 0.0059 - acc: 1.0000 - val_loss: 0.2190 - val_acc: 0.9333\n",
      "Epoch 1162/1200\n",
      "133/133 [==============================] - 0s 102us/step - loss: 0.0059 - acc: 1.0000 - val_loss: 0.2213 - val_acc: 0.9333\n",
      "Epoch 1163/1200\n",
      "133/133 [==============================] - 0s 135us/step - loss: 0.0059 - acc: 1.0000 - val_loss: 0.2218 - val_acc: 0.9333\n",
      "Epoch 1164/1200\n",
      "133/133 [==============================] - 0s 98us/step - loss: 0.0059 - acc: 1.0000 - val_loss: 0.2218 - val_acc: 0.9333\n",
      "Epoch 1165/1200\n",
      "133/133 [==============================] - 0s 103us/step - loss: 0.0058 - acc: 1.0000 - val_loss: 0.2209 - val_acc: 0.9333\n",
      "Epoch 1166/1200\n",
      "133/133 [==============================] - 0s 110us/step - loss: 0.0058 - acc: 1.0000 - val_loss: 0.2183 - val_acc: 0.9333\n",
      "Epoch 1167/1200\n",
      "133/133 [==============================] - 0s 115us/step - loss: 0.0057 - acc: 1.0000 - val_loss: 0.2182 - val_acc: 0.9333\n",
      "Epoch 1168/1200\n",
      "133/133 [==============================] - 0s 99us/step - loss: 0.0057 - acc: 1.0000 - val_loss: 0.2171 - val_acc: 0.9333\n",
      "Epoch 1169/1200\n",
      "133/133 [==============================] - 0s 106us/step - loss: 0.0057 - acc: 1.0000 - val_loss: 0.2206 - val_acc: 0.9333\n",
      "Epoch 1170/1200\n",
      "133/133 [==============================] - 0s 106us/step - loss: 0.0056 - acc: 1.0000 - val_loss: 0.2199 - val_acc: 0.9333\n",
      "Epoch 1171/1200\n",
      "133/133 [==============================] - 0s 107us/step - loss: 0.0055 - acc: 1.0000 - val_loss: 0.2229 - val_acc: 0.9333\n",
      "Epoch 1172/1200\n",
      "133/133 [==============================] - 0s 107us/step - loss: 0.0055 - acc: 1.0000 - val_loss: 0.2202 - val_acc: 0.9333\n",
      "Epoch 1173/1200\n",
      "133/133 [==============================] - 0s 115us/step - loss: 0.0055 - acc: 1.0000 - val_loss: 0.2216 - val_acc: 0.9333\n",
      "Epoch 1174/1200\n",
      "133/133 [==============================] - 0s 230us/step - loss: 0.0055 - acc: 1.0000 - val_loss: 0.2222 - val_acc: 0.9333\n",
      "Epoch 1175/1200\n",
      "133/133 [==============================] - 0s 142us/step - loss: 0.0054 - acc: 1.0000 - val_loss: 0.2252 - val_acc: 0.9333\n",
      "Epoch 1176/1200\n",
      "133/133 [==============================] - 0s 118us/step - loss: 0.0053 - acc: 1.0000 - val_loss: 0.2230 - val_acc: 0.9333\n",
      "Epoch 1177/1200\n",
      "133/133 [==============================] - 0s 119us/step - loss: 0.0054 - acc: 1.0000 - val_loss: 0.2235 - val_acc: 0.9333\n",
      "Epoch 1178/1200\n",
      "133/133 [==============================] - 0s 127us/step - loss: 0.0053 - acc: 1.0000 - val_loss: 0.2245 - val_acc: 0.9333\n",
      "Epoch 1179/1200\n",
      "133/133 [==============================] - 0s 114us/step - loss: 0.0053 - acc: 1.0000 - val_loss: 0.2222 - val_acc: 0.9333\n",
      "Epoch 1180/1200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "133/133 [==============================] - 0s 121us/step - loss: 0.0053 - acc: 1.0000 - val_loss: 0.2240 - val_acc: 0.9333\n",
      "Epoch 1181/1200\n",
      "133/133 [==============================] - 0s 121us/step - loss: 0.0052 - acc: 1.0000 - val_loss: 0.2286 - val_acc: 0.9333\n",
      "Epoch 1182/1200\n",
      "133/133 [==============================] - 0s 119us/step - loss: 0.0052 - acc: 1.0000 - val_loss: 0.2264 - val_acc: 0.9333\n",
      "Epoch 1183/1200\n",
      "133/133 [==============================] - 0s 108us/step - loss: 0.0051 - acc: 1.0000 - val_loss: 0.2287 - val_acc: 0.9333\n",
      "Epoch 1184/1200\n",
      "133/133 [==============================] - 0s 109us/step - loss: 0.0051 - acc: 1.0000 - val_loss: 0.2239 - val_acc: 0.9333\n",
      "Epoch 1185/1200\n",
      "133/133 [==============================] - 0s 110us/step - loss: 0.0050 - acc: 1.0000 - val_loss: 0.2227 - val_acc: 0.9333\n",
      "Epoch 1186/1200\n",
      "133/133 [==============================] - 0s 118us/step - loss: 0.0049 - acc: 1.0000 - val_loss: 0.2220 - val_acc: 0.9333\n",
      "Epoch 1187/1200\n",
      "133/133 [==============================] - 0s 100us/step - loss: 0.0049 - acc: 1.0000 - val_loss: 0.2208 - val_acc: 0.9333\n",
      "Epoch 1188/1200\n",
      "133/133 [==============================] - 0s 115us/step - loss: 0.0050 - acc: 1.0000 - val_loss: 0.2191 - val_acc: 0.9333\n",
      "Epoch 1189/1200\n",
      "133/133 [==============================] - 0s 111us/step - loss: 0.0049 - acc: 1.0000 - val_loss: 0.2205 - val_acc: 0.9333\n",
      "Epoch 1190/1200\n",
      "133/133 [==============================] - 0s 113us/step - loss: 0.0049 - acc: 1.0000 - val_loss: 0.2248 - val_acc: 0.9333\n",
      "Epoch 1191/1200\n",
      "133/133 [==============================] - 0s 124us/step - loss: 0.0048 - acc: 1.0000 - val_loss: 0.2223 - val_acc: 0.9333\n",
      "Epoch 1192/1200\n",
      "133/133 [==============================] - 0s 112us/step - loss: 0.0047 - acc: 1.0000 - val_loss: 0.2233 - val_acc: 0.9333\n",
      "Epoch 1193/1200\n",
      "133/133 [==============================] - 0s 108us/step - loss: 0.0048 - acc: 1.0000 - val_loss: 0.2240 - val_acc: 0.9333\n",
      "Epoch 1194/1200\n",
      "133/133 [==============================] - 0s 134us/step - loss: 0.0047 - acc: 1.0000 - val_loss: 0.2238 - val_acc: 0.9333\n",
      "Epoch 1195/1200\n",
      "133/133 [==============================] - 0s 103us/step - loss: 0.0047 - acc: 1.0000 - val_loss: 0.2247 - val_acc: 0.9333\n",
      "Epoch 1196/1200\n",
      "133/133 [==============================] - 0s 108us/step - loss: 0.0047 - acc: 1.0000 - val_loss: 0.2238 - val_acc: 0.9333\n",
      "Epoch 1197/1200\n",
      "133/133 [==============================] - 0s 108us/step - loss: 0.0046 - acc: 1.0000 - val_loss: 0.2262 - val_acc: 0.9333\n",
      "Epoch 1198/1200\n",
      "133/133 [==============================] - 0s 109us/step - loss: 0.0046 - acc: 1.0000 - val_loss: 0.2253 - val_acc: 0.9333\n",
      "Epoch 1199/1200\n",
      "133/133 [==============================] - 0s 119us/step - loss: 0.0045 - acc: 1.0000 - val_loss: 0.2294 - val_acc: 0.9333\n",
      "Epoch 1200/1200\n",
      "133/133 [==============================] - 0s 107us/step - loss: 0.0046 - acc: 1.0000 - val_loss: 0.2284 - val_acc: 0.9333\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(xtrain, ytrain, batch_size=15, epochs=1200, verbose =1, validation_data=(xtest,ytest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "\n",
    "train_acc = history.history[\"acc\"]\n",
    "valid_acc = history.history[\"val_acc\"]\n",
    "epochs = range(1200)\n",
    "\n",
    "plt.plot(epochs, train_acc, \"b\", label=\"Training Accuracy\")\n",
    "plt.plot(epochs, valid_acc, \"g\", label=\"Testing Accuracy\")\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.title(\"Model Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(epochs, history.history[\"loss\"], \"b\", label=\"Training Loss\")\n",
    "plt.plot(epochs, history.history[\"val_loss\"], \"g\", label=\"Testing Loss\")\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title(\"Model Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45/45 [==============================] - 0s 80us/step\n",
      "Loss= 0.22848729187001784\n",
      "Accuracy= 0.9333333373069763\n"
     ]
    }
   ],
   "source": [
    "lossAndAcc = model.evaluate(xtest, ytest)\n",
    "print(\"Loss=\", lossAndAcc[0])\n",
    "print(\"Accuracy=\", lossAndAcc[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
