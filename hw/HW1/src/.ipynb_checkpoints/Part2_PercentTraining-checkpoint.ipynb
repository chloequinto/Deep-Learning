{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Percentage of Training Dataset\n",
    "\n",
    "### Goal: \n",
    "Use 20%, 30%, 40%, 50%, 60%, 70%, 80%, 90%, and 100% of the total training dataset to train the neural work and test on it, respectively. You build 9 systems. Plot a graph where the axis indicate the percentage of the training set you used to train a system, and y-axis indicate the accuracy on the test set of each system  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import warnings \n",
    "warnings.filterwarnings('ignore') # feel free to comment this out \n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "import sklearn \n",
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical\n",
    "from keras import layers\n",
    "from keras import models \n",
    "from keras.layers import Dense\n",
    "from keras import optimizers \n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import RFE\n",
    "import math\n",
    "\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataProcessing(): \n",
    "    wine = sklearn.datasets.load_wine()\n",
    "    \n",
    "    df = pd.DataFrame(wine.data)\n",
    "    df.columns = wine.feature_names\n",
    "    df[\"class\"] = wine.target\n",
    "    \n",
    "    labels = df.loc[:,[\"class\"]] \n",
    "    features = df.drop([\"class\"],axis=1)\n",
    "    \n",
    "    xtrain, xtest, ytrain, ytest = train_test_split(features, labels)\n",
    "    ytrain = to_categorical(ytrain, 3)\n",
    "    ytest = to_categorical(ytest,3)\n",
    "    \n",
    "    scale = MinMaxScaler(feature_range=(0,1))\n",
    "    xtrain = scale.fit_transform(xtrain)\n",
    "    xtest = scale.fit_transform(xtest)\n",
    "    \n",
    "    return xtrain, xtest, ytrain, ytest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discussion:    \n",
    "    &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; I used the training set percentage to find the compliment(validation split). In other words, if I want 20% of the training set to be used during training, I'll just utilize validation_split attribute in model.fit() with the compliment, 80%. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainModel(val_split, xtrain, ytrain):\n",
    "    '''\n",
    "    Builds and runs training model \n",
    "    '''\n",
    "    print(\"Training the Model\")\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(Dense(10, input_dim=xtrain.shape[1],activation='relu'))\n",
    "    model.add(Dense(8, activation='relu'))\n",
    "    model.add(Dense(6, activation='relu'))\n",
    "    model.add(Dense(6, activation='relu'))\n",
    "    model.add(Dense(4, activation='relu'))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    \n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer = optimizers.RMSprop(lr = 1e-3), metrics = [\"acc\"])\n",
    "    \n",
    "    # Save weights \n",
    "    checkpoint = ModelCheckpoint(\"model_part2.hdf5\", monitor=\"loss\", verbose=1, save_best_only = True, mode=\"auto\", period=1)\n",
    "    \n",
    "    # Apply early stopping to prevent overfitting \n",
    "    monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=0, mode='auto',restore_best_weights=True)\n",
    "    \n",
    "    # Run Model \n",
    "    history = model.fit(xtrain, ytrain, batch_size=50, epochs=2000, verbose =0, validation_split=val_split, callbacks=[checkpoint,monitor])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testModel(xtest, ytest): \n",
    "    '''\n",
    "    Builds and runs testing model \n",
    "    '''\n",
    "    print(\"Testing the model\")\n",
    "    \n",
    "    evalModel = Sequential()\n",
    "    evalModel.add(Dense(10, input_dim=xtest.shape[1],activation='relu'))\n",
    "    evalModel.add(Dense(8, activation='relu'))\n",
    "    evalModel.add(Dense(6, activation='relu'))\n",
    "    evalModel.add(Dense(6, activation='relu'))\n",
    "    evalModel.add(Dense(4, activation='relu'))\n",
    "    evalModel.add(Dense(3, activation='softmax'))\n",
    "\n",
    "\n",
    "    evalModel.load_weights('model_part2.hdf5')\n",
    "    evalModel.compile(loss=\"categorical_crossentropy\", optimizer = optimizers.RMSprop(lr = 1e-3), metrics = [\"acc\"])\n",
    "    \n",
    "    lossAndAcc = evalModel.evaluate(xtest, ytest)\n",
    "\n",
    "    return lossAndAcc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(train_sizes, accuracy): \n",
    "    '''\n",
    "    Main driver for program \n",
    "    '''\n",
    "\n",
    "    xtrain, xtest, ytrain, ytest = dataProcessing()\n",
    "\n",
    "    for i in range(len(train_sizes)): \n",
    "        print(\"====================================================\")\n",
    "        print(\"Percent Training Size \", train_sizes[i])\n",
    "        \n",
    "        validation_split = round(1-train_sizes[i],2)\n",
    "        \n",
    "        # Train Model\n",
    "        trainModel(validation_split, xtrain, ytrain)\n",
    "        \n",
    "        # Test Model \n",
    "        currAcc = testModel(xtest, ytest)\n",
    "        \n",
    "        # Add accuracy to our list \n",
    "        accuracy.append(currAcc)\n",
    "        \n",
    "        print(\"====================================================\\n\")\n",
    "    return accuracy \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====================================================\n",
      "Percent Training Size  0.2\n",
      "Training the Model\n",
      "WARNING:tensorflow:From /usr/local/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/local/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "\n",
      "Epoch 00001: loss improved from inf to 1.09451, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00002: loss improved from 1.09451 to 1.08996, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00003: loss improved from 1.08996 to 1.08598, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00004: loss improved from 1.08598 to 1.08263, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00005: loss improved from 1.08263 to 1.08008, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00006: loss improved from 1.08008 to 1.07736, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00007: loss improved from 1.07736 to 1.07499, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00008: loss improved from 1.07499 to 1.07262, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00009: loss improved from 1.07262 to 1.06910, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00010: loss improved from 1.06910 to 1.06621, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00011: loss improved from 1.06621 to 1.06365, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00012: loss improved from 1.06365 to 1.06125, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00013: loss improved from 1.06125 to 1.05916, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00014: loss improved from 1.05916 to 1.05718, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00015: loss improved from 1.05718 to 1.05505, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00016: loss improved from 1.05505 to 1.05293, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00017: loss improved from 1.05293 to 1.05079, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00018: loss improved from 1.05079 to 1.04881, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00019: loss improved from 1.04881 to 1.04697, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00020: loss improved from 1.04697 to 1.04482, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00021: loss improved from 1.04482 to 1.04253, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00022: loss improved from 1.04253 to 1.04032, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00023: loss improved from 1.04032 to 1.03821, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00024: loss improved from 1.03821 to 1.03596, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00025: loss improved from 1.03596 to 1.03378, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00026: loss improved from 1.03378 to 1.03117, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00027: loss improved from 1.03117 to 1.02861, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00028: loss improved from 1.02861 to 1.02620, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00029: loss improved from 1.02620 to 1.02365, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00030: loss improved from 1.02365 to 1.02125, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00031: loss improved from 1.02125 to 1.01857, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00032: loss improved from 1.01857 to 1.01618, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00033: loss improved from 1.01618 to 1.01417, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00034: loss improved from 1.01417 to 1.01168, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00035: loss improved from 1.01168 to 1.00944, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00036: loss improved from 1.00944 to 1.00674, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00037: loss improved from 1.00674 to 1.00430, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00038: loss improved from 1.00430 to 1.00157, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00039: loss improved from 1.00157 to 0.99900, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00040: loss improved from 0.99900 to 0.99642, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00041: loss improved from 0.99642 to 0.99404, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00042: loss improved from 0.99404 to 0.99172, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00043: loss improved from 0.99172 to 0.98894, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00044: loss improved from 0.98894 to 0.98597, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00045: loss improved from 0.98597 to 0.98327, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00046: loss improved from 0.98327 to 0.98053, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00047: loss improved from 0.98053 to 0.97724, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00048: loss improved from 0.97724 to 0.97392, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00049: loss improved from 0.97392 to 0.97061, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00050: loss improved from 0.97061 to 0.96742, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00051: loss improved from 0.96742 to 0.96422, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00052: loss improved from 0.96422 to 0.96110, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00053: loss improved from 0.96110 to 0.95779, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00054: loss improved from 0.95779 to 0.95453, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00055: loss improved from 0.95453 to 0.95128, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00056: loss improved from 0.95128 to 0.94774, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00057: loss improved from 0.94774 to 0.94427, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00058: loss improved from 0.94427 to 0.94010, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00059: loss improved from 0.94010 to 0.93597, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00060: loss improved from 0.93597 to 0.93207, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00061: loss improved from 0.93207 to 0.92816, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00062: loss improved from 0.92816 to 0.92478, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00063: loss improved from 0.92478 to 0.92144, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00064: loss improved from 0.92144 to 0.91756, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00065: loss improved from 0.91756 to 0.91392, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00066: loss improved from 0.91392 to 0.91032, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00067: loss improved from 0.91032 to 0.90670, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00068: loss improved from 0.90670 to 0.90330, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00069: loss improved from 0.90330 to 0.90004, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00070: loss improved from 0.90004 to 0.89649, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00071: loss improved from 0.89649 to 0.89337, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00072: loss improved from 0.89337 to 0.88987, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00073: loss improved from 0.88987 to 0.88659, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00074: loss improved from 0.88659 to 0.88341, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00075: loss improved from 0.88341 to 0.88020, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00076: loss improved from 0.88020 to 0.87701, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00077: loss improved from 0.87701 to 0.87373, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00078: loss improved from 0.87373 to 0.87044, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00079: loss improved from 0.87044 to 0.86734, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00080: loss improved from 0.86734 to 0.86402, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00081: loss improved from 0.86402 to 0.86083, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00082: loss improved from 0.86083 to 0.85757, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00083: loss improved from 0.85757 to 0.85360, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00084: loss improved from 0.85360 to 0.84942, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00085: loss improved from 0.84942 to 0.84548, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00086: loss improved from 0.84548 to 0.84169, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00087: loss improved from 0.84169 to 0.83796, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00088: loss improved from 0.83796 to 0.83399, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00089: loss improved from 0.83399 to 0.83014, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00090: loss improved from 0.83014 to 0.82628, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00091: loss improved from 0.82628 to 0.82244, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00092: loss improved from 0.82244 to 0.81873, saving model to model_part2.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00093: loss improved from 0.81873 to 0.81465, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00094: loss improved from 0.81465 to 0.81099, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00095: loss improved from 0.81099 to 0.80735, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00096: loss improved from 0.80735 to 0.80371, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00097: loss improved from 0.80371 to 0.80038, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00098: loss improved from 0.80038 to 0.79672, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00099: loss improved from 0.79672 to 0.79303, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00100: loss improved from 0.79303 to 0.78913, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00101: loss improved from 0.78913 to 0.78586, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00102: loss improved from 0.78586 to 0.78296, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00103: loss improved from 0.78296 to 0.77906, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00104: loss improved from 0.77906 to 0.77546, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00105: loss improved from 0.77546 to 0.77193, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00106: loss improved from 0.77193 to 0.76858, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00107: loss improved from 0.76858 to 0.76517, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00108: loss improved from 0.76517 to 0.76213, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00109: loss improved from 0.76213 to 0.75865, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00110: loss improved from 0.75865 to 0.75555, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00111: loss improved from 0.75555 to 0.75241, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00112: loss improved from 0.75241 to 0.74927, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00113: loss improved from 0.74927 to 0.74624, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00114: loss improved from 0.74624 to 0.74315, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00115: loss improved from 0.74315 to 0.74013, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00116: loss improved from 0.74013 to 0.73704, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00117: loss improved from 0.73704 to 0.73408, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00118: loss improved from 0.73408 to 0.73113, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00119: loss improved from 0.73113 to 0.72850, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00120: loss improved from 0.72850 to 0.72524, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00121: loss improved from 0.72524 to 0.72229, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00122: loss improved from 0.72229 to 0.71966, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00123: loss improved from 0.71966 to 0.71651, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00124: loss improved from 0.71651 to 0.71414, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00125: loss improved from 0.71414 to 0.71053, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00126: loss improved from 0.71053 to 0.70789, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00127: loss improved from 0.70789 to 0.70522, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00128: loss improved from 0.70522 to 0.70296, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00129: loss improved from 0.70296 to 0.70037, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00130: loss improved from 0.70037 to 0.69785, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00131: loss improved from 0.69785 to 0.69559, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00132: loss improved from 0.69559 to 0.69357, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00133: loss improved from 0.69357 to 0.69123, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00134: loss improved from 0.69123 to 0.68878, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00135: loss improved from 0.68878 to 0.68658, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00136: loss improved from 0.68658 to 0.68434, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00137: loss improved from 0.68434 to 0.68212, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00138: loss improved from 0.68212 to 0.68007, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00139: loss improved from 0.68007 to 0.67834, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00140: loss improved from 0.67834 to 0.67636, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00141: loss improved from 0.67636 to 0.67447, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00142: loss improved from 0.67447 to 0.67215, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00143: loss improved from 0.67215 to 0.67007, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00144: loss improved from 0.67007 to 0.66825, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00145: loss improved from 0.66825 to 0.66617, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00146: loss improved from 0.66617 to 0.66437, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00147: loss improved from 0.66437 to 0.66273, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00148: loss improved from 0.66273 to 0.66087, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00149: loss improved from 0.66087 to 0.65938, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00150: loss improved from 0.65938 to 0.65848, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00151: loss improved from 0.65848 to 0.65633, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00152: loss improved from 0.65633 to 0.65516, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00153: loss improved from 0.65516 to 0.65303, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00154: loss did not improve from 0.65303\n",
      "\n",
      "Epoch 00155: loss improved from 0.65303 to 0.64992, saving model to model_part2.hdf5\n",
      "Testing the model\n",
      "45/45 [==============================] - 0s 988us/step\n",
      "====================================================\n",
      "\n",
      "====================================================\n",
      "Percent Training Size  0.3\n",
      "Training the Model\n",
      "\n",
      "Epoch 00001: loss improved from inf to 1.09922, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00002: loss improved from 1.09922 to 1.09737, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00003: loss improved from 1.09737 to 1.09607, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00004: loss improved from 1.09607 to 1.09524, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00005: loss improved from 1.09524 to 1.09462, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00006: loss improved from 1.09462 to 1.09422, saving model to model_part2.hdf5\n",
      "Testing the model\n",
      "45/45 [==============================] - 0s 907us/step\n",
      "====================================================\n",
      "\n",
      "====================================================\n",
      "Percent Training Size  0.4\n",
      "Training the Model\n",
      "\n",
      "Epoch 00001: loss improved from inf to 1.11019, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00002: loss improved from 1.11019 to 1.09947, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00003: loss improved from 1.09947 to 1.09858, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00004: loss improved from 1.09858 to 1.09839, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00005: loss improved from 1.09839 to 1.09826, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00006: loss improved from 1.09826 to 1.09795, saving model to model_part2.hdf5\n",
      "Testing the model\n",
      "45/45 [==============================] - 0s 1ms/step\n",
      "====================================================\n",
      "\n",
      "====================================================\n",
      "Percent Training Size  0.5\n",
      "Training the Model\n",
      "\n",
      "Epoch 00001: loss improved from inf to 1.09388, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00002: loss improved from 1.09388 to 1.08789, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00003: loss improved from 1.08789 to 1.08437, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00004: loss improved from 1.08437 to 1.08081, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00005: loss improved from 1.08081 to 1.07786, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00006: loss improved from 1.07786 to 1.07494, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00007: loss improved from 1.07494 to 1.07222, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00008: loss improved from 1.07222 to 1.06954, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00009: loss improved from 1.06954 to 1.06708, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00010: loss improved from 1.06708 to 1.06443, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00011: loss improved from 1.06443 to 1.06222, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00012: loss improved from 1.06222 to 1.05976, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00013: loss improved from 1.05976 to 1.05719, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00014: loss improved from 1.05719 to 1.05429, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00015: loss improved from 1.05429 to 1.05141, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00016: loss improved from 1.05141 to 1.04853, saving model to model_part2.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00017: loss improved from 1.04853 to 1.04571, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00018: loss improved from 1.04571 to 1.04331, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00019: loss improved from 1.04331 to 1.04073, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00020: loss improved from 1.04073 to 1.03819, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00021: loss improved from 1.03819 to 1.03483, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00022: loss improved from 1.03483 to 1.03206, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00023: loss improved from 1.03206 to 1.02886, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00024: loss improved from 1.02886 to 1.02572, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00025: loss improved from 1.02572 to 1.02259, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00026: loss improved from 1.02259 to 1.02052, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00027: loss improved from 1.02052 to 1.01707, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00028: loss improved from 1.01707 to 1.01361, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00029: loss improved from 1.01361 to 1.01023, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00030: loss improved from 1.01023 to 1.00733, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00031: loss improved from 1.00733 to 1.00354, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00032: loss improved from 1.00354 to 1.00018, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00033: loss improved from 1.00018 to 0.99686, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00034: loss improved from 0.99686 to 0.99358, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00035: loss improved from 0.99358 to 0.99023, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00036: loss improved from 0.99023 to 0.98702, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00037: loss improved from 0.98702 to 0.98276, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00038: loss improved from 0.98276 to 0.97867, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00039: loss improved from 0.97867 to 0.97494, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00040: loss improved from 0.97494 to 0.97148, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00041: loss improved from 0.97148 to 0.96690, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00042: loss improved from 0.96690 to 0.96279, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00043: loss improved from 0.96279 to 0.95888, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00044: loss improved from 0.95888 to 0.95551, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00045: loss improved from 0.95551 to 0.95056, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00046: loss improved from 0.95056 to 0.94671, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00047: loss improved from 0.94671 to 0.94325, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00048: loss improved from 0.94325 to 0.93957, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00049: loss improved from 0.93957 to 0.93601, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00050: loss improved from 0.93601 to 0.93214, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00051: loss improved from 0.93214 to 0.92872, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00052: loss improved from 0.92872 to 0.92495, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00053: loss improved from 0.92495 to 0.92072, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00054: loss improved from 0.92072 to 0.91648, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00055: loss improved from 0.91648 to 0.91204, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00056: loss improved from 0.91204 to 0.90783, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00057: loss improved from 0.90783 to 0.90329, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00058: loss improved from 0.90329 to 0.89853, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00059: loss improved from 0.89853 to 0.89478, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00060: loss improved from 0.89478 to 0.88994, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00061: loss improved from 0.88994 to 0.88574, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00062: loss improved from 0.88574 to 0.88166, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00063: loss improved from 0.88166 to 0.87813, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00064: loss improved from 0.87813 to 0.87351, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00065: loss improved from 0.87351 to 0.86951, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00066: loss improved from 0.86951 to 0.86530, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00067: loss improved from 0.86530 to 0.86155, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00068: loss improved from 0.86155 to 0.85720, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00069: loss improved from 0.85720 to 0.85329, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00070: loss improved from 0.85329 to 0.84982, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00071: loss improved from 0.84982 to 0.84621, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00072: loss improved from 0.84621 to 0.84216, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00073: loss improved from 0.84216 to 0.83788, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00074: loss improved from 0.83788 to 0.83382, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00075: loss improved from 0.83382 to 0.82942, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00076: loss improved from 0.82942 to 0.82523, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00077: loss improved from 0.82523 to 0.82176, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00078: loss improved from 0.82176 to 0.81822, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00079: loss improved from 0.81822 to 0.81427, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00080: loss improved from 0.81427 to 0.81053, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00081: loss improved from 0.81053 to 0.80679, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00082: loss improved from 0.80679 to 0.80306, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00083: loss improved from 0.80306 to 0.79948, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00084: loss improved from 0.79948 to 0.79659, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00085: loss improved from 0.79659 to 0.79197, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00086: loss improved from 0.79197 to 0.78905, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00087: loss improved from 0.78905 to 0.78568, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00088: loss improved from 0.78568 to 0.78212, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00089: loss improved from 0.78212 to 0.77925, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00090: loss improved from 0.77925 to 0.77642, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00091: loss improved from 0.77642 to 0.77317, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00092: loss improved from 0.77317 to 0.77022, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00093: loss improved from 0.77022 to 0.76751, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00094: loss improved from 0.76751 to 0.76482, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00095: loss improved from 0.76482 to 0.76209, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00096: loss improved from 0.76209 to 0.75971, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00097: loss improved from 0.75971 to 0.75677, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00098: loss improved from 0.75677 to 0.75364, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00099: loss improved from 0.75364 to 0.75112, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00100: loss improved from 0.75112 to 0.74851, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00101: loss improved from 0.74851 to 0.74647, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00102: loss improved from 0.74647 to 0.74380, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00103: loss improved from 0.74380 to 0.74135, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00104: loss improved from 0.74135 to 0.73940, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00105: loss improved from 0.73940 to 0.73747, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00106: loss improved from 0.73747 to 0.73543, saving model to model_part2.hdf5\n",
      "Testing the model\n",
      "45/45 [==============================] - 0s 3ms/step\n",
      "====================================================\n",
      "\n",
      "====================================================\n",
      "Percent Training Size  0.6\n",
      "Training the Model\n",
      "\n",
      "Epoch 00001: loss improved from inf to 1.09572, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00002: loss improved from 1.09572 to 1.09122, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00003: loss improved from 1.09122 to 1.08748, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00004: loss improved from 1.08748 to 1.08474, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00005: loss improved from 1.08474 to 1.08216, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00006: loss improved from 1.08216 to 1.07939, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00007: loss improved from 1.07939 to 1.07660, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00008: loss improved from 1.07660 to 1.07354, saving model to model_part2.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00009: loss improved from 1.07354 to 1.07039, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00010: loss improved from 1.07039 to 1.06697, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00011: loss improved from 1.06697 to 1.06327, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00012: loss improved from 1.06327 to 1.05975, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00013: loss improved from 1.05975 to 1.05716, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00014: loss improved from 1.05716 to 1.05259, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00015: loss improved from 1.05259 to 1.04930, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00016: loss improved from 1.04930 to 1.04542, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00017: loss improved from 1.04542 to 1.04198, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00018: loss improved from 1.04198 to 1.03837, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00019: loss improved from 1.03837 to 1.03487, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00020: loss improved from 1.03487 to 1.03175, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00021: loss improved from 1.03175 to 1.02757, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00022: loss improved from 1.02757 to 1.02403, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00023: loss improved from 1.02403 to 1.02033, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00024: loss improved from 1.02033 to 1.01600, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00025: loss improved from 1.01600 to 1.01347, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00026: loss improved from 1.01347 to 1.00844, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00027: loss improved from 1.00844 to 1.00455, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00028: loss improved from 1.00455 to 1.00074, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00029: loss improved from 1.00074 to 0.99631, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00030: loss improved from 0.99631 to 0.99193, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00031: loss improved from 0.99193 to 0.98727, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00032: loss improved from 0.98727 to 0.98235, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00033: loss improved from 0.98235 to 0.97749, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00034: loss improved from 0.97749 to 0.97241, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00035: loss improved from 0.97241 to 0.96769, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00036: loss improved from 0.96769 to 0.96272, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00037: loss improved from 0.96272 to 0.95755, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00038: loss improved from 0.95755 to 0.95234, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00039: loss improved from 0.95234 to 0.94640, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00040: loss improved from 0.94640 to 0.94094, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00041: loss improved from 0.94094 to 0.93514, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00042: loss improved from 0.93514 to 0.92959, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00043: loss improved from 0.92959 to 0.92411, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00044: loss improved from 0.92411 to 0.91801, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00045: loss improved from 0.91801 to 0.91366, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00046: loss improved from 0.91366 to 0.90594, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00047: loss improved from 0.90594 to 0.90001, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00048: loss improved from 0.90001 to 0.89413, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00049: loss improved from 0.89413 to 0.88880, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00050: loss improved from 0.88880 to 0.88191, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00051: loss improved from 0.88191 to 0.87690, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00052: loss improved from 0.87690 to 0.87052, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00053: loss improved from 0.87052 to 0.86407, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00054: loss improved from 0.86407 to 0.85798, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00055: loss improved from 0.85798 to 0.85173, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00056: loss improved from 0.85173 to 0.84495, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00057: loss improved from 0.84495 to 0.83879, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00058: loss improved from 0.83879 to 0.83210, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00059: loss improved from 0.83210 to 0.82563, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00060: loss improved from 0.82563 to 0.81930, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00061: loss improved from 0.81930 to 0.81318, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00062: loss improved from 0.81318 to 0.80794, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00063: loss improved from 0.80794 to 0.80049, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00064: loss improved from 0.80049 to 0.79366, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00065: loss improved from 0.79366 to 0.78789, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00066: loss improved from 0.78789 to 0.78117, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00067: loss improved from 0.78117 to 0.77422, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00068: loss improved from 0.77422 to 0.76813, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00069: loss improved from 0.76813 to 0.76200, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00070: loss improved from 0.76200 to 0.75532, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00071: loss improved from 0.75532 to 0.74871, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00072: loss improved from 0.74871 to 0.74225, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00073: loss improved from 0.74225 to 0.73550, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00074: loss improved from 0.73550 to 0.72832, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00075: loss improved from 0.72832 to 0.72228, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00076: loss improved from 0.72228 to 0.71564, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00077: loss improved from 0.71564 to 0.70838, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00078: loss improved from 0.70838 to 0.70198, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00079: loss improved from 0.70198 to 0.69624, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00080: loss improved from 0.69624 to 0.68842, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00081: loss improved from 0.68842 to 0.68173, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00082: loss improved from 0.68173 to 0.67514, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00083: loss improved from 0.67514 to 0.66918, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00084: loss improved from 0.66918 to 0.66175, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00085: loss improved from 0.66175 to 0.65472, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00086: loss improved from 0.65472 to 0.64805, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00087: loss improved from 0.64805 to 0.64123, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00088: loss improved from 0.64123 to 0.63511, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00089: loss improved from 0.63511 to 0.62804, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00090: loss improved from 0.62804 to 0.62211, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00091: loss improved from 0.62211 to 0.61599, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00092: loss improved from 0.61599 to 0.60982, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00093: loss improved from 0.60982 to 0.60333, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00094: loss improved from 0.60333 to 0.59706, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00095: loss improved from 0.59706 to 0.59089, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00096: loss improved from 0.59089 to 0.58574, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00097: loss improved from 0.58574 to 0.57878, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00098: loss improved from 0.57878 to 0.57366, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00099: loss improved from 0.57366 to 0.56736, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00100: loss improved from 0.56736 to 0.56202, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00101: loss improved from 0.56202 to 0.55625, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00102: loss improved from 0.55625 to 0.55069, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00103: loss improved from 0.55069 to 0.54450, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00104: loss improved from 0.54450 to 0.53984, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00105: loss improved from 0.53984 to 0.53384, saving model to model_part2.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00106: loss improved from 0.53384 to 0.52819, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00107: loss improved from 0.52819 to 0.52281, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00108: loss improved from 0.52281 to 0.51924, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00109: loss improved from 0.51924 to 0.51274, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00110: loss improved from 0.51274 to 0.50670, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00111: loss improved from 0.50670 to 0.50252, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00112: loss improved from 0.50252 to 0.49751, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00113: loss improved from 0.49751 to 0.49143, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00114: loss improved from 0.49143 to 0.48661, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00115: loss improved from 0.48661 to 0.48210, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00116: loss improved from 0.48210 to 0.47661, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00117: loss improved from 0.47661 to 0.47132, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00118: loss improved from 0.47132 to 0.46669, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00119: loss improved from 0.46669 to 0.46197, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00120: loss improved from 0.46197 to 0.45728, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00121: loss improved from 0.45728 to 0.45224, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00122: loss improved from 0.45224 to 0.44903, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00123: loss improved from 0.44903 to 0.44422, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00124: loss improved from 0.44422 to 0.43893, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00125: loss improved from 0.43893 to 0.43410, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00126: loss improved from 0.43410 to 0.43035, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00127: loss improved from 0.43035 to 0.42561, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00128: loss improved from 0.42561 to 0.42147, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00129: loss improved from 0.42147 to 0.41691, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00130: loss improved from 0.41691 to 0.41251, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00131: loss improved from 0.41251 to 0.40936, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00132: loss improved from 0.40936 to 0.40468, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00133: loss improved from 0.40468 to 0.40058, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00134: loss improved from 0.40058 to 0.39750, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00135: loss improved from 0.39750 to 0.39317, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00136: loss improved from 0.39317 to 0.38942, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00137: loss improved from 0.38942 to 0.38571, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00138: loss improved from 0.38571 to 0.38418, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00139: loss improved from 0.38418 to 0.37907, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00140: loss improved from 0.37907 to 0.37645, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00141: loss improved from 0.37645 to 0.37186, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00142: loss improved from 0.37186 to 0.36926, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00143: loss improved from 0.36926 to 0.36503, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00144: loss improved from 0.36503 to 0.36219, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00145: loss improved from 0.36219 to 0.35882, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00146: loss improved from 0.35882 to 0.35502, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00147: loss improved from 0.35502 to 0.35153, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00148: loss improved from 0.35153 to 0.34843, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00149: loss improved from 0.34843 to 0.34568, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00150: loss improved from 0.34568 to 0.34312, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00151: loss improved from 0.34312 to 0.33870, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00152: loss did not improve from 0.33870\n",
      "\n",
      "Epoch 00153: loss improved from 0.33870 to 0.33391, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00154: loss improved from 0.33391 to 0.32967, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00155: loss improved from 0.32967 to 0.32682, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00156: loss improved from 0.32682 to 0.32616, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00157: loss improved from 0.32616 to 0.32173, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00158: loss improved from 0.32173 to 0.32127, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00159: loss improved from 0.32127 to 0.31717, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00160: loss improved from 0.31717 to 0.31523, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00161: loss improved from 0.31523 to 0.31115, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00162: loss improved from 0.31115 to 0.30818, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00163: loss improved from 0.30818 to 0.30546, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00164: loss improved from 0.30546 to 0.30343, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00165: loss improved from 0.30343 to 0.30185, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00166: loss improved from 0.30185 to 0.29885, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00167: loss did not improve from 0.29885\n",
      "\n",
      "Epoch 00168: loss improved from 0.29885 to 0.29346, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00169: loss improved from 0.29346 to 0.29007, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00170: loss improved from 0.29007 to 0.28743, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00171: loss improved from 0.28743 to 0.28673, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00172: loss improved from 0.28673 to 0.28264, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00173: loss improved from 0.28264 to 0.27955, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00174: loss improved from 0.27955 to 0.27713, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00175: loss improved from 0.27713 to 0.27408, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00176: loss improved from 0.27408 to 0.27339, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00177: loss improved from 0.27339 to 0.26995, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00178: loss improved from 0.26995 to 0.26857, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00179: loss improved from 0.26857 to 0.26505, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00180: loss improved from 0.26505 to 0.26260, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00181: loss improved from 0.26260 to 0.26083, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00182: loss improved from 0.26083 to 0.25960, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00183: loss improved from 0.25960 to 0.25471, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00184: loss improved from 0.25471 to 0.25300, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00185: loss improved from 0.25300 to 0.25053, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00186: loss improved from 0.25053 to 0.24765, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00187: loss improved from 0.24765 to 0.24563, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00188: loss improved from 0.24563 to 0.24314, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00189: loss improved from 0.24314 to 0.24269, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00190: loss improved from 0.24269 to 0.23939, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00191: loss improved from 0.23939 to 0.23644, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00192: loss improved from 0.23644 to 0.23412, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00193: loss improved from 0.23412 to 0.23099, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00194: loss did not improve from 0.23099\n",
      "\n",
      "Epoch 00195: loss improved from 0.23099 to 0.22664, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00196: loss did not improve from 0.22664\n",
      "\n",
      "Epoch 00197: loss improved from 0.22664 to 0.22335, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00198: loss improved from 0.22335 to 0.22031, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00199: loss improved from 0.22031 to 0.21771, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00200: loss improved from 0.21771 to 0.21533, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00201: loss improved from 0.21533 to 0.21351, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00202: loss improved from 0.21351 to 0.21078, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00203: loss improved from 0.21078 to 0.20860, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00204: loss improved from 0.20860 to 0.20581, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00205: loss improved from 0.20581 to 0.20379, saving model to model_part2.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00206: loss improved from 0.20379 to 0.20098, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00207: loss improved from 0.20098 to 0.19818, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00208: loss improved from 0.19818 to 0.19637, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00209: loss improved from 0.19637 to 0.19424, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00210: loss improved from 0.19424 to 0.19114, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00211: loss improved from 0.19114 to 0.18803, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00212: loss improved from 0.18803 to 0.18748, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00213: loss improved from 0.18748 to 0.18431, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00214: loss improved from 0.18431 to 0.18156, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00215: loss improved from 0.18156 to 0.18001, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00216: loss improved from 0.18001 to 0.17656, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00217: loss improved from 0.17656 to 0.17465, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00218: loss improved from 0.17465 to 0.17216, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00219: loss improved from 0.17216 to 0.17085, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00220: loss improved from 0.17085 to 0.16850, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00221: loss improved from 0.16850 to 0.16681, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00222: loss improved from 0.16681 to 0.16433, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00223: loss improved from 0.16433 to 0.16130, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00224: loss did not improve from 0.16130\n",
      "\n",
      "Epoch 00225: loss improved from 0.16130 to 0.15974, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00226: loss improved from 0.15974 to 0.15561, saving model to model_part2.hdf5\n",
      "Testing the model\n",
      "45/45 [==============================] - 0s 1ms/step\n",
      "====================================================\n",
      "\n",
      "====================================================\n",
      "Percent Training Size  0.7\n",
      "Training the Model\n",
      "\n",
      "Epoch 00001: loss improved from inf to 1.09669, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00002: loss improved from 1.09669 to 1.07517, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00003: loss improved from 1.07517 to 1.05463, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00004: loss improved from 1.05463 to 1.03918, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00005: loss improved from 1.03918 to 1.02971, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00006: loss improved from 1.02971 to 1.02237, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00007: loss improved from 1.02237 to 1.01574, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00008: loss improved from 1.01574 to 1.01096, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00009: loss improved from 1.01096 to 1.00409, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00010: loss improved from 1.00409 to 0.99801, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00011: loss improved from 0.99801 to 0.99237, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00012: loss improved from 0.99237 to 0.98992, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00013: loss improved from 0.98992 to 0.98233, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00014: loss improved from 0.98233 to 0.97754, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00015: loss improved from 0.97754 to 0.97442, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00016: loss improved from 0.97442 to 0.96768, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00017: loss improved from 0.96768 to 0.96249, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00018: loss improved from 0.96249 to 0.95767, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00019: loss improved from 0.95767 to 0.95273, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00020: loss improved from 0.95273 to 0.94751, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00021: loss improved from 0.94751 to 0.94151, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00022: loss improved from 0.94151 to 0.93584, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00023: loss improved from 0.93584 to 0.92997, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00024: loss improved from 0.92997 to 0.92455, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00025: loss improved from 0.92455 to 0.91875, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00026: loss improved from 0.91875 to 0.91185, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00027: loss improved from 0.91185 to 0.90617, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00028: loss improved from 0.90617 to 0.89947, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00029: loss improved from 0.89947 to 0.89534, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00030: loss improved from 0.89534 to 0.88663, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00031: loss improved from 0.88663 to 0.87995, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00032: loss improved from 0.87995 to 0.87364, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00033: loss improved from 0.87364 to 0.86716, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00034: loss improved from 0.86716 to 0.86231, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00035: loss improved from 0.86231 to 0.85546, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00036: loss improved from 0.85546 to 0.84803, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00037: loss improved from 0.84803 to 0.84140, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00038: loss improved from 0.84140 to 0.83493, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00039: loss improved from 0.83493 to 0.82808, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00040: loss improved from 0.82808 to 0.82158, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00041: loss improved from 0.82158 to 0.81495, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00042: loss improved from 0.81495 to 0.80772, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00043: loss improved from 0.80772 to 0.80289, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00044: loss improved from 0.80289 to 0.79458, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00045: loss improved from 0.79458 to 0.78860, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00046: loss improved from 0.78860 to 0.78305, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00047: loss improved from 0.78305 to 0.77503, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00048: loss improved from 0.77503 to 0.76771, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00049: loss improved from 0.76771 to 0.76098, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00050: loss improved from 0.76098 to 0.75562, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00051: loss improved from 0.75562 to 0.74664, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00052: loss improved from 0.74664 to 0.74041, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00053: loss improved from 0.74041 to 0.73284, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00054: loss improved from 0.73284 to 0.72560, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00055: loss improved from 0.72560 to 0.71808, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00056: loss improved from 0.71808 to 0.71091, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00057: loss improved from 0.71091 to 0.70256, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00058: loss improved from 0.70256 to 0.69631, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00059: loss improved from 0.69631 to 0.68899, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00060: loss improved from 0.68899 to 0.68101, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00061: loss improved from 0.68101 to 0.67264, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00062: loss improved from 0.67264 to 0.66593, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00063: loss improved from 0.66593 to 0.65716, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00064: loss improved from 0.65716 to 0.65069, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00065: loss improved from 0.65069 to 0.64258, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00066: loss improved from 0.64258 to 0.63447, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00067: loss improved from 0.63447 to 0.62631, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00068: loss improved from 0.62631 to 0.61972, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00069: loss improved from 0.61972 to 0.61132, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00070: loss improved from 0.61132 to 0.60657, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00071: loss improved from 0.60657 to 0.59623, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00072: loss improved from 0.59623 to 0.58901, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00073: loss improved from 0.58901 to 0.58197, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00074: loss improved from 0.58197 to 0.57480, saving model to model_part2.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00075: loss improved from 0.57480 to 0.56674, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00076: loss improved from 0.56674 to 0.55891, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00077: loss improved from 0.55891 to 0.55200, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00078: loss improved from 0.55200 to 0.54412, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00079: loss improved from 0.54412 to 0.53760, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00080: loss improved from 0.53760 to 0.52886, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00081: loss improved from 0.52886 to 0.52133, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00082: loss improved from 0.52133 to 0.51356, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00083: loss improved from 0.51356 to 0.50647, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00084: loss improved from 0.50647 to 0.50566, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00085: loss improved from 0.50566 to 0.49424, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00086: loss improved from 0.49424 to 0.48689, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00087: loss improved from 0.48689 to 0.48047, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00088: loss improved from 0.48047 to 0.47264, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00089: loss improved from 0.47264 to 0.46693, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00090: loss improved from 0.46693 to 0.46253, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00091: loss improved from 0.46253 to 0.45401, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00092: loss improved from 0.45401 to 0.44961, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00093: loss improved from 0.44961 to 0.44175, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00094: loss improved from 0.44175 to 0.43593, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00095: loss improved from 0.43593 to 0.42950, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00096: loss improved from 0.42950 to 0.42463, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00097: loss improved from 0.42463 to 0.41812, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00098: loss improved from 0.41812 to 0.41143, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00099: loss improved from 0.41143 to 0.40514, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00100: loss improved from 0.40514 to 0.39905, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00101: loss improved from 0.39905 to 0.39288, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00102: loss improved from 0.39288 to 0.38886, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00103: loss improved from 0.38886 to 0.38473, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00104: loss improved from 0.38473 to 0.37608, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00105: loss improved from 0.37608 to 0.36996, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00106: loss improved from 0.36996 to 0.36654, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00107: loss improved from 0.36654 to 0.36150, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00108: loss improved from 0.36150 to 0.35355, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00109: loss improved from 0.35355 to 0.34858, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00110: loss improved from 0.34858 to 0.34373, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00111: loss improved from 0.34373 to 0.34147, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00112: loss improved from 0.34147 to 0.33498, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00113: loss improved from 0.33498 to 0.32780, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00114: loss improved from 0.32780 to 0.32416, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00115: loss improved from 0.32416 to 0.31869, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00116: loss improved from 0.31869 to 0.31353, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00117: loss improved from 0.31353 to 0.30861, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00118: loss improved from 0.30861 to 0.30522, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00119: loss improved from 0.30522 to 0.30153, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00120: loss improved from 0.30153 to 0.29708, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00121: loss improved from 0.29708 to 0.29059, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00122: loss improved from 0.29059 to 0.28531, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00123: loss improved from 0.28531 to 0.28242, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00124: loss improved from 0.28242 to 0.27735, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00125: loss improved from 0.27735 to 0.27287, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00126: loss improved from 0.27287 to 0.26856, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00127: loss improved from 0.26856 to 0.26337, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00128: loss improved from 0.26337 to 0.26290, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00129: loss improved from 0.26290 to 0.25669, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00130: loss improved from 0.25669 to 0.25126, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00131: loss improved from 0.25126 to 0.25078, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00132: loss improved from 0.25078 to 0.24549, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00133: loss improved from 0.24549 to 0.24149, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00134: loss improved from 0.24149 to 0.23640, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00135: loss improved from 0.23640 to 0.23264, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00136: loss improved from 0.23264 to 0.22896, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00137: loss improved from 0.22896 to 0.22832, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00138: loss improved from 0.22832 to 0.22559, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00139: loss improved from 0.22559 to 0.21799, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00140: loss improved from 0.21799 to 0.21469, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00141: loss improved from 0.21469 to 0.21098, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00142: loss improved from 0.21098 to 0.20726, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00143: loss did not improve from 0.20726\n",
      "\n",
      "Epoch 00144: loss improved from 0.20726 to 0.20195, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00145: loss improved from 0.20195 to 0.19831, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00146: loss improved from 0.19831 to 0.19677, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00147: loss improved from 0.19677 to 0.19276, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00148: loss improved from 0.19276 to 0.19030, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00149: loss improved from 0.19030 to 0.18878, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00150: loss improved from 0.18878 to 0.18436, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00151: loss improved from 0.18436 to 0.18150, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00152: loss improved from 0.18150 to 0.17922, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00153: loss improved from 0.17922 to 0.17606, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00154: loss improved from 0.17606 to 0.17272, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00155: loss improved from 0.17272 to 0.16937, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00156: loss improved from 0.16937 to 0.16573, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00157: loss improved from 0.16573 to 0.16422, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00158: loss improved from 0.16422 to 0.16008, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00159: loss improved from 0.16008 to 0.15800, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00160: loss improved from 0.15800 to 0.15370, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00161: loss improved from 0.15370 to 0.15280, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00162: loss improved from 0.15280 to 0.15027, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00163: loss improved from 0.15027 to 0.14515, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00164: loss improved from 0.14515 to 0.14377, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00165: loss improved from 0.14377 to 0.13990, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00166: loss did not improve from 0.13990\n",
      "\n",
      "Epoch 00167: loss improved from 0.13990 to 0.13518, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00168: loss improved from 0.13518 to 0.13226, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00169: loss did not improve from 0.13226\n",
      "\n",
      "Epoch 00170: loss improved from 0.13226 to 0.12858, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00171: loss improved from 0.12858 to 0.12602, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00172: loss did not improve from 0.12602\n",
      "\n",
      "Epoch 00173: loss did not improve from 0.12602\n",
      "\n",
      "Epoch 00174: loss improved from 0.12602 to 0.12233, saving model to model_part2.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing the model\n",
      "45/45 [==============================] - 0s 2ms/step\n",
      "====================================================\n",
      "\n",
      "====================================================\n",
      "Percent Training Size  0.8\n",
      "Training the Model\n",
      "\n",
      "Epoch 00001: loss improved from inf to 1.09798, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00002: loss improved from 1.09798 to 1.09748, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00003: loss improved from 1.09748 to 1.09687, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00004: loss improved from 1.09687 to 1.09675, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00005: loss improved from 1.09675 to 1.09621, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00006: loss improved from 1.09621 to 1.09486, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00007: loss improved from 1.09486 to 1.09251, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00008: loss improved from 1.09251 to 1.08359, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00009: loss improved from 1.08359 to 1.07424, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00010: loss improved from 1.07424 to 1.05908, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00011: loss improved from 1.05908 to 1.04586, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00012: loss improved from 1.04586 to 1.03610, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00013: loss improved from 1.03610 to 1.02876, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00014: loss improved from 1.02876 to 1.01966, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00015: loss improved from 1.01966 to 1.01127, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00016: loss improved from 1.01127 to 1.00377, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00017: loss improved from 1.00377 to 0.99367, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00018: loss improved from 0.99367 to 0.98396, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00019: loss improved from 0.98396 to 0.97365, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00020: loss improved from 0.97365 to 0.96454, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00021: loss improved from 0.96454 to 0.95511, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00022: loss improved from 0.95511 to 0.94590, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00023: loss improved from 0.94590 to 0.93745, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00024: loss improved from 0.93745 to 0.92930, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00025: loss improved from 0.92930 to 0.92052, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00026: loss improved from 0.92052 to 0.91062, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00027: loss improved from 0.91062 to 0.90273, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00028: loss improved from 0.90273 to 0.89507, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00029: loss improved from 0.89507 to 0.88697, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00030: loss improved from 0.88697 to 0.87886, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00031: loss improved from 0.87886 to 0.87184, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00032: loss improved from 0.87184 to 0.86468, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00033: loss improved from 0.86468 to 0.85851, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00034: loss improved from 0.85851 to 0.85281, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00035: loss improved from 0.85281 to 0.84756, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00036: loss improved from 0.84756 to 0.84097, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00037: loss improved from 0.84097 to 0.83635, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00038: loss improved from 0.83635 to 0.83390, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00039: loss improved from 0.83390 to 0.82834, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00040: loss improved from 0.82834 to 0.82379, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00041: loss improved from 0.82379 to 0.81844, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00042: loss improved from 0.81844 to 0.81357, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00043: loss improved from 0.81357 to 0.81221, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00044: loss improved from 0.81221 to 0.80530, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00045: loss improved from 0.80530 to 0.80311, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00046: loss improved from 0.80311 to 0.80035, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00047: loss improved from 0.80035 to 0.79493, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00048: loss improved from 0.79493 to 0.79152, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00049: loss improved from 0.79152 to 0.78803, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00050: loss improved from 0.78803 to 0.78564, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00051: loss improved from 0.78564 to 0.78459, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00052: loss improved from 0.78459 to 0.77798, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00053: loss improved from 0.77798 to 0.77637, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00054: loss improved from 0.77637 to 0.77356, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00055: loss improved from 0.77356 to 0.77046, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00056: loss improved from 0.77046 to 0.76816, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00057: loss improved from 0.76816 to 0.76472, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00058: loss did not improve from 0.76472\n",
      "\n",
      "Epoch 00059: loss improved from 0.76472 to 0.75924, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00060: loss improved from 0.75924 to 0.75683, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00061: loss improved from 0.75683 to 0.75452, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00062: loss did not improve from 0.75452\n",
      "\n",
      "Epoch 00063: loss improved from 0.75452 to 0.74874, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00064: loss improved from 0.74874 to 0.74658, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00065: loss did not improve from 0.74658\n",
      "\n",
      "Epoch 00066: loss improved from 0.74658 to 0.74326, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00067: loss did not improve from 0.74326\n",
      "\n",
      "Epoch 00068: loss improved from 0.74326 to 0.74177, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00069: loss improved from 0.74177 to 0.73630, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00070: loss improved from 0.73630 to 0.73461, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00071: loss did not improve from 0.73461\n",
      "\n",
      "Epoch 00072: loss improved from 0.73461 to 0.73087, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00073: loss improved from 0.73087 to 0.72972, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00074: loss improved from 0.72972 to 0.72660, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00075: loss did not improve from 0.72660\n",
      "\n",
      "Epoch 00076: loss improved from 0.72660 to 0.72383, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00077: loss improved from 0.72383 to 0.72316, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00078: loss improved from 0.72316 to 0.71923, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00079: loss improved from 0.71923 to 0.71812, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00080: loss improved from 0.71812 to 0.71518, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00081: loss improved from 0.71518 to 0.71329, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00082: loss improved from 0.71329 to 0.71207, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00083: loss improved from 0.71207 to 0.70972, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00084: loss did not improve from 0.70972\n",
      "\n",
      "Epoch 00085: loss improved from 0.70972 to 0.70811, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00086: loss improved from 0.70811 to 0.70513, saving model to model_part2.hdf5\n",
      "Testing the model\n",
      "45/45 [==============================] - 0s 2ms/step\n",
      "====================================================\n",
      "\n",
      "====================================================\n",
      "Percent Training Size  0.9\n",
      "Training the Model\n",
      "\n",
      "Epoch 00001: loss improved from inf to 1.09450, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00002: loss improved from 1.09450 to 1.08958, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00003: loss improved from 1.08958 to 1.08713, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00004: loss improved from 1.08713 to 1.08495, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00005: loss improved from 1.08495 to 1.08253, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00006: loss improved from 1.08253 to 1.08005, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00007: loss improved from 1.08005 to 1.07723, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00008: loss improved from 1.07723 to 1.07426, saving model to model_part2.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00009: loss improved from 1.07426 to 1.07087, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00010: loss improved from 1.07087 to 1.06783, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00011: loss improved from 1.06783 to 1.06475, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00012: loss improved from 1.06475 to 1.06209, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00013: loss improved from 1.06209 to 1.05891, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00014: loss improved from 1.05891 to 1.05556, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00015: loss improved from 1.05556 to 1.05185, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00016: loss improved from 1.05185 to 1.04867, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00017: loss improved from 1.04867 to 1.04460, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00018: loss improved from 1.04460 to 1.04063, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00019: loss improved from 1.04063 to 1.03719, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00020: loss improved from 1.03719 to 1.03257, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00021: loss improved from 1.03257 to 1.02857, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00022: loss improved from 1.02857 to 1.02470, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00023: loss improved from 1.02470 to 1.02117, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00024: loss improved from 1.02117 to 1.01744, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00025: loss improved from 1.01744 to 1.01222, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00026: loss improved from 1.01222 to 1.00756, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00027: loss improved from 1.00756 to 1.00244, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00028: loss improved from 1.00244 to 0.99672, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00029: loss improved from 0.99672 to 0.99175, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00030: loss improved from 0.99175 to 0.98597, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00031: loss improved from 0.98597 to 0.98037, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00032: loss improved from 0.98037 to 0.97478, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00033: loss improved from 0.97478 to 0.96925, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00034: loss improved from 0.96925 to 0.96317, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00035: loss improved from 0.96317 to 0.95750, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00036: loss improved from 0.95750 to 0.95424, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00037: loss improved from 0.95424 to 0.94645, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00038: loss improved from 0.94645 to 0.94070, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00039: loss improved from 0.94070 to 0.93523, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00040: loss improved from 0.93523 to 0.92920, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00041: loss improved from 0.92920 to 0.92332, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00042: loss improved from 0.92332 to 0.91742, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00043: loss improved from 0.91742 to 0.91129, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00044: loss improved from 0.91129 to 0.90509, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00045: loss improved from 0.90509 to 0.89888, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00046: loss improved from 0.89888 to 0.89357, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00047: loss improved from 0.89357 to 0.88800, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00048: loss improved from 0.88800 to 0.88457, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00049: loss improved from 0.88457 to 0.87745, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00050: loss improved from 0.87745 to 0.87343, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00051: loss improved from 0.87343 to 0.87001, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00052: loss improved from 0.87001 to 0.86406, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00053: loss improved from 0.86406 to 0.86012, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00054: loss improved from 0.86012 to 0.85629, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00055: loss improved from 0.85629 to 0.85209, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00056: loss improved from 0.85209 to 0.84760, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00057: loss improved from 0.84760 to 0.84312, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00058: loss improved from 0.84312 to 0.83863, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00059: loss improved from 0.83863 to 0.83553, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00060: loss improved from 0.83553 to 0.83056, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00061: loss improved from 0.83056 to 0.82592, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00062: loss improved from 0.82592 to 0.82329, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00063: loss improved from 0.82329 to 0.81775, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00064: loss improved from 0.81775 to 0.81397, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00065: loss improved from 0.81397 to 0.81006, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00066: loss improved from 0.81006 to 0.80676, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00067: loss improved from 0.80676 to 0.80276, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00068: loss improved from 0.80276 to 0.79950, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00069: loss improved from 0.79950 to 0.79575, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00070: loss improved from 0.79575 to 0.79135, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00071: loss improved from 0.79135 to 0.78859, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00072: loss improved from 0.78859 to 0.78775, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00073: loss improved from 0.78775 to 0.78233, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00074: loss improved from 0.78233 to 0.77999, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00075: loss improved from 0.77999 to 0.77721, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00076: loss improved from 0.77721 to 0.77397, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00077: loss improved from 0.77397 to 0.77107, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00078: loss improved from 0.77107 to 0.77012, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00079: loss improved from 0.77012 to 0.76756, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00080: loss improved from 0.76756 to 0.76429, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00081: loss improved from 0.76429 to 0.76155, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00082: loss improved from 0.76155 to 0.76152, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00083: loss improved from 0.76152 to 0.75744, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00084: loss improved from 0.75744 to 0.75549, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00085: loss improved from 0.75549 to 0.75306, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00086: loss improved from 0.75306 to 0.75156, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00087: loss improved from 0.75156 to 0.74893, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00088: loss improved from 0.74893 to 0.74682, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00089: loss did not improve from 0.74682\n",
      "\n",
      "Epoch 00090: loss improved from 0.74682 to 0.74308, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00091: loss improved from 0.74308 to 0.74220, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00092: loss improved from 0.74220 to 0.73967, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00093: loss improved from 0.73967 to 0.73689, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00094: loss improved from 0.73689 to 0.73589, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00095: loss improved from 0.73589 to 0.73445, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00096: loss improved from 0.73445 to 0.73263, saving model to model_part2.hdf5\n",
      "Testing the model\n",
      "45/45 [==============================] - 0s 3ms/step\n",
      "====================================================\n",
      "\n",
      "====================================================\n",
      "Percent Training Size  1\n",
      "Training the Model\n",
      "\n",
      "Epoch 00001: loss improved from inf to 1.09441, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00002: loss improved from 1.09441 to 1.08804, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00003: loss improved from 1.08804 to 1.08392, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00004: loss improved from 1.08392 to 1.07918, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00005: loss improved from 1.07918 to 1.07477, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00006: loss improved from 1.07477 to 1.06989, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00007: loss improved from 1.06989 to 1.06523, saving model to model_part2.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00008: loss improved from 1.06523 to 1.06021, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00009: loss improved from 1.06021 to 1.05474, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00010: loss improved from 1.05474 to 1.04934, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00011: loss improved from 1.04934 to 1.04335, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00012: loss improved from 1.04335 to 1.03736, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00013: loss improved from 1.03736 to 1.03083, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00014: loss improved from 1.03083 to 1.02414, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00015: loss improved from 1.02414 to 1.01689, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00016: loss improved from 1.01689 to 1.00957, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00017: loss improved from 1.00957 to 1.00227, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00018: loss improved from 1.00227 to 0.99492, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00019: loss improved from 0.99492 to 0.98667, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00020: loss improved from 0.98667 to 0.97922, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00021: loss improved from 0.97922 to 0.97275, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00022: loss improved from 0.97275 to 0.96426, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00023: loss improved from 0.96426 to 0.95640, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00024: loss improved from 0.95640 to 0.94938, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00025: loss improved from 0.94938 to 0.94119, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00026: loss improved from 0.94119 to 0.93394, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00027: loss improved from 0.93394 to 0.92625, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00028: loss improved from 0.92625 to 0.91851, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00029: loss improved from 0.91851 to 0.91041, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00030: loss improved from 0.91041 to 0.90375, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00031: loss improved from 0.90375 to 0.89687, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00032: loss improved from 0.89687 to 0.88910, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00033: loss improved from 0.88910 to 0.88257, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00034: loss improved from 0.88257 to 0.87538, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00035: loss improved from 0.87538 to 0.86851, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00036: loss improved from 0.86851 to 0.86475, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00037: loss improved from 0.86475 to 0.85650, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00038: loss improved from 0.85650 to 0.85047, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00039: loss improved from 0.85047 to 0.84484, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00040: loss improved from 0.84484 to 0.83942, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00041: loss improved from 0.83942 to 0.83346, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00042: loss improved from 0.83346 to 0.82810, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00043: loss improved from 0.82810 to 0.82295, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00044: loss improved from 0.82295 to 0.81729, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00045: loss improved from 0.81729 to 0.81267, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00046: loss improved from 0.81267 to 0.80896, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00047: loss improved from 0.80896 to 0.80303, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00048: loss improved from 0.80303 to 0.79803, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00049: loss improved from 0.79803 to 0.79392, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00050: loss improved from 0.79392 to 0.79171, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00051: loss improved from 0.79171 to 0.78553, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00052: loss improved from 0.78553 to 0.78156, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00053: loss improved from 0.78156 to 0.77864, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00054: loss improved from 0.77864 to 0.77426, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00055: loss improved from 0.77426 to 0.77060, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00056: loss improved from 0.77060 to 0.76634, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00057: loss improved from 0.76634 to 0.76261, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00058: loss improved from 0.76261 to 0.75858, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00059: loss improved from 0.75858 to 0.75499, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00060: loss improved from 0.75499 to 0.75174, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00061: loss improved from 0.75174 to 0.74799, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00062: loss improved from 0.74799 to 0.74526, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00063: loss improved from 0.74526 to 0.74422, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00064: loss improved from 0.74422 to 0.73883, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00065: loss improved from 0.73883 to 0.73651, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00066: loss improved from 0.73651 to 0.73422, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00067: loss improved from 0.73422 to 0.72981, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00068: loss improved from 0.72981 to 0.72726, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00069: loss improved from 0.72726 to 0.72354, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00070: loss did not improve from 0.72354\n",
      "\n",
      "Epoch 00071: loss improved from 0.72354 to 0.71804, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00072: loss improved from 0.71804 to 0.71583, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00073: loss improved from 0.71583 to 0.71277, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00074: loss improved from 0.71277 to 0.70990, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00075: loss improved from 0.70990 to 0.70683, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00076: loss improved from 0.70683 to 0.70512, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00077: loss improved from 0.70512 to 0.70119, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00078: loss improved from 0.70119 to 0.69849, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00079: loss improved from 0.69849 to 0.69766, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00080: loss improved from 0.69766 to 0.69333, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00081: loss improved from 0.69333 to 0.69033, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00082: loss improved from 0.69033 to 0.68749, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00083: loss improved from 0.68749 to 0.68718, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00084: loss improved from 0.68718 to 0.68229, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00085: loss improved from 0.68229 to 0.68058, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00086: loss improved from 0.68058 to 0.67698, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00087: loss improved from 0.67698 to 0.67532, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00088: loss improved from 0.67532 to 0.67250, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00089: loss improved from 0.67250 to 0.66902, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00090: loss improved from 0.66902 to 0.66679, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00091: loss improved from 0.66679 to 0.66328, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00092: loss improved from 0.66328 to 0.66144, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00093: loss improved from 0.66144 to 0.65755, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00094: loss improved from 0.65755 to 0.65445, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00095: loss improved from 0.65445 to 0.65291, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00096: loss improved from 0.65291 to 0.64896, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00097: loss improved from 0.64896 to 0.64671, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00098: loss improved from 0.64671 to 0.64501, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00099: loss improved from 0.64501 to 0.64071, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00100: loss improved from 0.64071 to 0.63748, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00101: loss improved from 0.63748 to 0.63419, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00102: loss improved from 0.63419 to 0.63183, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00103: loss improved from 0.63183 to 0.62874, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00104: loss improved from 0.62874 to 0.62542, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00105: loss improved from 0.62542 to 0.62323, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00106: loss improved from 0.62323 to 0.62135, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00107: loss improved from 0.62135 to 0.61686, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00108: loss improved from 0.61686 to 0.61630, saving model to model_part2.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00109: loss improved from 0.61630 to 0.61183, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00110: loss improved from 0.61183 to 0.60840, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00111: loss improved from 0.60840 to 0.60672, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00112: loss improved from 0.60672 to 0.60257, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00113: loss improved from 0.60257 to 0.59879, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00114: loss improved from 0.59879 to 0.59662, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00115: loss improved from 0.59662 to 0.59314, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00116: loss improved from 0.59314 to 0.59005, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00117: loss improved from 0.59005 to 0.58781, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00118: loss improved from 0.58781 to 0.58426, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00119: loss improved from 0.58426 to 0.58084, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00120: loss improved from 0.58084 to 0.57751, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00121: loss improved from 0.57751 to 0.57450, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00122: loss improved from 0.57450 to 0.57020, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00123: loss improved from 0.57020 to 0.56743, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00124: loss improved from 0.56743 to 0.56631, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00125: loss improved from 0.56631 to 0.56053, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00126: loss improved from 0.56053 to 0.55759, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00127: loss improved from 0.55759 to 0.55326, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00128: loss improved from 0.55326 to 0.55063, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00129: loss improved from 0.55063 to 0.54613, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00130: loss improved from 0.54613 to 0.54234, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00131: loss improved from 0.54234 to 0.54050, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00132: loss improved from 0.54050 to 0.53493, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00133: loss improved from 0.53493 to 0.53087, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00134: loss improved from 0.53087 to 0.52796, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00135: loss improved from 0.52796 to 0.52350, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00136: loss improved from 0.52350 to 0.51925, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00137: loss improved from 0.51925 to 0.51443, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00138: loss improved from 0.51443 to 0.51186, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00139: loss improved from 0.51186 to 0.50581, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00140: loss improved from 0.50581 to 0.50235, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00141: loss improved from 0.50235 to 0.49921, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00142: loss improved from 0.49921 to 0.49220, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00143: loss improved from 0.49220 to 0.48800, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00144: loss improved from 0.48800 to 0.48405, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00145: loss improved from 0.48405 to 0.47971, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00146: loss improved from 0.47971 to 0.47444, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00147: loss improved from 0.47444 to 0.46913, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00148: loss improved from 0.46913 to 0.46344, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00149: loss improved from 0.46344 to 0.45946, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00150: loss improved from 0.45946 to 0.45574, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00151: loss improved from 0.45574 to 0.44946, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00152: loss improved from 0.44946 to 0.44443, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00153: loss improved from 0.44443 to 0.43905, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00154: loss improved from 0.43905 to 0.43354, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00155: loss improved from 0.43354 to 0.42858, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00156: loss improved from 0.42858 to 0.42359, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00157: loss improved from 0.42359 to 0.41933, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00158: loss improved from 0.41933 to 0.41263, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00159: loss improved from 0.41263 to 0.40854, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00160: loss improved from 0.40854 to 0.40399, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00161: loss improved from 0.40399 to 0.39811, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00162: loss improved from 0.39811 to 0.39257, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00163: loss improved from 0.39257 to 0.38694, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00164: loss improved from 0.38694 to 0.38169, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00165: loss improved from 0.38169 to 0.37818, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00166: loss improved from 0.37818 to 0.37123, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00167: loss improved from 0.37123 to 0.36866, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00168: loss improved from 0.36866 to 0.36052, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00169: loss improved from 0.36052 to 0.35552, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00170: loss improved from 0.35552 to 0.35091, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00171: loss improved from 0.35091 to 0.34450, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00172: loss improved from 0.34450 to 0.33879, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00173: loss improved from 0.33879 to 0.33553, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00174: loss improved from 0.33553 to 0.33302, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00175: loss improved from 0.33302 to 0.32530, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00176: loss improved from 0.32530 to 0.31933, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00177: loss improved from 0.31933 to 0.31632, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00178: loss improved from 0.31632 to 0.30973, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00179: loss improved from 0.30973 to 0.30469, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00180: loss improved from 0.30469 to 0.29937, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00181: loss improved from 0.29937 to 0.29577, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00182: loss improved from 0.29577 to 0.29569, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00183: loss improved from 0.29569 to 0.28451, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00184: loss improved from 0.28451 to 0.28094, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00185: loss improved from 0.28094 to 0.27589, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00186: loss improved from 0.27589 to 0.27186, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00187: loss improved from 0.27186 to 0.26666, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00188: loss improved from 0.26666 to 0.26286, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00189: loss improved from 0.26286 to 0.25609, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00190: loss improved from 0.25609 to 0.25290, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00191: loss improved from 0.25290 to 0.24856, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00192: loss improved from 0.24856 to 0.24232, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00193: loss improved from 0.24232 to 0.23926, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00194: loss improved from 0.23926 to 0.23774, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00195: loss improved from 0.23774 to 0.23015, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00196: loss improved from 0.23015 to 0.22696, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00197: loss improved from 0.22696 to 0.22187, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00198: loss improved from 0.22187 to 0.21725, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00199: loss improved from 0.21725 to 0.21323, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00200: loss improved from 0.21323 to 0.20831, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00201: loss improved from 0.20831 to 0.20589, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00202: loss improved from 0.20589 to 0.20132, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00203: loss improved from 0.20132 to 0.19599, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00204: loss improved from 0.19599 to 0.19155, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00205: loss improved from 0.19155 to 0.18855, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00206: loss improved from 0.18855 to 0.18498, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00207: loss improved from 0.18498 to 0.18276, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00208: loss improved from 0.18276 to 0.17778, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00209: loss improved from 0.17778 to 0.17233, saving model to model_part2.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00210: loss improved from 0.17233 to 0.16995, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00211: loss improved from 0.16995 to 0.16524, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00212: loss improved from 0.16524 to 0.16219, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00213: loss improved from 0.16219 to 0.16071, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00214: loss improved from 0.16071 to 0.15412, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00215: loss improved from 0.15412 to 0.15363, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00216: loss improved from 0.15363 to 0.14768, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00217: loss improved from 0.14768 to 0.14640, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00218: loss improved from 0.14640 to 0.14465, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00219: loss improved from 0.14465 to 0.13945, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00220: loss improved from 0.13945 to 0.13603, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00221: loss improved from 0.13603 to 0.13297, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00222: loss improved from 0.13297 to 0.13068, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00223: loss improved from 0.13068 to 0.12824, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00224: loss improved from 0.12824 to 0.12433, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00225: loss improved from 0.12433 to 0.12358, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00226: loss improved from 0.12358 to 0.12002, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00227: loss improved from 0.12002 to 0.11716, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00228: loss improved from 0.11716 to 0.11592, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00229: loss improved from 0.11592 to 0.11209, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00230: loss improved from 0.11209 to 0.10978, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00231: loss improved from 0.10978 to 0.10826, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00232: loss improved from 0.10826 to 0.10443, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00233: loss improved from 0.10443 to 0.10233, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00234: loss improved from 0.10233 to 0.09984, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00235: loss did not improve from 0.09984\n",
      "\n",
      "Epoch 00236: loss did not improve from 0.09984\n",
      "\n",
      "Epoch 00237: loss improved from 0.09984 to 0.09503, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00238: loss improved from 0.09503 to 0.09296, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00239: loss improved from 0.09296 to 0.09229, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00240: loss improved from 0.09229 to 0.08967, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00241: loss improved from 0.08967 to 0.08691, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00242: loss improved from 0.08691 to 0.08592, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00243: loss improved from 0.08592 to 0.08507, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00244: loss improved from 0.08507 to 0.08426, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00245: loss improved from 0.08426 to 0.08067, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00246: loss improved from 0.08067 to 0.07967, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00247: loss did not improve from 0.07967\n",
      "\n",
      "Epoch 00248: loss improved from 0.07967 to 0.07633, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00249: loss improved from 0.07633 to 0.07568, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00250: loss improved from 0.07568 to 0.07531, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00251: loss improved from 0.07531 to 0.07286, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00252: loss improved from 0.07286 to 0.07128, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00253: loss did not improve from 0.07128\n",
      "\n",
      "Epoch 00254: loss improved from 0.07128 to 0.06909, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00255: loss improved from 0.06909 to 0.06845, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00256: loss improved from 0.06845 to 0.06698, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00257: loss improved from 0.06698 to 0.06531, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00258: loss did not improve from 0.06531\n",
      "\n",
      "Epoch 00259: loss did not improve from 0.06531\n",
      "\n",
      "Epoch 00260: loss improved from 0.06531 to 0.06383, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00261: loss improved from 0.06383 to 0.06197, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00262: loss did not improve from 0.06197\n",
      "\n",
      "Epoch 00263: loss improved from 0.06197 to 0.06014, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00264: loss improved from 0.06014 to 0.05916, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00265: loss improved from 0.05916 to 0.05857, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00266: loss improved from 0.05857 to 0.05767, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00267: loss did not improve from 0.05767\n",
      "\n",
      "Epoch 00268: loss improved from 0.05767 to 0.05651, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00269: loss improved from 0.05651 to 0.05578, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00270: loss did not improve from 0.05578\n",
      "\n",
      "Epoch 00271: loss improved from 0.05578 to 0.05324, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00272: loss improved from 0.05324 to 0.05191, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00273: loss did not improve from 0.05191\n",
      "\n",
      "Epoch 00274: loss improved from 0.05191 to 0.05080, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00275: loss improved from 0.05080 to 0.04943, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00276: loss improved from 0.04943 to 0.04872, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00277: loss improved from 0.04872 to 0.04759, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00278: loss did not improve from 0.04759\n",
      "\n",
      "Epoch 00279: loss improved from 0.04759 to 0.04520, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00280: loss improved from 0.04520 to 0.04366, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00281: loss improved from 0.04366 to 0.04286, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00282: loss did not improve from 0.04286\n",
      "\n",
      "Epoch 00283: loss did not improve from 0.04286\n",
      "\n",
      "Epoch 00284: loss improved from 0.04286 to 0.04092, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00285: loss improved from 0.04092 to 0.03845, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00286: loss did not improve from 0.03845\n",
      "\n",
      "Epoch 00287: loss improved from 0.03845 to 0.03728, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00288: loss improved from 0.03728 to 0.03548, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00289: loss improved from 0.03548 to 0.03465, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00290: loss did not improve from 0.03465\n",
      "\n",
      "Epoch 00291: loss improved from 0.03465 to 0.03386, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00292: loss improved from 0.03386 to 0.03141, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00293: loss did not improve from 0.03141\n",
      "\n",
      "Epoch 00294: loss did not improve from 0.03141\n",
      "\n",
      "Epoch 00295: loss did not improve from 0.03141\n",
      "\n",
      "Epoch 00296: loss improved from 0.03141 to 0.02849, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00297: loss did not improve from 0.02849\n",
      "\n",
      "Epoch 00298: loss did not improve from 0.02849\n",
      "\n",
      "Epoch 00299: loss did not improve from 0.02849\n",
      "\n",
      "Epoch 00300: loss improved from 0.02849 to 0.02821, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00301: loss improved from 0.02821 to 0.02672, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00302: loss did not improve from 0.02672\n",
      "\n",
      "Epoch 00303: loss did not improve from 0.02672\n",
      "\n",
      "Epoch 00304: loss did not improve from 0.02672\n",
      "\n",
      "Epoch 00305: loss improved from 0.02672 to 0.02427, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00306: loss did not improve from 0.02427\n",
      "\n",
      "Epoch 00307: loss did not improve from 0.02427\n",
      "\n",
      "Epoch 00308: loss did not improve from 0.02427\n",
      "\n",
      "Epoch 00309: loss did not improve from 0.02427\n",
      "\n",
      "Epoch 00310: loss improved from 0.02427 to 0.02182, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00311: loss did not improve from 0.02182\n",
      "\n",
      "Epoch 00312: loss did not improve from 0.02182\n",
      "\n",
      "Epoch 00313: loss did not improve from 0.02182\n",
      "\n",
      "Epoch 00314: loss did not improve from 0.02182\n",
      "\n",
      "Epoch 00315: loss improved from 0.02182 to 0.02113, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00316: loss did not improve from 0.02113\n",
      "\n",
      "Epoch 00317: loss improved from 0.02113 to 0.01987, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00318: loss improved from 0.01987 to 0.01962, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00319: loss did not improve from 0.01962\n",
      "\n",
      "Epoch 00320: loss did not improve from 0.01962\n",
      "\n",
      "Epoch 00321: loss did not improve from 0.01962\n",
      "\n",
      "Epoch 00322: loss improved from 0.01962 to 0.01929, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00323: loss improved from 0.01929 to 0.01882, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00324: loss improved from 0.01882 to 0.01702, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00325: loss improved from 0.01702 to 0.01636, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00326: loss did not improve from 0.01636\n",
      "\n",
      "Epoch 00327: loss improved from 0.01636 to 0.01538, saving model to model_part2.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00328: loss did not improve from 0.01538\n",
      "\n",
      "Epoch 00329: loss did not improve from 0.01538\n",
      "\n",
      "Epoch 00330: loss improved from 0.01538 to 0.01532, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00331: loss did not improve from 0.01532\n",
      "\n",
      "Epoch 00332: loss improved from 0.01532 to 0.01456, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00333: loss did not improve from 0.01456\n",
      "\n",
      "Epoch 00334: loss did not improve from 0.01456\n",
      "\n",
      "Epoch 00335: loss did not improve from 0.01456\n",
      "\n",
      "Epoch 00336: loss improved from 0.01456 to 0.01413, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00337: loss did not improve from 0.01413\n",
      "\n",
      "Epoch 00338: loss improved from 0.01413 to 0.01371, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00339: loss improved from 0.01371 to 0.01286, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00340: loss did not improve from 0.01286\n",
      "\n",
      "Epoch 00341: loss did not improve from 0.01286\n",
      "\n",
      "Epoch 00342: loss did not improve from 0.01286\n",
      "\n",
      "Epoch 00343: loss improved from 0.01286 to 0.01277, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00344: loss did not improve from 0.01277\n",
      "\n",
      "Epoch 00345: loss improved from 0.01277 to 0.01162, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00346: loss did not improve from 0.01162\n",
      "\n",
      "Epoch 00347: loss improved from 0.01162 to 0.01135, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00348: loss improved from 0.01135 to 0.01083, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00349: loss did not improve from 0.01083\n",
      "\n",
      "Epoch 00350: loss did not improve from 0.01083\n",
      "\n",
      "Epoch 00351: loss improved from 0.01083 to 0.00972, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00352: loss did not improve from 0.00972\n",
      "\n",
      "Epoch 00353: loss improved from 0.00972 to 0.00966, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00354: loss did not improve from 0.00966\n",
      "\n",
      "Epoch 00355: loss did not improve from 0.00966\n",
      "\n",
      "Epoch 00356: loss improved from 0.00966 to 0.00911, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00357: loss improved from 0.00911 to 0.00909, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00358: loss did not improve from 0.00909\n",
      "\n",
      "Epoch 00359: loss did not improve from 0.00909\n",
      "\n",
      "Epoch 00360: loss improved from 0.00909 to 0.00845, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00361: loss did not improve from 0.00845\n",
      "\n",
      "Epoch 00362: loss did not improve from 0.00845\n",
      "\n",
      "Epoch 00363: loss improved from 0.00845 to 0.00812, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00364: loss did not improve from 0.00812\n",
      "\n",
      "Epoch 00365: loss improved from 0.00812 to 0.00742, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00366: loss improved from 0.00742 to 0.00724, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00367: loss did not improve from 0.00724\n",
      "\n",
      "Epoch 00368: loss did not improve from 0.00724\n",
      "\n",
      "Epoch 00369: loss did not improve from 0.00724\n",
      "\n",
      "Epoch 00370: loss improved from 0.00724 to 0.00695, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00371: loss did not improve from 0.00695\n",
      "\n",
      "Epoch 00372: loss improved from 0.00695 to 0.00653, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00373: loss did not improve from 0.00653\n",
      "\n",
      "Epoch 00374: loss improved from 0.00653 to 0.00609, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00375: loss improved from 0.00609 to 0.00586, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00376: loss did not improve from 0.00586\n",
      "\n",
      "Epoch 00377: loss did not improve from 0.00586\n",
      "\n",
      "Epoch 00378: loss did not improve from 0.00586\n",
      "\n",
      "Epoch 00379: loss did not improve from 0.00586\n",
      "\n",
      "Epoch 00380: loss improved from 0.00586 to 0.00565, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00381: loss improved from 0.00565 to 0.00537, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00382: loss improved from 0.00537 to 0.00518, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00383: loss did not improve from 0.00518\n",
      "\n",
      "Epoch 00384: loss did not improve from 0.00518\n",
      "\n",
      "Epoch 00385: loss improved from 0.00518 to 0.00491, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00386: loss improved from 0.00491 to 0.00472, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00387: loss improved from 0.00472 to 0.00470, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00388: loss did not improve from 0.00470\n",
      "\n",
      "Epoch 00389: loss did not improve from 0.00470\n",
      "\n",
      "Epoch 00390: loss did not improve from 0.00470\n",
      "\n",
      "Epoch 00391: loss did not improve from 0.00470\n",
      "\n",
      "Epoch 00392: loss improved from 0.00470 to 0.00412, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00393: loss did not improve from 0.00412\n",
      "\n",
      "Epoch 00394: loss did not improve from 0.00412\n",
      "\n",
      "Epoch 00395: loss did not improve from 0.00412\n",
      "\n",
      "Epoch 00396: loss did not improve from 0.00412\n",
      "\n",
      "Epoch 00397: loss did not improve from 0.00412\n",
      "\n",
      "Epoch 00398: loss did not improve from 0.00412\n",
      "\n",
      "Epoch 00399: loss improved from 0.00412 to 0.00372, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00400: loss did not improve from 0.00372\n",
      "\n",
      "Epoch 00401: loss did not improve from 0.00372\n",
      "\n",
      "Epoch 00402: loss improved from 0.00372 to 0.00351, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00403: loss improved from 0.00351 to 0.00332, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00404: loss improved from 0.00332 to 0.00324, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00405: loss did not improve from 0.00324\n",
      "\n",
      "Epoch 00406: loss did not improve from 0.00324\n",
      "\n",
      "Epoch 00407: loss did not improve from 0.00324\n",
      "\n",
      "Epoch 00408: loss improved from 0.00324 to 0.00303, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00409: loss improved from 0.00303 to 0.00282, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00410: loss did not improve from 0.00282\n",
      "\n",
      "Epoch 00411: loss did not improve from 0.00282\n",
      "\n",
      "Epoch 00412: loss did not improve from 0.00282\n",
      "\n",
      "Epoch 00413: loss did not improve from 0.00282\n",
      "\n",
      "Epoch 00414: loss did not improve from 0.00282\n",
      "\n",
      "Epoch 00415: loss did not improve from 0.00282\n",
      "\n",
      "Epoch 00416: loss improved from 0.00282 to 0.00233, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00417: loss did not improve from 0.00233\n",
      "\n",
      "Epoch 00418: loss improved from 0.00233 to 0.00218, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00419: loss did not improve from 0.00218\n",
      "\n",
      "Epoch 00420: loss did not improve from 0.00218\n",
      "\n",
      "Epoch 00421: loss did not improve from 0.00218\n",
      "\n",
      "Epoch 00422: loss did not improve from 0.00218\n",
      "\n",
      "Epoch 00423: loss did not improve from 0.00218\n",
      "\n",
      "Epoch 00424: loss did not improve from 0.00218\n",
      "\n",
      "Epoch 00425: loss did not improve from 0.00218\n",
      "\n",
      "Epoch 00426: loss improved from 0.00218 to 0.00199, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00427: loss did not improve from 0.00199\n",
      "\n",
      "Epoch 00428: loss did not improve from 0.00199\n",
      "\n",
      "Epoch 00429: loss improved from 0.00199 to 0.00181, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00430: loss improved from 0.00181 to 0.00171, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00431: loss did not improve from 0.00171\n",
      "\n",
      "Epoch 00432: loss improved from 0.00171 to 0.00156, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00433: loss did not improve from 0.00156\n",
      "\n",
      "Epoch 00434: loss did not improve from 0.00156\n",
      "\n",
      "Epoch 00435: loss did not improve from 0.00156\n",
      "\n",
      "Epoch 00436: loss did not improve from 0.00156\n",
      "\n",
      "Epoch 00437: loss improved from 0.00156 to 0.00151, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00438: loss improved from 0.00151 to 0.00145, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00439: loss improved from 0.00145 to 0.00140, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00440: loss did not improve from 0.00140\n",
      "\n",
      "Epoch 00441: loss did not improve from 0.00140\n",
      "\n",
      "Epoch 00442: loss did not improve from 0.00140\n",
      "\n",
      "Epoch 00443: loss did not improve from 0.00140\n",
      "\n",
      "Epoch 00444: loss did not improve from 0.00140\n",
      "\n",
      "Epoch 00445: loss improved from 0.00140 to 0.00118, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00446: loss did not improve from 0.00118\n",
      "\n",
      "Epoch 00447: loss did not improve from 0.00118\n",
      "\n",
      "Epoch 00448: loss improved from 0.00118 to 0.00117, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00449: loss did not improve from 0.00117\n",
      "\n",
      "Epoch 00450: loss did not improve from 0.00117\n",
      "\n",
      "Epoch 00451: loss improved from 0.00117 to 0.00116, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00452: loss improved from 0.00116 to 0.00096, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00453: loss did not improve from 0.00096\n",
      "\n",
      "Epoch 00454: loss did not improve from 0.00096\n",
      "\n",
      "Epoch 00455: loss did not improve from 0.00096\n",
      "\n",
      "Epoch 00456: loss improved from 0.00096 to 0.00092, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00457: loss did not improve from 0.00092\n",
      "\n",
      "Epoch 00458: loss did not improve from 0.00092\n",
      "\n",
      "Epoch 00459: loss improved from 0.00092 to 0.00090, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00460: loss improved from 0.00090 to 0.00084, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00461: loss did not improve from 0.00084\n",
      "\n",
      "Epoch 00462: loss did not improve from 0.00084\n",
      "\n",
      "Epoch 00463: loss did not improve from 0.00084\n",
      "\n",
      "Epoch 00464: loss improved from 0.00084 to 0.00072, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00465: loss improved from 0.00072 to 0.00069, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00466: loss did not improve from 0.00069\n",
      "\n",
      "Epoch 00467: loss did not improve from 0.00069\n",
      "\n",
      "Epoch 00468: loss did not improve from 0.00069\n",
      "\n",
      "Epoch 00469: loss did not improve from 0.00069\n",
      "\n",
      "Epoch 00470: loss did not improve from 0.00069\n",
      "\n",
      "Epoch 00471: loss improved from 0.00069 to 0.00063, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00472: loss improved from 0.00063 to 0.00059, saving model to model_part2.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00473: loss did not improve from 0.00059\n",
      "\n",
      "Epoch 00474: loss did not improve from 0.00059\n",
      "\n",
      "Epoch 00475: loss improved from 0.00059 to 0.00059, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00476: loss did not improve from 0.00059\n",
      "\n",
      "Epoch 00477: loss improved from 0.00059 to 0.00050, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00478: loss did not improve from 0.00050\n",
      "\n",
      "Epoch 00479: loss did not improve from 0.00050\n",
      "\n",
      "Epoch 00480: loss did not improve from 0.00050\n",
      "\n",
      "Epoch 00481: loss did not improve from 0.00050\n",
      "\n",
      "Epoch 00482: loss did not improve from 0.00050\n",
      "\n",
      "Epoch 00483: loss improved from 0.00050 to 0.00048, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00484: loss did not improve from 0.00048\n",
      "\n",
      "Epoch 00485: loss did not improve from 0.00048\n",
      "\n",
      "Epoch 00486: loss improved from 0.00048 to 0.00047, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00487: loss improved from 0.00047 to 0.00043, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00488: loss improved from 0.00043 to 0.00041, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00489: loss improved from 0.00041 to 0.00041, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00490: loss did not improve from 0.00041\n",
      "\n",
      "Epoch 00491: loss did not improve from 0.00041\n",
      "\n",
      "Epoch 00492: loss did not improve from 0.00041\n",
      "\n",
      "Epoch 00493: loss improved from 0.00041 to 0.00036, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00494: loss improved from 0.00036 to 0.00033, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00495: loss did not improve from 0.00033\n",
      "\n",
      "Epoch 00496: loss did not improve from 0.00033\n",
      "\n",
      "Epoch 00497: loss improved from 0.00033 to 0.00029, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00498: loss improved from 0.00029 to 0.00027, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00499: loss did not improve from 0.00027\n",
      "\n",
      "Epoch 00500: loss did not improve from 0.00027\n",
      "\n",
      "Epoch 00501: loss did not improve from 0.00027\n",
      "\n",
      "Epoch 00502: loss did not improve from 0.00027\n",
      "\n",
      "Epoch 00503: loss improved from 0.00027 to 0.00026, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00504: loss did not improve from 0.00026\n",
      "\n",
      "Epoch 00505: loss improved from 0.00026 to 0.00025, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00506: loss improved from 0.00025 to 0.00023, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00507: loss did not improve from 0.00023\n",
      "\n",
      "Epoch 00508: loss did not improve from 0.00023\n",
      "\n",
      "Epoch 00509: loss improved from 0.00023 to 0.00022, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00510: loss improved from 0.00022 to 0.00019, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00511: loss did not improve from 0.00019\n",
      "\n",
      "Epoch 00512: loss did not improve from 0.00019\n",
      "\n",
      "Epoch 00513: loss improved from 0.00019 to 0.00019, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00514: loss improved from 0.00019 to 0.00019, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00515: loss improved from 0.00019 to 0.00019, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00516: loss did not improve from 0.00019\n",
      "\n",
      "Epoch 00517: loss improved from 0.00019 to 0.00016, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00518: loss improved from 0.00016 to 0.00015, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00519: loss did not improve from 0.00015\n",
      "\n",
      "Epoch 00520: loss did not improve from 0.00015\n",
      "\n",
      "Epoch 00521: loss did not improve from 0.00015\n",
      "\n",
      "Epoch 00522: loss improved from 0.00015 to 0.00015, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00523: loss did not improve from 0.00015\n",
      "\n",
      "Epoch 00524: loss improved from 0.00015 to 0.00014, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00525: loss did not improve from 0.00014\n",
      "\n",
      "Epoch 00526: loss improved from 0.00014 to 0.00014, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00527: loss improved from 0.00014 to 0.00012, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00528: loss did not improve from 0.00012\n",
      "\n",
      "Epoch 00529: loss improved from 0.00012 to 0.00012, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00530: loss did not improve from 0.00012\n",
      "\n",
      "Epoch 00531: loss improved from 0.00012 to 0.00010, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00532: loss did not improve from 0.00010\n",
      "\n",
      "Epoch 00533: loss did not improve from 0.00010\n",
      "\n",
      "Epoch 00534: loss improved from 0.00010 to 0.00010, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00535: loss did not improve from 0.00010\n",
      "\n",
      "Epoch 00536: loss improved from 0.00010 to 0.00009, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00537: loss did not improve from 0.00009\n",
      "\n",
      "Epoch 00538: loss improved from 0.00009 to 0.00008, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00539: loss improved from 0.00008 to 0.00008, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00540: loss improved from 0.00008 to 0.00008, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00541: loss improved from 0.00008 to 0.00007, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00542: loss did not improve from 0.00007\n",
      "\n",
      "Epoch 00543: loss did not improve from 0.00007\n",
      "\n",
      "Epoch 00544: loss improved from 0.00007 to 0.00006, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00545: loss did not improve from 0.00006\n",
      "\n",
      "Epoch 00546: loss did not improve from 0.00006\n",
      "\n",
      "Epoch 00547: loss did not improve from 0.00006\n",
      "\n",
      "Epoch 00548: loss improved from 0.00006 to 0.00006, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00549: loss improved from 0.00006 to 0.00006, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00550: loss did not improve from 0.00006\n",
      "\n",
      "Epoch 00551: loss did not improve from 0.00006\n",
      "\n",
      "Epoch 00552: loss improved from 0.00006 to 0.00005, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00553: loss improved from 0.00005 to 0.00005, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00554: loss did not improve from 0.00005\n",
      "\n",
      "Epoch 00555: loss did not improve from 0.00005\n",
      "\n",
      "Epoch 00556: loss did not improve from 0.00005\n",
      "\n",
      "Epoch 00557: loss improved from 0.00005 to 0.00004, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00558: loss improved from 0.00004 to 0.00004, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00559: loss did not improve from 0.00004\n",
      "\n",
      "Epoch 00560: loss did not improve from 0.00004\n",
      "\n",
      "Epoch 00561: loss did not improve from 0.00004\n",
      "\n",
      "Epoch 00562: loss did not improve from 0.00004\n",
      "\n",
      "Epoch 00563: loss improved from 0.00004 to 0.00004, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00564: loss improved from 0.00004 to 0.00003, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00565: loss improved from 0.00003 to 0.00003, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00566: loss did not improve from 0.00003\n",
      "\n",
      "Epoch 00567: loss improved from 0.00003 to 0.00003, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00568: loss did not improve from 0.00003\n",
      "\n",
      "Epoch 00569: loss did not improve from 0.00003\n",
      "\n",
      "Epoch 00570: loss did not improve from 0.00003\n",
      "\n",
      "Epoch 00571: loss did not improve from 0.00003\n",
      "\n",
      "Epoch 00572: loss improved from 0.00003 to 0.00003, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00573: loss improved from 0.00003 to 0.00003, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00574: loss improved from 0.00003 to 0.00002, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00575: loss did not improve from 0.00002\n",
      "\n",
      "Epoch 00576: loss improved from 0.00002 to 0.00002, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00577: loss did not improve from 0.00002\n",
      "\n",
      "Epoch 00578: loss improved from 0.00002 to 0.00002, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00579: loss improved from 0.00002 to 0.00002, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00580: loss did not improve from 0.00002\n",
      "\n",
      "Epoch 00581: loss did not improve from 0.00002\n",
      "\n",
      "Epoch 00582: loss improved from 0.00002 to 0.00002, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00583: loss improved from 0.00002 to 0.00002, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00584: loss did not improve from 0.00002\n",
      "\n",
      "Epoch 00585: loss did not improve from 0.00002\n",
      "\n",
      "Epoch 00586: loss did not improve from 0.00002\n",
      "\n",
      "Epoch 00587: loss improved from 0.00002 to 0.00001, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00588: loss improved from 0.00001 to 0.00001, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00589: loss did not improve from 0.00001\n",
      "\n",
      "Epoch 00590: loss did not improve from 0.00001\n",
      "\n",
      "Epoch 00591: loss improved from 0.00001 to 0.00001, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00592: loss improved from 0.00001 to 0.00001, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00593: loss did not improve from 0.00001\n",
      "\n",
      "Epoch 00594: loss did not improve from 0.00001\n",
      "\n",
      "Epoch 00595: loss did not improve from 0.00001\n",
      "\n",
      "Epoch 00596: loss did not improve from 0.00001\n",
      "\n",
      "Epoch 00597: loss improved from 0.00001 to 0.00001, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00598: loss improved from 0.00001 to 0.00001, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00599: loss did not improve from 0.00001\n",
      "\n",
      "Epoch 00600: loss did not improve from 0.00001\n",
      "\n",
      "Epoch 00601: loss improved from 0.00001 to 0.00001, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00602: loss did not improve from 0.00001\n",
      "\n",
      "Epoch 00603: loss did not improve from 0.00001\n",
      "\n",
      "Epoch 00604: loss improved from 0.00001 to 0.00001, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00605: loss improved from 0.00001 to 0.00001, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00606: loss did not improve from 0.00001\n",
      "\n",
      "Epoch 00607: loss improved from 0.00001 to 0.00001, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00608: loss did not improve from 0.00001\n",
      "\n",
      "Epoch 00609: loss did not improve from 0.00001\n",
      "\n",
      "Epoch 00610: loss did not improve from 0.00001\n",
      "\n",
      "Epoch 00611: loss improved from 0.00001 to 0.00001, saving model to model_part2.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00612: loss improved from 0.00001 to 0.00001, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00613: loss improved from 0.00001 to 0.00001, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00614: loss did not improve from 0.00001\n",
      "\n",
      "Epoch 00615: loss improved from 0.00001 to 0.00001, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00616: loss did not improve from 0.00001\n",
      "\n",
      "Epoch 00617: loss improved from 0.00001 to 0.00001, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00618: loss improved from 0.00001 to 0.00001, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00619: loss did not improve from 0.00001\n",
      "\n",
      "Epoch 00620: loss did not improve from 0.00001\n",
      "\n",
      "Epoch 00621: loss did not improve from 0.00001\n",
      "\n",
      "Epoch 00622: loss improved from 0.00001 to 0.00001, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00623: loss improved from 0.00001 to 0.00000, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00624: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00625: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00626: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00627: loss improved from 0.00000 to 0.00000, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00628: loss improved from 0.00000 to 0.00000, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00629: loss improved from 0.00000 to 0.00000, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00630: loss improved from 0.00000 to 0.00000, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00631: loss improved from 0.00000 to 0.00000, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00632: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00633: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00634: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00635: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00636: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00637: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00638: loss improved from 0.00000 to 0.00000, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00639: loss improved from 0.00000 to 0.00000, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00640: loss improved from 0.00000 to 0.00000, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00641: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00642: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00643: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00644: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00645: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00646: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00647: loss improved from 0.00000 to 0.00000, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00648: loss improved from 0.00000 to 0.00000, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00649: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00650: loss improved from 0.00000 to 0.00000, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00651: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00652: loss improved from 0.00000 to 0.00000, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00653: loss improved from 0.00000 to 0.00000, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00654: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00655: loss improved from 0.00000 to 0.00000, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00656: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00657: loss improved from 0.00000 to 0.00000, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00658: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00659: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00660: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00661: loss improved from 0.00000 to 0.00000, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00662: loss improved from 0.00000 to 0.00000, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00663: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00664: loss improved from 0.00000 to 0.00000, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00665: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00666: loss improved from 0.00000 to 0.00000, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00667: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00668: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00669: loss improved from 0.00000 to 0.00000, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00670: loss improved from 0.00000 to 0.00000, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00671: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00672: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00673: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00674: loss improved from 0.00000 to 0.00000, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00675: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00676: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00677: loss improved from 0.00000 to 0.00000, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00678: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00679: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00680: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00681: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00682: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00683: loss improved from 0.00000 to 0.00000, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00684: loss improved from 0.00000 to 0.00000, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00685: loss improved from 0.00000 to 0.00000, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00686: loss improved from 0.00000 to 0.00000, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00687: loss improved from 0.00000 to 0.00000, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00688: loss improved from 0.00000 to 0.00000, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00689: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00690: loss improved from 0.00000 to 0.00000, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00691: loss improved from 0.00000 to 0.00000, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00692: loss improved from 0.00000 to 0.00000, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00693: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00694: loss improved from 0.00000 to 0.00000, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00695: loss improved from 0.00000 to 0.00000, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00696: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00697: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00698: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00699: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00700: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00701: loss improved from 0.00000 to 0.00000, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00702: loss improved from 0.00000 to 0.00000, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00703: loss improved from 0.00000 to 0.00000, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00704: loss improved from 0.00000 to 0.00000, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00705: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00706: loss improved from 0.00000 to 0.00000, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00707: loss improved from 0.00000 to 0.00000, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00708: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00709: loss improved from 0.00000 to 0.00000, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00710: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00711: loss improved from 0.00000 to 0.00000, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00712: loss improved from 0.00000 to 0.00000, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00713: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00714: loss improved from 0.00000 to 0.00000, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00715: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00716: loss improved from 0.00000 to 0.00000, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00717: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00718: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00719: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00720: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00721: loss improved from 0.00000 to 0.00000, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00722: loss improved from 0.00000 to 0.00000, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00723: loss improved from 0.00000 to 0.00000, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00724: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00725: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00726: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00727: loss improved from 0.00000 to 0.00000, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00728: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00729: loss improved from 0.00000 to 0.00000, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00730: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00731: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00732: loss improved from 0.00000 to 0.00000, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00733: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00734: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00735: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00736: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00737: loss improved from 0.00000 to 0.00000, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00738: loss improved from 0.00000 to 0.00000, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00739: loss improved from 0.00000 to 0.00000, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00740: loss improved from 0.00000 to 0.00000, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00741: loss improved from 0.00000 to 0.00000, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00742: loss did not improve from 0.00000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00743: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00744: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00745: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00746: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00747: loss improved from 0.00000 to 0.00000, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00748: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00749: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00750: loss improved from 0.00000 to 0.00000, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00751: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00752: loss improved from 0.00000 to 0.00000, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00753: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00754: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00755: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00756: loss improved from 0.00000 to 0.00000, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00757: loss improved from 0.00000 to 0.00000, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00758: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00759: loss improved from 0.00000 to 0.00000, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00760: loss improved from 0.00000 to 0.00000, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00761: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00762: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00763: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00764: loss improved from 0.00000 to 0.00000, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00765: loss improved from 0.00000 to 0.00000, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00766: loss improved from 0.00000 to 0.00000, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00767: loss improved from 0.00000 to 0.00000, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00768: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00769: loss improved from 0.00000 to 0.00000, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00770: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00771: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00772: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00773: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00774: loss improved from 0.00000 to 0.00000, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00775: loss improved from 0.00000 to 0.00000, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00776: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00777: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00778: loss improved from 0.00000 to 0.00000, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00779: loss improved from 0.00000 to 0.00000, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00780: loss improved from 0.00000 to 0.00000, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00781: loss improved from 0.00000 to 0.00000, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00782: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00783: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00784: loss improved from 0.00000 to 0.00000, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00785: loss improved from 0.00000 to 0.00000, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00786: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00787: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00788: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00789: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00790: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00791: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00792: loss improved from 0.00000 to 0.00000, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00793: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00794: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00795: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00796: loss improved from 0.00000 to 0.00000, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00797: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00798: loss improved from 0.00000 to 0.00000, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00799: loss improved from 0.00000 to 0.00000, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00800: loss improved from 0.00000 to 0.00000, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00801: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00802: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00803: loss improved from 0.00000 to 0.00000, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00804: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00805: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00806: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00807: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00808: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00809: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00810: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00811: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00812: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00813: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00814: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00815: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00816: loss improved from 0.00000 to 0.00000, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00817: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00818: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00819: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00820: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00821: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00822: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00823: loss improved from 0.00000 to 0.00000, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00824: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00825: loss improved from 0.00000 to 0.00000, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00826: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00827: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00828: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00829: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00830: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00831: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00832: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00833: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00834: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00835: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00836: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00837: loss improved from 0.00000 to 0.00000, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00838: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00839: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00840: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00841: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00842: loss improved from 0.00000 to 0.00000, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00843: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00844: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00845: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00846: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00847: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00848: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00849: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00850: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00851: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00852: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00853: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00854: loss improved from 0.00000 to 0.00000, saving model to model_part2.hdf5\n",
      "\n",
      "Epoch 00855: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00856: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00857: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00858: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00859: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00860: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00861: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00862: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00863: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00864: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00865: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00866: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00867: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00868: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00869: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00870: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00871: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00872: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00873: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00874: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00875: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00876: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00877: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00878: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00879: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00880: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00881: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00882: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00883: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00884: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00885: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00886: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00887: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00888: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00889: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00890: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00891: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00892: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00893: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00894: loss did not improve from 0.00000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00895: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00896: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00897: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00898: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00899: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00900: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00901: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00902: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00903: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00904: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00905: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00906: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00907: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00908: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00909: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00910: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00911: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00912: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00913: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00914: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00915: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00916: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00917: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00918: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00919: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00920: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00921: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00922: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00923: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00924: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00925: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00926: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00927: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00928: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00929: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00930: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00931: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00932: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00933: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00934: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00935: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00936: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00937: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00938: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00939: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00940: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00941: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00942: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00943: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00944: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00945: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00946: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00947: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00948: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00949: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00950: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00951: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00952: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00953: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00954: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00955: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00956: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00957: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00958: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00959: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00960: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00961: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00962: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00963: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00964: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00965: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00966: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00967: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00968: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00969: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00970: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00971: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00972: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00973: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00974: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00975: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00976: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00977: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00978: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00979: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00980: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00981: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00982: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00983: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00984: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00985: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00986: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00987: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00988: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00989: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00990: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00991: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00992: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00993: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00994: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00995: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00996: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00997: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00998: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 00999: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01000: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01001: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01002: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01003: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01004: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01005: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01006: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01007: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01008: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01009: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01010: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01011: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01012: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01013: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01014: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01015: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01016: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01017: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01018: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01019: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01020: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01021: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01022: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01023: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01024: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01025: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01026: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01027: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01028: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01029: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01030: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01031: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01032: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01033: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01034: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01035: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01036: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01037: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01038: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01039: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01040: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01041: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01042: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01043: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01044: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01045: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01046: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01047: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01048: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01049: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01050: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01051: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01052: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01053: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01054: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01055: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01056: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01057: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01058: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01059: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01060: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01061: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01062: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01063: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01064: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01065: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01066: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01067: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01068: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01069: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01070: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01071: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01072: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01073: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01074: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01075: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01076: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01077: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01078: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01079: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01080: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01081: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01082: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01083: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01084: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01085: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01086: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01087: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01088: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01089: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01090: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01091: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01092: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01093: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01094: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01095: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01096: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01097: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01098: loss did not improve from 0.00000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 01099: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01100: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01101: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01102: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01103: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01104: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01105: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01106: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01107: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01108: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01109: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01110: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01111: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01112: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01113: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01114: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01115: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01116: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01117: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01118: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01119: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01120: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01121: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01122: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01123: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01124: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01125: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01126: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01127: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01128: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01129: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01130: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01131: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01132: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01133: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01134: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01135: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01136: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01137: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01138: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01139: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01140: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01141: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01142: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01143: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01144: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01145: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01146: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01147: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01148: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01149: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01150: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01151: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01152: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01153: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01154: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01155: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01156: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01157: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01158: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01159: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01160: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01161: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01162: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01163: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01164: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01165: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01166: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01167: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01168: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01169: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01170: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01171: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01172: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01173: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01174: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01175: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01176: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01177: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01178: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01179: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01180: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01181: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01182: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01183: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01184: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01185: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01186: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01187: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01188: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01189: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01190: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01191: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01192: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01193: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01194: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01195: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01196: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01197: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01198: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01199: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01200: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01201: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01202: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01203: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01204: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01205: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01206: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01207: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01208: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01209: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01210: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01211: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01212: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01213: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01214: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01215: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01216: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01217: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01218: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01219: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01220: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01221: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01222: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01223: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01224: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01225: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01226: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01227: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01228: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01229: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01230: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01231: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01232: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01233: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01234: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01235: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01236: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01237: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01238: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01239: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01240: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01241: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01242: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01243: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01244: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01245: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01246: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01247: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01248: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01249: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01250: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01251: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01252: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01253: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01254: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01255: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01256: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01257: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01258: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01259: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01260: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01261: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01262: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01263: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01264: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01265: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01266: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01267: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01268: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01269: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01270: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01271: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01272: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01273: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01274: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01275: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01276: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01277: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01278: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01279: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01280: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01281: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01282: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01283: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01284: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01285: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01286: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01287: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01288: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01289: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01290: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01291: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01292: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01293: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01294: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01295: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01296: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01297: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01298: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01299: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01300: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01301: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01302: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01303: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01304: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01305: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01306: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01307: loss did not improve from 0.00000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 01308: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01309: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01310: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01311: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01312: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01313: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01314: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01315: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01316: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01317: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01318: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01319: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01320: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01321: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01322: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01323: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01324: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01325: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01326: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01327: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01328: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01329: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01330: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01331: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01332: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01333: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01334: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01335: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01336: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01337: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01338: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01339: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01340: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01341: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01342: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01343: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01344: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01345: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01346: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01347: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01348: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01349: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01350: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01351: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01352: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01353: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01354: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01355: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01356: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01357: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01358: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01359: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01360: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01361: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01362: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01363: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01364: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01365: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01366: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01367: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01368: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01369: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01370: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01371: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01372: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01373: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01374: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01375: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01376: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01377: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01378: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01379: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01380: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01381: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01382: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01383: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01384: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01385: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01386: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01387: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01388: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01389: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01390: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01391: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01392: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01393: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01394: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01395: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01396: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01397: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01398: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01399: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01400: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01401: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01402: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01403: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01404: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01405: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01406: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01407: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01408: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01409: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01410: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01411: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01412: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01413: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01414: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01415: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01416: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01417: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01418: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01419: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01420: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01421: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01422: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01423: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01424: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01425: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01426: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01427: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01428: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01429: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01430: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01431: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01432: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01433: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01434: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01435: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01436: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01437: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01438: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01439: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01440: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01441: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01442: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01443: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01444: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01445: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01446: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01447: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01448: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01449: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01450: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01451: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01452: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01453: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01454: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01455: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01456: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01457: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01458: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01459: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01460: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01461: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01462: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01463: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01464: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01465: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01466: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01467: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01468: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01469: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01470: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01471: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01472: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01473: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01474: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01475: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01476: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01477: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01478: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01479: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01480: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01481: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01482: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01483: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01484: loss did not improve from 0.00000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 01485: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01486: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01487: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01488: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01489: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01490: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01491: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01492: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01493: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01494: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01495: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01496: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01497: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01498: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01499: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01500: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01501: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01502: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01503: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01504: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01505: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01506: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01507: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01508: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01509: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01510: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01511: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01512: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01513: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01514: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01515: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01516: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01517: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01518: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01519: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01520: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01521: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01522: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01523: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01524: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01525: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01526: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01527: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01528: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01529: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01530: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01531: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01532: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01533: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01534: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01535: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01536: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01537: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01538: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01539: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01540: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01541: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01542: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01543: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01544: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01545: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01546: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01547: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01548: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01549: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01550: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01551: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01552: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01553: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01554: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01555: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01556: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01557: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01558: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01559: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01560: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01561: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01562: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01563: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01564: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01565: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01566: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01567: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01568: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01569: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01570: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01571: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01572: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01573: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01574: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01575: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01576: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01577: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01578: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01579: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01580: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01581: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01582: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01583: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01584: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01585: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01586: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01587: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01588: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01589: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01590: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01591: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01592: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01593: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01594: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01595: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01596: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01597: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01598: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01599: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01600: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01601: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01602: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01603: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01604: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01605: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01606: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01607: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01608: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01609: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01610: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01611: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01612: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01613: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01614: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01615: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01616: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01617: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01618: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01619: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01620: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01621: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01622: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01623: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01624: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01625: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01626: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01627: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01628: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01629: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01630: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01631: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01632: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01633: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01634: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01635: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01636: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01637: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01638: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01639: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01640: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01641: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01642: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01643: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01644: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01645: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01646: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01647: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01648: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01649: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01650: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01651: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01652: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01653: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01654: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01655: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01656: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01657: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01658: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01659: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01660: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01661: loss did not improve from 0.00000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 01662: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01663: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01664: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01665: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01666: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01667: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01668: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01669: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01670: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01671: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01672: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01673: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01674: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01675: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01676: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01677: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01678: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01679: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01680: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01681: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01682: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01683: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01684: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01685: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01686: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01687: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01688: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01689: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01690: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01691: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01692: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01693: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01694: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01695: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01696: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01697: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01698: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01699: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01700: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01701: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01702: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01703: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01704: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01705: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01706: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01707: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01708: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01709: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01710: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01711: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01712: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01713: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01714: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01715: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01716: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01717: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01718: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01719: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01720: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01721: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01722: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01723: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01724: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01725: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01726: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01727: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01728: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01729: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01730: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01731: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01732: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01733: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01734: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01735: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01736: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01737: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01738: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01739: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01740: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01741: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01742: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01743: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01744: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01745: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01746: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01747: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01748: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01749: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01750: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01751: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01752: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01753: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01754: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01755: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01756: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01757: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01758: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01759: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01760: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01761: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01762: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01763: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01764: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01765: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01766: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01767: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01768: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01769: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01770: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01771: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01772: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01773: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01774: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01775: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01776: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01777: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01778: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01779: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01780: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01781: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01782: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01783: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01784: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01785: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01786: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01787: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01788: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01789: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01790: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01791: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01792: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01793: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01794: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01795: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01796: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01797: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01798: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01799: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01800: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01801: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01802: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01803: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01804: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01805: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01806: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01807: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01808: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01809: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01810: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01811: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01812: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01813: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01814: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01815: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01816: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01817: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01818: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01819: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01820: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01821: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01822: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01823: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01824: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01825: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01826: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01827: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01828: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01829: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01830: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01831: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01832: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01833: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01834: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01835: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01836: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01837: loss did not improve from 0.00000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 01838: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01839: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01840: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01841: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01842: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01843: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01844: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01845: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01846: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01847: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01848: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01849: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01850: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01851: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01852: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01853: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01854: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01855: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01856: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01857: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01858: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01859: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01860: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01861: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01862: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01863: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01864: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01865: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01866: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01867: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01868: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01869: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01870: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01871: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01872: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01873: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01874: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01875: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01876: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01877: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01878: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01879: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01880: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01881: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01882: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01883: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01884: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01885: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01886: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01887: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01888: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01889: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01890: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01891: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01892: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01893: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01894: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01895: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01896: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01897: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01898: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01899: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01900: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01901: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01902: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01903: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01904: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01905: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01906: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01907: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01908: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01909: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01910: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01911: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01912: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01913: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01914: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01915: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01916: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01917: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01918: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01919: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01920: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01921: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01922: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01923: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01924: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01925: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01926: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01927: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01928: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01929: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01930: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01931: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01932: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01933: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01934: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01935: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01936: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01937: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01938: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01939: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01940: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01941: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01942: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01943: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01944: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01945: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01946: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01947: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01948: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01949: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01950: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01951: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01952: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01953: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01954: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01955: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01956: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01957: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01958: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01959: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01960: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01961: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01962: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01963: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01964: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01965: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01966: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01967: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01968: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01969: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01970: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01971: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01972: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01973: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01974: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01975: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01976: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01977: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01978: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01979: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01980: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01981: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01982: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01983: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01984: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01985: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01986: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01987: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01988: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01989: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01990: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01991: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01992: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01993: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01994: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01995: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01996: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01997: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01998: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 01999: loss did not improve from 0.00000\n",
      "\n",
      "Epoch 02000: loss did not improve from 0.00000\n",
      "Testing the model\n",
      "45/45 [==============================] - 0s 2ms/step\n",
      "====================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "accuracy = []\n",
    "train_sizes = [0.20, 0.30, 0.40, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
    "\n",
    "accuracy = main(train_sizes, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_sizes, accuracy)\n",
    "plt.xlabel(\"Percent Training Size\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Percent Features vs Accuracy\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
