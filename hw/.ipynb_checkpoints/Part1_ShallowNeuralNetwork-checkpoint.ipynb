{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chloe Quinto \n",
    "CS 583 HW 1  \n",
    "4/13/20      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "import sklearn \n",
    "from sklearn import datasets\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical\n",
    "from keras import layers\n",
    "from keras import models \n",
    "from keras.layers import Dense\n",
    "from keras import optimizers \n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Preprocessing "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Load Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wine Features: ['alcohol', 'malic_acid', 'ash', 'alcalinity_of_ash', 'magnesium', 'total_phenols', 'flavanoids', 'nonflavanoid_phenols', 'proanthocyanins', 'color_intensity', 'hue', 'od280/od315_of_diluted_wines', 'proline']\n",
      "\n",
      "\n",
      "Wine Data: [[1.423e+01 1.710e+00 2.430e+00 ... 1.040e+00 3.920e+00 1.065e+03]\n",
      " [1.320e+01 1.780e+00 2.140e+00 ... 1.050e+00 3.400e+00 1.050e+03]\n",
      " [1.316e+01 2.360e+00 2.670e+00 ... 1.030e+00 3.170e+00 1.185e+03]\n",
      " ...\n",
      " [1.327e+01 4.280e+00 2.260e+00 ... 5.900e-01 1.560e+00 8.350e+02]\n",
      " [1.317e+01 2.590e+00 2.370e+00 ... 6.000e-01 1.620e+00 8.400e+02]\n",
      " [1.413e+01 4.100e+00 2.740e+00 ... 6.100e-01 1.600e+00 5.600e+02]]\n",
      "\n",
      "\n",
      "Wine Target: [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2]\n"
     ]
    }
   ],
   "source": [
    "wine = sklearn.datasets.load_wine()\n",
    "\n",
    "print(\"Wine Features:\", wine.feature_names)\n",
    "print('\\n')\n",
    "print(\"Wine Data:\", wine.data)\n",
    "print('\\n')\n",
    "print(\"Wine Target:\", wine.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Build Dataframe "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alcohol</th>\n",
       "      <th>malic_acid</th>\n",
       "      <th>ash</th>\n",
       "      <th>alcalinity_of_ash</th>\n",
       "      <th>magnesium</th>\n",
       "      <th>total_phenols</th>\n",
       "      <th>flavanoids</th>\n",
       "      <th>nonflavanoid_phenols</th>\n",
       "      <th>proanthocyanins</th>\n",
       "      <th>color_intensity</th>\n",
       "      <th>hue</th>\n",
       "      <th>od280/od315_of_diluted_wines</th>\n",
       "      <th>proline</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100.0</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113.0</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735.0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   alcohol  malic_acid   ash  alcalinity_of_ash  magnesium  total_phenols  \\\n",
       "0    14.23        1.71  2.43               15.6      127.0           2.80   \n",
       "1    13.20        1.78  2.14               11.2      100.0           2.65   \n",
       "2    13.16        2.36  2.67               18.6      101.0           2.80   \n",
       "3    14.37        1.95  2.50               16.8      113.0           3.85   \n",
       "4    13.24        2.59  2.87               21.0      118.0           2.80   \n",
       "\n",
       "   flavanoids  nonflavanoid_phenols  proanthocyanins  color_intensity   hue  \\\n",
       "0        3.06                  0.28             2.29             5.64  1.04   \n",
       "1        2.76                  0.26             1.28             4.38  1.05   \n",
       "2        3.24                  0.30             2.81             5.68  1.03   \n",
       "3        3.49                  0.24             2.18             7.80  0.86   \n",
       "4        2.69                  0.39             1.82             4.32  1.04   \n",
       "\n",
       "   od280/od315_of_diluted_wines  proline  class  \n",
       "0                          3.92   1065.0      0  \n",
       "1                          3.40   1050.0      0  \n",
       "2                          3.17   1185.0      0  \n",
       "3                          3.45   1480.0      0  \n",
       "4                          2.93    735.0      0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(wine.data)\n",
    "df.columns = wine.feature_names\n",
    "df[\"class\"] = wine.target\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Check for Null Values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "alcohol                         0\n",
       "malic_acid                      0\n",
       "ash                             0\n",
       "alcalinity_of_ash               0\n",
       "magnesium                       0\n",
       "total_phenols                   0\n",
       "flavanoids                      0\n",
       "nonflavanoid_phenols            0\n",
       "proanthocyanins                 0\n",
       "color_intensity                 0\n",
       "hue                             0\n",
       "od280/od315_of_diluted_wines    0\n",
       "proline                         0\n",
       "class                           0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Partition data into training and testing set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(178, 1)\n",
      "(178, 13)\n"
     ]
    }
   ],
   "source": [
    "labels = df.loc[:,[\"class\"]] \n",
    "features = df.drop([\"class\"],axis=1)\n",
    "print(labels.shape)\n",
    "print(features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain, xtest, ytrain, ytest = train_test_split(features, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convet class vectirs to binary class matrices\n",
    "ytrain = to_categorical(ytrain, 3)\n",
    "ytest = to_categorical(ytest,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xtrain (133, 13)\n",
      "xtest (45, 13)\n",
      "ytrain (133, 3)\n",
      "ytest (45, 3)\n"
     ]
    }
   ],
   "source": [
    "print(\"xtrain\", xtrain.shape)\n",
    "print(\"xtest\", xtest.shape)\n",
    "print(\"ytrain\", ytrain.shape)\n",
    "print(\"ytest\", ytest.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale data between 0 and 1 \n",
    "scale = MinMaxScaler(feature_range=(0,1))\n",
    "xtrain = scale.fit_transform(xtrain)\n",
    "xtest = scale.fit_transform(xtest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5  Randomly partition the training set to training and validation sets "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Randomly partition the 133 training samples into two sets \n",
    "* a training set containing  100 samples \n",
    "* a validation set containing 33 samples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of xtrain:  (100, 13)\n",
      "Shape of ytrain:  (100, 3)\n",
      "Shape of xval:  (33, 13)\n",
      "Shape of yval:  (33, 3)\n"
     ]
    }
   ],
   "source": [
    "rand_idx = np.random.permutation(133)\n",
    "train_idx = rand_idx[0:100]\n",
    "valid_idx = rand_idx[100:133]\n",
    "\n",
    "xval = xtrain[valid_idx,:]\n",
    "yval = ytrain[valid_idx,:]\n",
    "\n",
    "xtr = xtrain[train_idx, :]\n",
    "ytr = ytrain[train_idx,:]\n",
    "\n",
    "print(\"Shape of xtrain: \", xtr.shape)\n",
    "print(\"Shape of ytrain: \", ytr.shape)\n",
    "print(\"Shape of xval: \", xval.shape)\n",
    "print(\"Shape of yval: \", yval.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Build NN and tune its hyper-parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(10, input_dim=13,activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(6, activation='relu'))\n",
    "model.add(Dense(6, activation='relu'))\n",
    "model.add(Dense(4, activation='relu'))\n",
    "model.add(Dense(3, activation='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  2.1 Initial Weights and Biases "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights [[-0.22897628  0.37725383 -0.0926134  -0.43601793 -0.4804559  -0.06052336\n",
      "   0.01129556  0.42120236  0.49997848  0.35657   ]\n",
      " [-0.2190117   0.2457273   0.3940096   0.21852142  0.43790215 -0.25230402\n",
      "   0.08242571 -0.3466528  -0.3401684   0.28933114]\n",
      " [-0.44933432  0.36306685  0.43252683  0.12508911 -0.35598284  0.32851082\n",
      "  -0.05983207 -0.18854514 -0.21770105  0.41354465]\n",
      " [-0.47867167 -0.32333082 -0.2676612   0.14333284 -0.495076    0.3183878\n",
      "  -0.2551809  -0.07265502  0.06578678 -0.1188519 ]\n",
      " [-0.27695203 -0.27064696  0.1148743  -0.2463823   0.12206793  0.09502685\n",
      "  -0.4799095   0.3935392  -0.17361927  0.2888065 ]\n",
      " [-0.19385824  0.22443295  0.26920688  0.06914127  0.360779    0.18477964\n",
      "  -0.12794676  0.17325091  0.11879051  0.18650627]\n",
      " [ 0.47951907 -0.24004754 -0.20242545 -0.35490477 -0.48822272  0.21555895\n",
      "  -0.19565973  0.4096977  -0.18394941 -0.26803893]\n",
      " [-0.1990146   0.23820221 -0.18128893 -0.29543746 -0.34832415 -0.11081594\n",
      "  -0.11503771 -0.37069476  0.18211234 -0.32803077]\n",
      " [-0.36863253  0.24786115 -0.04822889  0.00953156 -0.15264592 -0.09053704\n",
      "   0.00661969 -0.14213577  0.48175496 -0.30906314]\n",
      " [ 0.12016433  0.18425143  0.4376235  -0.03966874 -0.16733578 -0.02828038\n",
      "  -0.24052185  0.05785006  0.24663949 -0.00997579]\n",
      " [-0.38262013 -0.05732     0.39166468  0.02906072 -0.49901047 -0.00347006\n",
      "  -0.35340357  0.46754247  0.14971298 -0.33958048]\n",
      " [-0.05104271 -0.2536904  -0.09860149 -0.30571473 -0.09590846 -0.21019483\n",
      "  -0.4140001  -0.4094929  -0.24324846  0.4717167 ]\n",
      " [-0.27694654 -0.41167557 -0.27409434  0.3318727   0.08840102  0.30069047\n",
      "   0.38692003  0.11612517  0.26525056 -0.46215668]]\n",
      "\n",
      "\n",
      "Bias [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "print(\"Weights\", model.layers[0].get_weights()[0])\n",
    "print(\"\\n\")\n",
    "print(\"Bias\", model.layers[0].get_weights()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 10)                140       \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 8)                 88        \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 6)                 54        \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 6)                 42        \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 4)                 28        \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 3)                 15        \n",
      "=================================================================\n",
      "Total params: 367\n",
      "Trainable params: 367\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\", optimizer = optimizers.RMSprop(lr = 1e-3), metrics = [\"acc\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 Save the weights of our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(\"best_model.hdf5\", monitor=\"loss\", verbose=1, save_best_only = True, mode=\"auto\", period=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 Apply early stopping to prevent overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=1, mode='auto',restore_best_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/anaconda3/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 100 samples, validate on 33 samples\n",
      "Epoch 1/2000\n",
      "100/100 [==============================] - 0s 2ms/step - loss: 1.1093 - acc: 0.0800 - val_loss: 1.0885 - val_acc: 0.2727\n",
      "\n",
      "Epoch 00001: loss improved from inf to 1.10933, saving model to best_model.hdf5\n",
      "Epoch 2/2000\n",
      "100/100 [==============================] - 0s 65us/step - loss: 1.0992 - acc: 0.2900 - val_loss: 1.0826 - val_acc: 0.4242\n",
      "\n",
      "Epoch 00002: loss improved from 1.10933 to 1.09920, saving model to best_model.hdf5\n",
      "Epoch 3/2000\n",
      "100/100 [==============================] - 0s 87us/step - loss: 1.0935 - acc: 0.3600 - val_loss: 1.0782 - val_acc: 0.4242\n",
      "\n",
      "Epoch 00003: loss improved from 1.09920 to 1.09347, saving model to best_model.hdf5\n",
      "Epoch 4/2000\n",
      "100/100 [==============================] - 0s 78us/step - loss: 1.0894 - acc: 0.3600 - val_loss: 1.0752 - val_acc: 0.4242\n",
      "\n",
      "Epoch 00004: loss improved from 1.09347 to 1.08936, saving model to best_model.hdf5\n",
      "Epoch 5/2000\n",
      "100/100 [==============================] - 0s 75us/step - loss: 1.0866 - acc: 0.3700 - val_loss: 1.0726 - val_acc: 0.4242\n",
      "\n",
      "Epoch 00005: loss improved from 1.08936 to 1.08659, saving model to best_model.hdf5\n",
      "Epoch 6/2000\n",
      "100/100 [==============================] - 0s 65us/step - loss: 1.0841 - acc: 0.4100 - val_loss: 1.0701 - val_acc: 0.4242\n",
      "\n",
      "Epoch 00006: loss improved from 1.08659 to 1.08414, saving model to best_model.hdf5\n",
      "Epoch 7/2000\n",
      "100/100 [==============================] - 0s 53us/step - loss: 1.0820 - acc: 0.4200 - val_loss: 1.0674 - val_acc: 0.4242\n",
      "\n",
      "Epoch 00007: loss improved from 1.08414 to 1.08200, saving model to best_model.hdf5\n",
      "Epoch 8/2000\n",
      "100/100 [==============================] - 0s 69us/step - loss: 1.0799 - acc: 0.4400 - val_loss: 1.0651 - val_acc: 0.4848\n",
      "\n",
      "Epoch 00008: loss improved from 1.08200 to 1.07986, saving model to best_model.hdf5\n",
      "Epoch 9/2000\n",
      "100/100 [==============================] - 0s 97us/step - loss: 1.0777 - acc: 0.4700 - val_loss: 1.0626 - val_acc: 0.4848\n",
      "\n",
      "Epoch 00009: loss improved from 1.07986 to 1.07775, saving model to best_model.hdf5\n",
      "Epoch 10/2000\n",
      "100/100 [==============================] - 0s 68us/step - loss: 1.0766 - acc: 0.4600 - val_loss: 1.0608 - val_acc: 0.5152\n",
      "\n",
      "Epoch 00010: loss improved from 1.07775 to 1.07662, saving model to best_model.hdf5\n",
      "Epoch 11/2000\n",
      "100/100 [==============================] - 0s 84us/step - loss: 1.0740 - acc: 0.5100 - val_loss: 1.0584 - val_acc: 0.4848\n",
      "\n",
      "Epoch 00011: loss improved from 1.07662 to 1.07400, saving model to best_model.hdf5\n",
      "Epoch 12/2000\n",
      "100/100 [==============================] - 0s 68us/step - loss: 1.0721 - acc: 0.5000 - val_loss: 1.0560 - val_acc: 0.4848\n",
      "\n",
      "Epoch 00012: loss improved from 1.07400 to 1.07213, saving model to best_model.hdf5\n",
      "Epoch 13/2000\n",
      "100/100 [==============================] - 0s 133us/step - loss: 1.0706 - acc: 0.5200 - val_loss: 1.0537 - val_acc: 0.4848\n",
      "\n",
      "Epoch 00013: loss improved from 1.07213 to 1.07056, saving model to best_model.hdf5\n",
      "Epoch 14/2000\n",
      "100/100 [==============================] - 0s 89us/step - loss: 1.0687 - acc: 0.5000 - val_loss: 1.0516 - val_acc: 0.4848\n",
      "\n",
      "Epoch 00014: loss improved from 1.07056 to 1.06869, saving model to best_model.hdf5\n",
      "Epoch 15/2000\n",
      "100/100 [==============================] - 0s 64us/step - loss: 1.0667 - acc: 0.5200 - val_loss: 1.0492 - val_acc: 0.4848\n",
      "\n",
      "Epoch 00015: loss improved from 1.06869 to 1.06666, saving model to best_model.hdf5\n",
      "Epoch 16/2000\n",
      "100/100 [==============================] - 0s 123us/step - loss: 1.0669 - acc: 0.4700 - val_loss: 1.0473 - val_acc: 0.4848\n",
      "\n",
      "Epoch 00016: loss did not improve from 1.06666\n",
      "Epoch 17/2000\n",
      "100/100 [==============================] - 0s 78us/step - loss: 1.0635 - acc: 0.5300 - val_loss: 1.0450 - val_acc: 0.4848\n",
      "\n",
      "Epoch 00017: loss improved from 1.06666 to 1.06354, saving model to best_model.hdf5\n",
      "Epoch 18/2000\n",
      "100/100 [==============================] - 0s 107us/step - loss: 1.0617 - acc: 0.5300 - val_loss: 1.0428 - val_acc: 0.5152\n",
      "\n",
      "Epoch 00018: loss improved from 1.06354 to 1.06166, saving model to best_model.hdf5\n",
      "Epoch 19/2000\n",
      "100/100 [==============================] - 0s 107us/step - loss: 1.0598 - acc: 0.5300 - val_loss: 1.0403 - val_acc: 0.5152\n",
      "\n",
      "Epoch 00019: loss improved from 1.06166 to 1.05981, saving model to best_model.hdf5\n",
      "Epoch 20/2000\n",
      "100/100 [==============================] - 0s 94us/step - loss: 1.0582 - acc: 0.5400 - val_loss: 1.0379 - val_acc: 0.5455\n",
      "\n",
      "Epoch 00020: loss improved from 1.05981 to 1.05820, saving model to best_model.hdf5\n",
      "Epoch 21/2000\n",
      "100/100 [==============================] - 0s 83us/step - loss: 1.0563 - acc: 0.5400 - val_loss: 1.0355 - val_acc: 0.5455\n",
      "\n",
      "Epoch 00021: loss improved from 1.05820 to 1.05630, saving model to best_model.hdf5\n",
      "Epoch 22/2000\n",
      "100/100 [==============================] - 0s 140us/step - loss: 1.0541 - acc: 0.5400 - val_loss: 1.0328 - val_acc: 0.5455\n",
      "\n",
      "Epoch 00022: loss improved from 1.05630 to 1.05413, saving model to best_model.hdf5\n",
      "Epoch 23/2000\n",
      "100/100 [==============================] - 0s 64us/step - loss: 1.0521 - acc: 0.5400 - val_loss: 1.0300 - val_acc: 0.5455\n",
      "\n",
      "Epoch 00023: loss improved from 1.05413 to 1.05211, saving model to best_model.hdf5\n",
      "Epoch 24/2000\n",
      "100/100 [==============================] - 0s 64us/step - loss: 1.0502 - acc: 0.5400 - val_loss: 1.0272 - val_acc: 0.5455\n",
      "\n",
      "Epoch 00024: loss improved from 1.05211 to 1.05024, saving model to best_model.hdf5\n",
      "Epoch 25/2000\n",
      "100/100 [==============================] - 0s 48us/step - loss: 1.0489 - acc: 0.5400 - val_loss: 1.0249 - val_acc: 0.5455\n",
      "\n",
      "Epoch 00025: loss improved from 1.05024 to 1.04889, saving model to best_model.hdf5\n",
      "Epoch 26/2000\n",
      "100/100 [==============================] - 0s 45us/step - loss: 1.0464 - acc: 0.5400 - val_loss: 1.0222 - val_acc: 0.5455\n",
      "\n",
      "Epoch 00026: loss improved from 1.04889 to 1.04639, saving model to best_model.hdf5\n",
      "Epoch 27/2000\n",
      "100/100 [==============================] - 0s 90us/step - loss: 1.0440 - acc: 0.5400 - val_loss: 1.0193 - val_acc: 0.5455\n",
      "\n",
      "Epoch 00027: loss improved from 1.04639 to 1.04402, saving model to best_model.hdf5\n",
      "Epoch 28/2000\n",
      "100/100 [==============================] - 0s 47us/step - loss: 1.0419 - acc: 0.5400 - val_loss: 1.0163 - val_acc: 0.5455\n",
      "\n",
      "Epoch 00028: loss improved from 1.04402 to 1.04188, saving model to best_model.hdf5\n",
      "Epoch 29/2000\n",
      "100/100 [==============================] - 0s 67us/step - loss: 1.0397 - acc: 0.5400 - val_loss: 1.0134 - val_acc: 0.5455\n",
      "\n",
      "Epoch 00029: loss improved from 1.04188 to 1.03974, saving model to best_model.hdf5\n",
      "Epoch 30/2000\n",
      "100/100 [==============================] - 0s 48us/step - loss: 1.0392 - acc: 0.5600 - val_loss: 1.0106 - val_acc: 0.5455\n",
      "\n",
      "Epoch 00030: loss improved from 1.03974 to 1.03916, saving model to best_model.hdf5\n",
      "Epoch 31/2000\n",
      "100/100 [==============================] - 0s 63us/step - loss: 1.0359 - acc: 0.5600 - val_loss: 1.0077 - val_acc: 0.5455\n",
      "\n",
      "Epoch 00031: loss improved from 1.03916 to 1.03586, saving model to best_model.hdf5\n",
      "Epoch 32/2000\n",
      "100/100 [==============================] - 0s 46us/step - loss: 1.0334 - acc: 0.5600 - val_loss: 1.0045 - val_acc: 0.5455\n",
      "\n",
      "Epoch 00032: loss improved from 1.03586 to 1.03336, saving model to best_model.hdf5\n",
      "Epoch 33/2000\n",
      "100/100 [==============================] - 0s 62us/step - loss: 1.0326 - acc: 0.5400 - val_loss: 1.0020 - val_acc: 0.5455\n",
      "\n",
      "Epoch 00033: loss improved from 1.03336 to 1.03258, saving model to best_model.hdf5\n",
      "Epoch 34/2000\n",
      "100/100 [==============================] - 0s 50us/step - loss: 1.0292 - acc: 0.5600 - val_loss: 0.9989 - val_acc: 0.5455\n",
      "\n",
      "Epoch 00034: loss improved from 1.03258 to 1.02915, saving model to best_model.hdf5\n",
      "Epoch 35/2000\n",
      "100/100 [==============================] - ETA: 0s - loss: 1.0234 - acc: 0.540 - 0s 71us/step - loss: 1.0277 - acc: 0.5600 - val_loss: 0.9961 - val_acc: 0.5455\n",
      "\n",
      "Epoch 00035: loss improved from 1.02915 to 1.02770, saving model to best_model.hdf5\n",
      "Epoch 36/2000\n",
      "100/100 [==============================] - 0s 60us/step - loss: 1.0255 - acc: 0.5600 - val_loss: 0.9934 - val_acc: 0.5455\n",
      "\n",
      "Epoch 00036: loss improved from 1.02770 to 1.02548, saving model to best_model.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/2000\n",
      "100/100 [==============================] - 0s 55us/step - loss: 1.0230 - acc: 0.5700 - val_loss: 0.9901 - val_acc: 0.5455\n",
      "\n",
      "Epoch 00037: loss improved from 1.02548 to 1.02295, saving model to best_model.hdf5\n",
      "Epoch 38/2000\n",
      "100/100 [==============================] - 0s 47us/step - loss: 1.0207 - acc: 0.5600 - val_loss: 0.9868 - val_acc: 0.5455\n",
      "\n",
      "Epoch 00038: loss improved from 1.02295 to 1.02068, saving model to best_model.hdf5\n",
      "Epoch 39/2000\n",
      "100/100 [==============================] - 0s 78us/step - loss: 1.0187 - acc: 0.5700 - val_loss: 0.9834 - val_acc: 0.5455\n",
      "\n",
      "Epoch 00039: loss improved from 1.02068 to 1.01868, saving model to best_model.hdf5\n",
      "Epoch 40/2000\n",
      "100/100 [==============================] - 0s 57us/step - loss: 1.0166 - acc: 0.5700 - val_loss: 0.9800 - val_acc: 0.5455\n",
      "\n",
      "Epoch 00040: loss improved from 1.01868 to 1.01655, saving model to best_model.hdf5\n",
      "Epoch 41/2000\n",
      "100/100 [==============================] - 0s 48us/step - loss: 1.0142 - acc: 0.5700 - val_loss: 0.9769 - val_acc: 0.5455\n",
      "\n",
      "Epoch 00041: loss improved from 1.01655 to 1.01416, saving model to best_model.hdf5\n",
      "Epoch 42/2000\n",
      "100/100 [==============================] - 0s 71us/step - loss: 1.0114 - acc: 0.5700 - val_loss: 0.9733 - val_acc: 0.5455\n",
      "\n",
      "Epoch 00042: loss improved from 1.01416 to 1.01140, saving model to best_model.hdf5\n",
      "Epoch 43/2000\n",
      "100/100 [==============================] - 0s 64us/step - loss: 1.0087 - acc: 0.5700 - val_loss: 0.9696 - val_acc: 0.5455\n",
      "\n",
      "Epoch 00043: loss improved from 1.01140 to 1.00873, saving model to best_model.hdf5\n",
      "Epoch 44/2000\n",
      "100/100 [==============================] - 0s 58us/step - loss: 1.0062 - acc: 0.5700 - val_loss: 0.9659 - val_acc: 0.5455\n",
      "\n",
      "Epoch 00044: loss improved from 1.00873 to 1.00624, saving model to best_model.hdf5\n",
      "Epoch 45/2000\n",
      "100/100 [==============================] - 0s 62us/step - loss: 1.0037 - acc: 0.5700 - val_loss: 0.9623 - val_acc: 0.5455\n",
      "\n",
      "Epoch 00045: loss improved from 1.00624 to 1.00371, saving model to best_model.hdf5\n",
      "Epoch 46/2000\n",
      "100/100 [==============================] - 0s 47us/step - loss: 1.0009 - acc: 0.5700 - val_loss: 0.9585 - val_acc: 0.5455\n",
      "\n",
      "Epoch 00046: loss improved from 1.00371 to 1.00093, saving model to best_model.hdf5\n",
      "Epoch 47/2000\n",
      "100/100 [==============================] - 0s 57us/step - loss: 0.9999 - acc: 0.5700 - val_loss: 0.9557 - val_acc: 0.5758\n",
      "\n",
      "Epoch 00047: loss improved from 1.00093 to 0.99985, saving model to best_model.hdf5\n",
      "Epoch 48/2000\n",
      "100/100 [==============================] - 0s 75us/step - loss: 0.9961 - acc: 0.5700 - val_loss: 0.9516 - val_acc: 0.5758\n",
      "\n",
      "Epoch 00048: loss improved from 0.99985 to 0.99606, saving model to best_model.hdf5\n",
      "Epoch 49/2000\n",
      "100/100 [==============================] - 0s 52us/step - loss: 0.9943 - acc: 0.5600 - val_loss: 0.9486 - val_acc: 0.5758\n",
      "\n",
      "Epoch 00049: loss improved from 0.99606 to 0.99427, saving model to best_model.hdf5\n",
      "Epoch 50/2000\n",
      "100/100 [==============================] - 0s 51us/step - loss: 0.9909 - acc: 0.5700 - val_loss: 0.9451 - val_acc: 0.5758\n",
      "\n",
      "Epoch 00050: loss improved from 0.99427 to 0.99091, saving model to best_model.hdf5\n",
      "Epoch 51/2000\n",
      "100/100 [==============================] - 0s 89us/step - loss: 0.9884 - acc: 0.5700 - val_loss: 0.9411 - val_acc: 0.5758\n",
      "\n",
      "Epoch 00051: loss improved from 0.99091 to 0.98838, saving model to best_model.hdf5\n",
      "Epoch 52/2000\n",
      "100/100 [==============================] - 0s 67us/step - loss: 0.9856 - acc: 0.5700 - val_loss: 0.9376 - val_acc: 0.5758\n",
      "\n",
      "Epoch 00052: loss improved from 0.98838 to 0.98562, saving model to best_model.hdf5\n",
      "Epoch 53/2000\n",
      "100/100 [==============================] - 0s 62us/step - loss: 0.9841 - acc: 0.5700 - val_loss: 0.9344 - val_acc: 0.5758\n",
      "\n",
      "Epoch 00053: loss improved from 0.98562 to 0.98414, saving model to best_model.hdf5\n",
      "Epoch 54/2000\n",
      "100/100 [==============================] - 0s 61us/step - loss: 0.9806 - acc: 0.5700 - val_loss: 0.9304 - val_acc: 0.5758\n",
      "\n",
      "Epoch 00054: loss improved from 0.98414 to 0.98063, saving model to best_model.hdf5\n",
      "Epoch 55/2000\n",
      "100/100 [==============================] - 0s 63us/step - loss: 0.9781 - acc: 0.5800 - val_loss: 0.9261 - val_acc: 0.5758\n",
      "\n",
      "Epoch 00055: loss improved from 0.98063 to 0.97805, saving model to best_model.hdf5\n",
      "Epoch 56/2000\n",
      "100/100 [==============================] - 0s 51us/step - loss: 0.9762 - acc: 0.5700 - val_loss: 0.9232 - val_acc: 0.5758\n",
      "\n",
      "Epoch 00056: loss improved from 0.97805 to 0.97615, saving model to best_model.hdf5\n",
      "Epoch 57/2000\n",
      "100/100 [==============================] - 0s 51us/step - loss: 0.9741 - acc: 0.5800 - val_loss: 0.9191 - val_acc: 0.5758\n",
      "\n",
      "Epoch 00057: loss improved from 0.97615 to 0.97408, saving model to best_model.hdf5\n",
      "Epoch 58/2000\n",
      "100/100 [==============================] - 0s 71us/step - loss: 0.9704 - acc: 0.5800 - val_loss: 0.9155 - val_acc: 0.5758\n",
      "\n",
      "Epoch 00058: loss improved from 0.97408 to 0.97039, saving model to best_model.hdf5\n",
      "Epoch 59/2000\n",
      "100/100 [==============================] - 0s 66us/step - loss: 0.9691 - acc: 0.5700 - val_loss: 0.9124 - val_acc: 0.5758\n",
      "\n",
      "Epoch 00059: loss improved from 0.97039 to 0.96913, saving model to best_model.hdf5\n",
      "Epoch 60/2000\n",
      "100/100 [==============================] - 0s 64us/step - loss: 0.9663 - acc: 0.5800 - val_loss: 0.9091 - val_acc: 0.5758\n",
      "\n",
      "Epoch 00060: loss improved from 0.96913 to 0.96631, saving model to best_model.hdf5\n",
      "Epoch 61/2000\n",
      "100/100 [==============================] - 0s 51us/step - loss: 0.9633 - acc: 0.5800 - val_loss: 0.9050 - val_acc: 0.5758\n",
      "\n",
      "Epoch 00061: loss improved from 0.96631 to 0.96326, saving model to best_model.hdf5\n",
      "Epoch 62/2000\n",
      "100/100 [==============================] - 0s 59us/step - loss: 0.9607 - acc: 0.5800 - val_loss: 0.9012 - val_acc: 0.5758\n",
      "\n",
      "Epoch 00062: loss improved from 0.96326 to 0.96069, saving model to best_model.hdf5\n",
      "Epoch 63/2000\n",
      "100/100 [==============================] - 0s 62us/step - loss: 0.9595 - acc: 0.6000 - val_loss: 0.8978 - val_acc: 0.5758\n",
      "\n",
      "Epoch 00063: loss improved from 0.96069 to 0.95945, saving model to best_model.hdf5\n",
      "Epoch 64/2000\n",
      "100/100 [==============================] - 0s 60us/step - loss: 0.9563 - acc: 0.5800 - val_loss: 0.8946 - val_acc: 0.5758\n",
      "\n",
      "Epoch 00064: loss improved from 0.95945 to 0.95632, saving model to best_model.hdf5\n",
      "Epoch 65/2000\n",
      "100/100 [==============================] - 0s 51us/step - loss: 0.9540 - acc: 0.5900 - val_loss: 0.8908 - val_acc: 0.5758\n",
      "\n",
      "Epoch 00065: loss improved from 0.95632 to 0.95396, saving model to best_model.hdf5\n",
      "Epoch 66/2000\n",
      "100/100 [==============================] - 0s 52us/step - loss: 0.9509 - acc: 0.5800 - val_loss: 0.8874 - val_acc: 0.5758\n",
      "\n",
      "Epoch 00066: loss improved from 0.95396 to 0.95086, saving model to best_model.hdf5\n",
      "Epoch 67/2000\n",
      "100/100 [==============================] - 0s 55us/step - loss: 0.9490 - acc: 0.6000 - val_loss: 0.8836 - val_acc: 0.5758\n",
      "\n",
      "Epoch 00067: loss improved from 0.95086 to 0.94896, saving model to best_model.hdf5\n",
      "Epoch 68/2000\n",
      "100/100 [==============================] - 0s 62us/step - loss: 0.9464 - acc: 0.6000 - val_loss: 0.8799 - val_acc: 0.5758\n",
      "\n",
      "Epoch 00068: loss improved from 0.94896 to 0.94637, saving model to best_model.hdf5\n",
      "Epoch 69/2000\n",
      "100/100 [==============================] - 0s 51us/step - loss: 0.9432 - acc: 0.6000 - val_loss: 0.8763 - val_acc: 0.6061\n",
      "\n",
      "Epoch 00069: loss improved from 0.94637 to 0.94324, saving model to best_model.hdf5\n",
      "Epoch 70/2000\n",
      "100/100 [==============================] - 0s 52us/step - loss: 0.9409 - acc: 0.6000 - val_loss: 0.8723 - val_acc: 0.6061\n",
      "\n",
      "Epoch 00070: loss improved from 0.94324 to 0.94093, saving model to best_model.hdf5\n",
      "Epoch 71/2000\n",
      "100/100 [==============================] - 0s 61us/step - loss: 0.9380 - acc: 0.6000 - val_loss: 0.8686 - val_acc: 0.6061\n",
      "\n",
      "Epoch 00071: loss improved from 0.94093 to 0.93801, saving model to best_model.hdf5\n",
      "Epoch 72/2000\n",
      "100/100 [==============================] - 0s 47us/step - loss: 0.9352 - acc: 0.6100 - val_loss: 0.8647 - val_acc: 0.6061\n",
      "\n",
      "Epoch 00072: loss improved from 0.93801 to 0.93523, saving model to best_model.hdf5\n",
      "Epoch 73/2000\n",
      "100/100 [==============================] - 0s 52us/step - loss: 0.9335 - acc: 0.6100 - val_loss: 0.8613 - val_acc: 0.6364\n",
      "\n",
      "Epoch 00073: loss improved from 0.93523 to 0.93355, saving model to best_model.hdf5\n",
      "Epoch 74/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 0s 51us/step - loss: 0.9300 - acc: 0.6300 - val_loss: 0.8573 - val_acc: 0.6667\n",
      "\n",
      "Epoch 00074: loss improved from 0.93355 to 0.92998, saving model to best_model.hdf5\n",
      "Epoch 75/2000\n",
      "100/100 [==============================] - 0s 62us/step - loss: 0.9271 - acc: 0.6300 - val_loss: 0.8532 - val_acc: 0.6667\n",
      "\n",
      "Epoch 00075: loss improved from 0.92998 to 0.92706, saving model to best_model.hdf5\n",
      "Epoch 76/2000\n",
      "100/100 [==============================] - 0s 72us/step - loss: 0.9256 - acc: 0.6300 - val_loss: 0.8492 - val_acc: 0.6667\n",
      "\n",
      "Epoch 00076: loss improved from 0.92706 to 0.92559, saving model to best_model.hdf5\n",
      "Epoch 77/2000\n",
      "100/100 [==============================] - 0s 57us/step - loss: 0.9258 - acc: 0.6300 - val_loss: 0.8461 - val_acc: 0.6667\n",
      "\n",
      "Epoch 00077: loss did not improve from 0.92559\n",
      "Epoch 78/2000\n",
      "100/100 [==============================] - 0s 76us/step - loss: 0.9197 - acc: 0.6300 - val_loss: 0.8428 - val_acc: 0.6667\n",
      "\n",
      "Epoch 00078: loss improved from 0.92559 to 0.91966, saving model to best_model.hdf5\n",
      "Epoch 79/2000\n",
      "100/100 [==============================] - 0s 54us/step - loss: 0.9173 - acc: 0.6300 - val_loss: 0.8395 - val_acc: 0.6667\n",
      "\n",
      "Epoch 00079: loss improved from 0.91966 to 0.91734, saving model to best_model.hdf5\n",
      "Epoch 80/2000\n",
      "100/100 [==============================] - 0s 86us/step - loss: 0.9154 - acc: 0.6300 - val_loss: 0.8363 - val_acc: 0.6667\n",
      "\n",
      "Epoch 00080: loss improved from 0.91734 to 0.91536, saving model to best_model.hdf5\n",
      "Epoch 81/2000\n",
      "100/100 [==============================] - 0s 69us/step - loss: 0.9126 - acc: 0.6300 - val_loss: 0.8327 - val_acc: 0.6667\n",
      "\n",
      "Epoch 00081: loss improved from 0.91536 to 0.91260, saving model to best_model.hdf5\n",
      "Epoch 82/2000\n",
      "100/100 [==============================] - 0s 64us/step - loss: 0.9105 - acc: 0.6400 - val_loss: 0.8289 - val_acc: 0.6667\n",
      "\n",
      "Epoch 00082: loss improved from 0.91260 to 0.91049, saving model to best_model.hdf5\n",
      "Epoch 83/2000\n",
      "100/100 [==============================] - 0s 61us/step - loss: 0.9101 - acc: 0.6300 - val_loss: 0.8260 - val_acc: 0.6667\n",
      "\n",
      "Epoch 00083: loss improved from 0.91049 to 0.91014, saving model to best_model.hdf5\n",
      "Epoch 84/2000\n",
      "100/100 [==============================] - 0s 55us/step - loss: 0.9056 - acc: 0.6400 - val_loss: 0.8225 - val_acc: 0.6667\n",
      "\n",
      "Epoch 00084: loss improved from 0.91014 to 0.90561, saving model to best_model.hdf5\n",
      "Epoch 85/2000\n",
      "100/100 [==============================] - 0s 80us/step - loss: 0.9044 - acc: 0.6300 - val_loss: 0.8193 - val_acc: 0.6667\n",
      "\n",
      "Epoch 00085: loss improved from 0.90561 to 0.90439, saving model to best_model.hdf5\n",
      "Epoch 86/2000\n",
      "100/100 [==============================] - 0s 60us/step - loss: 0.9038 - acc: 0.6500 - val_loss: 0.8160 - val_acc: 0.6667\n",
      "\n",
      "Epoch 00086: loss improved from 0.90439 to 0.90381, saving model to best_model.hdf5\n",
      "Epoch 87/2000\n",
      "100/100 [==============================] - 0s 58us/step - loss: 0.8993 - acc: 0.6400 - val_loss: 0.8127 - val_acc: 0.6667\n",
      "\n",
      "Epoch 00087: loss improved from 0.90381 to 0.89928, saving model to best_model.hdf5\n",
      "Epoch 88/2000\n",
      "100/100 [==============================] - 0s 54us/step - loss: 0.8969 - acc: 0.6400 - val_loss: 0.8095 - val_acc: 0.6667\n",
      "\n",
      "Epoch 00088: loss improved from 0.89928 to 0.89693, saving model to best_model.hdf5\n",
      "Epoch 89/2000\n",
      "100/100 [==============================] - 0s 69us/step - loss: 0.8951 - acc: 0.6400 - val_loss: 0.8066 - val_acc: 0.6667\n",
      "\n",
      "Epoch 00089: loss improved from 0.89693 to 0.89513, saving model to best_model.hdf5\n",
      "Epoch 90/2000\n",
      "100/100 [==============================] - 0s 92us/step - loss: 0.8924 - acc: 0.6400 - val_loss: 0.8035 - val_acc: 0.6667\n",
      "\n",
      "Epoch 00090: loss improved from 0.89513 to 0.89241, saving model to best_model.hdf5\n",
      "Epoch 91/2000\n",
      "100/100 [==============================] - 0s 59us/step - loss: 0.8902 - acc: 0.6500 - val_loss: 0.7996 - val_acc: 0.6667\n",
      "\n",
      "Epoch 00091: loss improved from 0.89241 to 0.89025, saving model to best_model.hdf5\n",
      "Epoch 92/2000\n",
      "100/100 [==============================] - 0s 56us/step - loss: 0.8876 - acc: 0.6400 - val_loss: 0.7962 - val_acc: 0.6667\n",
      "\n",
      "Epoch 00092: loss improved from 0.89025 to 0.88759, saving model to best_model.hdf5\n",
      "Epoch 93/2000\n",
      "100/100 [==============================] - 0s 62us/step - loss: 0.8856 - acc: 0.6500 - val_loss: 0.7927 - val_acc: 0.6667\n",
      "\n",
      "Epoch 00093: loss improved from 0.88759 to 0.88557, saving model to best_model.hdf5\n",
      "Epoch 94/2000\n",
      "100/100 [==============================] - 0s 57us/step - loss: 0.8841 - acc: 0.6400 - val_loss: 0.7893 - val_acc: 0.6667\n",
      "\n",
      "Epoch 00094: loss improved from 0.88557 to 0.88408, saving model to best_model.hdf5\n",
      "Epoch 95/2000\n",
      "100/100 [==============================] - 0s 47us/step - loss: 0.8814 - acc: 0.6400 - val_loss: 0.7863 - val_acc: 0.6667\n",
      "\n",
      "Epoch 00095: loss improved from 0.88408 to 0.88144, saving model to best_model.hdf5\n",
      "Epoch 96/2000\n",
      "100/100 [==============================] - 0s 63us/step - loss: 0.8787 - acc: 0.6400 - val_loss: 0.7832 - val_acc: 0.6667\n",
      "\n",
      "Epoch 00096: loss improved from 0.88144 to 0.87866, saving model to best_model.hdf5\n",
      "Epoch 97/2000\n",
      "100/100 [==============================] - 0s 70us/step - loss: 0.8764 - acc: 0.6400 - val_loss: 0.7799 - val_acc: 0.6667\n",
      "\n",
      "Epoch 00097: loss improved from 0.87866 to 0.87642, saving model to best_model.hdf5\n",
      "Epoch 98/2000\n",
      "100/100 [==============================] - 0s 70us/step - loss: 0.8756 - acc: 0.6400 - val_loss: 0.7771 - val_acc: 0.6667\n",
      "\n",
      "Epoch 00098: loss improved from 0.87642 to 0.87558, saving model to best_model.hdf5\n",
      "Epoch 99/2000\n",
      "100/100 [==============================] - 0s 93us/step - loss: 0.8718 - acc: 0.6500 - val_loss: 0.7737 - val_acc: 0.6667\n",
      "\n",
      "Epoch 00099: loss improved from 0.87558 to 0.87178, saving model to best_model.hdf5\n",
      "Epoch 100/2000\n",
      "100/100 [==============================] - 0s 73us/step - loss: 0.8697 - acc: 0.6500 - val_loss: 0.7703 - val_acc: 0.6667\n",
      "\n",
      "Epoch 00100: loss improved from 0.87178 to 0.86965, saving model to best_model.hdf5\n",
      "Epoch 101/2000\n",
      "100/100 [==============================] - 0s 72us/step - loss: 0.8717 - acc: 0.6400 - val_loss: 0.7684 - val_acc: 0.6970\n",
      "\n",
      "Epoch 00101: loss did not improve from 0.86965\n",
      "Epoch 102/2000\n",
      "100/100 [==============================] - 0s 93us/step - loss: 0.8671 - acc: 0.6500 - val_loss: 0.7657 - val_acc: 0.6970\n",
      "\n",
      "Epoch 00102: loss improved from 0.86965 to 0.86713, saving model to best_model.hdf5\n",
      "Epoch 103/2000\n",
      "100/100 [==============================] - 0s 61us/step - loss: 0.8636 - acc: 0.6500 - val_loss: 0.7638 - val_acc: 0.6970\n",
      "\n",
      "Epoch 00103: loss improved from 0.86713 to 0.86361, saving model to best_model.hdf5\n",
      "Epoch 104/2000\n",
      "100/100 [==============================] - 0s 53us/step - loss: 0.8624 - acc: 0.6500 - val_loss: 0.7609 - val_acc: 0.6970\n",
      "\n",
      "Epoch 00104: loss improved from 0.86361 to 0.86245, saving model to best_model.hdf5\n",
      "Epoch 105/2000\n",
      "100/100 [==============================] - 0s 68us/step - loss: 0.8596 - acc: 0.6500 - val_loss: 0.7593 - val_acc: 0.6970\n",
      "\n",
      "Epoch 00105: loss improved from 0.86245 to 0.85955, saving model to best_model.hdf5\n",
      "Epoch 106/2000\n",
      "100/100 [==============================] - 0s 53us/step - loss: 0.8576 - acc: 0.6500 - val_loss: 0.7563 - val_acc: 0.6970\n",
      "\n",
      "Epoch 00106: loss improved from 0.85955 to 0.85764, saving model to best_model.hdf5\n",
      "Epoch 107/2000\n",
      "100/100 [==============================] - 0s 66us/step - loss: 0.8552 - acc: 0.6500 - val_loss: 0.7539 - val_acc: 0.6970\n",
      "\n",
      "Epoch 00107: loss improved from 0.85764 to 0.85524, saving model to best_model.hdf5\n",
      "Epoch 108/2000\n",
      "100/100 [==============================] - 0s 49us/step - loss: 0.8540 - acc: 0.6500 - val_loss: 0.7511 - val_acc: 0.6970\n",
      "\n",
      "Epoch 00108: loss improved from 0.85524 to 0.85397, saving model to best_model.hdf5\n",
      "Epoch 109/2000\n",
      "100/100 [==============================] - 0s 72us/step - loss: 0.8513 - acc: 0.6500 - val_loss: 0.7486 - val_acc: 0.6970\n",
      "\n",
      "Epoch 00109: loss improved from 0.85397 to 0.85133, saving model to best_model.hdf5\n",
      "Epoch 110/2000\n",
      "100/100 [==============================] - 0s 52us/step - loss: 0.8495 - acc: 0.6500 - val_loss: 0.7455 - val_acc: 0.6970\n",
      "\n",
      "Epoch 00110: loss improved from 0.85133 to 0.84948, saving model to best_model.hdf5\n",
      "Epoch 111/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 0s 56us/step - loss: 0.8471 - acc: 0.6500 - val_loss: 0.7449 - val_acc: 0.6970\n",
      "\n",
      "Epoch 00111: loss improved from 0.84948 to 0.84710, saving model to best_model.hdf5\n",
      "Epoch 112/2000\n",
      "100/100 [==============================] - 0s 62us/step - loss: 0.8446 - acc: 0.6600 - val_loss: 0.7424 - val_acc: 0.6970\n",
      "\n",
      "Epoch 00112: loss improved from 0.84710 to 0.84463, saving model to best_model.hdf5\n",
      "Epoch 113/2000\n",
      "100/100 [==============================] - 0s 61us/step - loss: 0.8434 - acc: 0.6600 - val_loss: 0.7391 - val_acc: 0.6970\n",
      "\n",
      "Epoch 00113: loss improved from 0.84463 to 0.84343, saving model to best_model.hdf5\n",
      "Epoch 114/2000\n",
      "100/100 [==============================] - 0s 68us/step - loss: 0.8403 - acc: 0.6600 - val_loss: 0.7364 - val_acc: 0.6970\n",
      "\n",
      "Epoch 00114: loss improved from 0.84343 to 0.84032, saving model to best_model.hdf5\n",
      "Epoch 115/2000\n",
      "100/100 [==============================] - 0s 62us/step - loss: 0.8399 - acc: 0.6600 - val_loss: 0.7342 - val_acc: 0.6970\n",
      "\n",
      "Epoch 00115: loss improved from 0.84032 to 0.83988, saving model to best_model.hdf5\n",
      "Epoch 116/2000\n",
      "100/100 [==============================] - 0s 82us/step - loss: 0.8367 - acc: 0.6500 - val_loss: 0.7321 - val_acc: 0.6970\n",
      "\n",
      "Epoch 00116: loss improved from 0.83988 to 0.83669, saving model to best_model.hdf5\n",
      "Epoch 117/2000\n",
      "100/100 [==============================] - 0s 49us/step - loss: 0.8358 - acc: 0.6600 - val_loss: 0.7315 - val_acc: 0.6970\n",
      "\n",
      "Epoch 00117: loss improved from 0.83669 to 0.83583, saving model to best_model.hdf5\n",
      "Epoch 118/2000\n",
      "100/100 [==============================] - 0s 53us/step - loss: 0.8320 - acc: 0.6600 - val_loss: 0.7289 - val_acc: 0.6970\n",
      "\n",
      "Epoch 00118: loss improved from 0.83583 to 0.83196, saving model to best_model.hdf5\n",
      "Epoch 119/2000\n",
      "100/100 [==============================] - 0s 67us/step - loss: 0.8299 - acc: 0.6600 - val_loss: 0.7265 - val_acc: 0.6970\n",
      "\n",
      "Epoch 00119: loss improved from 0.83196 to 0.82988, saving model to best_model.hdf5\n",
      "Epoch 120/2000\n",
      "100/100 [==============================] - 0s 59us/step - loss: 0.8314 - acc: 0.6500 - val_loss: 0.7256 - val_acc: 0.6970\n",
      "\n",
      "Epoch 00120: loss did not improve from 0.82988\n",
      "Epoch 121/2000\n",
      "100/100 [==============================] - 0s 57us/step - loss: 0.8261 - acc: 0.6600 - val_loss: 0.7239 - val_acc: 0.6970\n",
      "\n",
      "Epoch 00121: loss improved from 0.82988 to 0.82610, saving model to best_model.hdf5\n",
      "Epoch 122/2000\n",
      "100/100 [==============================] - 0s 63us/step - loss: 0.8234 - acc: 0.6600 - val_loss: 0.7209 - val_acc: 0.6970\n",
      "\n",
      "Epoch 00122: loss improved from 0.82610 to 0.82338, saving model to best_model.hdf5\n",
      "Epoch 123/2000\n",
      "100/100 [==============================] - 0s 93us/step - loss: 0.8223 - acc: 0.6600 - val_loss: 0.7182 - val_acc: 0.6970\n",
      "\n",
      "Epoch 00123: loss improved from 0.82338 to 0.82227, saving model to best_model.hdf5\n",
      "Epoch 124/2000\n",
      "100/100 [==============================] - 0s 73us/step - loss: 0.8212 - acc: 0.6700 - val_loss: 0.7158 - val_acc: 0.6970\n",
      "\n",
      "Epoch 00124: loss improved from 0.82227 to 0.82117, saving model to best_model.hdf5\n",
      "Epoch 125/2000\n",
      "100/100 [==============================] - 0s 79us/step - loss: 0.8179 - acc: 0.6700 - val_loss: 0.7138 - val_acc: 0.6970\n",
      "\n",
      "Epoch 00125: loss improved from 0.82117 to 0.81790, saving model to best_model.hdf5\n",
      "Epoch 126/2000\n",
      "100/100 [==============================] - 0s 52us/step - loss: 0.8150 - acc: 0.6600 - val_loss: 0.7132 - val_acc: 0.6970\n",
      "\n",
      "Epoch 00126: loss improved from 0.81790 to 0.81499, saving model to best_model.hdf5\n",
      "Epoch 127/2000\n",
      "100/100 [==============================] - 0s 65us/step - loss: 0.8134 - acc: 0.6700 - val_loss: 0.7121 - val_acc: 0.6970\n",
      "\n",
      "Epoch 00127: loss improved from 0.81499 to 0.81336, saving model to best_model.hdf5\n",
      "Epoch 128/2000\n",
      "100/100 [==============================] - 0s 64us/step - loss: 0.8106 - acc: 0.6900 - val_loss: 0.7097 - val_acc: 0.6970\n",
      "\n",
      "Epoch 00128: loss improved from 0.81336 to 0.81062, saving model to best_model.hdf5\n",
      "Epoch 129/2000\n",
      "100/100 [==============================] - 0s 57us/step - loss: 0.8085 - acc: 0.6900 - val_loss: 0.7073 - val_acc: 0.6970\n",
      "\n",
      "Epoch 00129: loss improved from 0.81062 to 0.80854, saving model to best_model.hdf5\n",
      "Epoch 130/2000\n",
      "100/100 [==============================] - 0s 51us/step - loss: 0.8061 - acc: 0.6900 - val_loss: 0.7051 - val_acc: 0.6970\n",
      "\n",
      "Epoch 00130: loss improved from 0.80854 to 0.80610, saving model to best_model.hdf5\n",
      "Epoch 131/2000\n",
      "100/100 [==============================] - 0s 49us/step - loss: 0.8079 - acc: 0.6900 - val_loss: 0.7030 - val_acc: 0.6970\n",
      "\n",
      "Epoch 00131: loss did not improve from 0.80610\n",
      "Epoch 132/2000\n",
      "100/100 [==============================] - 0s 64us/step - loss: 0.8018 - acc: 0.6900 - val_loss: 0.7014 - val_acc: 0.6970\n",
      "\n",
      "Epoch 00132: loss improved from 0.80610 to 0.80182, saving model to best_model.hdf5\n",
      "Epoch 133/2000\n",
      "100/100 [==============================] - 0s 48us/step - loss: 0.8000 - acc: 0.7000 - val_loss: 0.6986 - val_acc: 0.6970\n",
      "\n",
      "Epoch 00133: loss improved from 0.80182 to 0.79999, saving model to best_model.hdf5\n",
      "Epoch 134/2000\n",
      "100/100 [==============================] - 0s 65us/step - loss: 0.7975 - acc: 0.7000 - val_loss: 0.6968 - val_acc: 0.6970\n",
      "\n",
      "Epoch 00134: loss improved from 0.79999 to 0.79755, saving model to best_model.hdf5\n",
      "Epoch 135/2000\n",
      "100/100 [==============================] - 0s 72us/step - loss: 0.7969 - acc: 0.6900 - val_loss: 0.6960 - val_acc: 0.6970\n",
      "\n",
      "Epoch 00135: loss improved from 0.79755 to 0.79692, saving model to best_model.hdf5\n",
      "Epoch 136/2000\n",
      "100/100 [==============================] - 0s 56us/step - loss: 0.7930 - acc: 0.7000 - val_loss: 0.6936 - val_acc: 0.6970\n",
      "\n",
      "Epoch 00136: loss improved from 0.79692 to 0.79303, saving model to best_model.hdf5\n",
      "Epoch 137/2000\n",
      "100/100 [==============================] - 0s 54us/step - loss: 0.7928 - acc: 0.6900 - val_loss: 0.6927 - val_acc: 0.6970\n",
      "\n",
      "Epoch 00137: loss improved from 0.79303 to 0.79277, saving model to best_model.hdf5\n",
      "Epoch 138/2000\n",
      "100/100 [==============================] - 0s 56us/step - loss: 0.7919 - acc: 0.6900 - val_loss: 0.6913 - val_acc: 0.6970\n",
      "\n",
      "Epoch 00138: loss improved from 0.79277 to 0.79189, saving model to best_model.hdf5\n",
      "Epoch 139/2000\n",
      "100/100 [==============================] - 0s 82us/step - loss: 0.7870 - acc: 0.7000 - val_loss: 0.6887 - val_acc: 0.6970\n",
      "\n",
      "Epoch 00139: loss improved from 0.79189 to 0.78705, saving model to best_model.hdf5\n",
      "Epoch 140/2000\n",
      "100/100 [==============================] - 0s 57us/step - loss: 0.7862 - acc: 0.7000 - val_loss: 0.6875 - val_acc: 0.6970\n",
      "\n",
      "Epoch 00140: loss improved from 0.78705 to 0.78620, saving model to best_model.hdf5\n",
      "Epoch 141/2000\n",
      "100/100 [==============================] - 0s 49us/step - loss: 0.7829 - acc: 0.7000 - val_loss: 0.6851 - val_acc: 0.6970\n",
      "\n",
      "Epoch 00141: loss improved from 0.78620 to 0.78291, saving model to best_model.hdf5\n",
      "Epoch 142/2000\n",
      "100/100 [==============================] - 0s 46us/step - loss: 0.7807 - acc: 0.7000 - val_loss: 0.6832 - val_acc: 0.6970\n",
      "\n",
      "Epoch 00142: loss improved from 0.78291 to 0.78066, saving model to best_model.hdf5\n",
      "Epoch 143/2000\n",
      "100/100 [==============================] - 0s 50us/step - loss: 0.7826 - acc: 0.7000 - val_loss: 0.6815 - val_acc: 0.6970\n",
      "\n",
      "Epoch 00143: loss did not improve from 0.78066\n",
      "Epoch 144/2000\n",
      "100/100 [==============================] - 0s 68us/step - loss: 0.7768 - acc: 0.7000 - val_loss: 0.6799 - val_acc: 0.6970\n",
      "\n",
      "Epoch 00144: loss improved from 0.78066 to 0.77680, saving model to best_model.hdf5\n",
      "Epoch 145/2000\n",
      "100/100 [==============================] - 0s 48us/step - loss: 0.7738 - acc: 0.7000 - val_loss: 0.6789 - val_acc: 0.6970\n",
      "\n",
      "Epoch 00145: loss improved from 0.77680 to 0.77380, saving model to best_model.hdf5\n",
      "Epoch 146/2000\n",
      "100/100 [==============================] - 0s 51us/step - loss: 0.7726 - acc: 0.7000 - val_loss: 0.6785 - val_acc: 0.6970\n",
      "\n",
      "Epoch 00146: loss improved from 0.77380 to 0.77258, saving model to best_model.hdf5\n",
      "Epoch 147/2000\n",
      "100/100 [==============================] - 0s 68us/step - loss: 0.7704 - acc: 0.7000 - val_loss: 0.6776 - val_acc: 0.6970\n",
      "\n",
      "Epoch 00147: loss improved from 0.77258 to 0.77037, saving model to best_model.hdf5\n",
      "Epoch 148/2000\n",
      "100/100 [==============================] - 0s 53us/step - loss: 0.7675 - acc: 0.7000 - val_loss: 0.6749 - val_acc: 0.6970\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 00148: loss improved from 0.77037 to 0.76752, saving model to best_model.hdf5\n",
      "Epoch 149/2000\n",
      "100/100 [==============================] - 0s 51us/step - loss: 0.7688 - acc: 0.7000 - val_loss: 0.6716 - val_acc: 0.6970\n",
      "\n",
      "Epoch 00149: loss did not improve from 0.76752\n",
      "Epoch 150/2000\n",
      "100/100 [==============================] - 0s 97us/step - loss: 0.7630 - acc: 0.7000 - val_loss: 0.6702 - val_acc: 0.6970\n",
      "\n",
      "Epoch 00150: loss improved from 0.76752 to 0.76300, saving model to best_model.hdf5\n",
      "Epoch 151/2000\n",
      "100/100 [==============================] - 0s 61us/step - loss: 0.7642 - acc: 0.7000 - val_loss: 0.6679 - val_acc: 0.6970\n",
      "\n",
      "Epoch 00151: loss did not improve from 0.76300\n",
      "Epoch 152/2000\n",
      "100/100 [==============================] - 0s 72us/step - loss: 0.7591 - acc: 0.7000 - val_loss: 0.6676 - val_acc: 0.6970\n",
      "\n",
      "Epoch 00152: loss improved from 0.76300 to 0.75908, saving model to best_model.hdf5\n",
      "Epoch 153/2000\n",
      "100/100 [==============================] - 0s 58us/step - loss: 0.7570 - acc: 0.7000 - val_loss: 0.6663 - val_acc: 0.6970\n",
      "\n",
      "Epoch 00153: loss improved from 0.75908 to 0.75704, saving model to best_model.hdf5\n",
      "Epoch 154/2000\n",
      "100/100 [==============================] - 0s 81us/step - loss: 0.7579 - acc: 0.7000 - val_loss: 0.6663 - val_acc: 0.6970\n",
      "\n",
      "Epoch 00154: loss did not improve from 0.75704\n",
      "Epoch 155/2000\n",
      "100/100 [==============================] - 0s 71us/step - loss: 0.7537 - acc: 0.7000 - val_loss: 0.6632 - val_acc: 0.6970\n",
      "\n",
      "Epoch 00155: loss improved from 0.75704 to 0.75368, saving model to best_model.hdf5\n",
      "Epoch 156/2000\n",
      "100/100 [==============================] - 0s 70us/step - loss: 0.7513 - acc: 0.7000 - val_loss: 0.6623 - val_acc: 0.6970\n",
      "\n",
      "Epoch 00156: loss improved from 0.75368 to 0.75134, saving model to best_model.hdf5\n",
      "Epoch 157/2000\n",
      "100/100 [==============================] - 0s 52us/step - loss: 0.7494 - acc: 0.7000 - val_loss: 0.6592 - val_acc: 0.6970\n",
      "\n",
      "Epoch 00157: loss improved from 0.75134 to 0.74940, saving model to best_model.hdf5\n",
      "Epoch 158/2000\n",
      "100/100 [==============================] - 0s 53us/step - loss: 0.7478 - acc: 0.7000 - val_loss: 0.6585 - val_acc: 0.6970\n",
      "\n",
      "Epoch 00158: loss improved from 0.74940 to 0.74779, saving model to best_model.hdf5\n",
      "Epoch 159/2000\n",
      "100/100 [==============================] - 0s 88us/step - loss: 0.7456 - acc: 0.7000 - val_loss: 0.6559 - val_acc: 0.6970\n",
      "\n",
      "Epoch 00159: loss improved from 0.74779 to 0.74561, saving model to best_model.hdf5\n",
      "Epoch 160/2000\n",
      "100/100 [==============================] - 0s 47us/step - loss: 0.7447 - acc: 0.7000 - val_loss: 0.6544 - val_acc: 0.6970\n",
      "\n",
      "Epoch 00160: loss improved from 0.74561 to 0.74473, saving model to best_model.hdf5\n",
      "Epoch 161/2000\n",
      "100/100 [==============================] - 0s 69us/step - loss: 0.7424 - acc: 0.7000 - val_loss: 0.6549 - val_acc: 0.6970\n",
      "\n",
      "Epoch 00161: loss improved from 0.74473 to 0.74237, saving model to best_model.hdf5\n",
      "Epoch 162/2000\n",
      "100/100 [==============================] - 0s 77us/step - loss: 0.7397 - acc: 0.7000 - val_loss: 0.6522 - val_acc: 0.6970\n",
      "\n",
      "Epoch 00162: loss improved from 0.74237 to 0.73973, saving model to best_model.hdf5\n",
      "Epoch 163/2000\n",
      "100/100 [==============================] - 0s 57us/step - loss: 0.7374 - acc: 0.7000 - val_loss: 0.6500 - val_acc: 0.6970\n",
      "\n",
      "Epoch 00163: loss improved from 0.73973 to 0.73745, saving model to best_model.hdf5\n",
      "Epoch 164/2000\n",
      "100/100 [==============================] - 0s 48us/step - loss: 0.7355 - acc: 0.7000 - val_loss: 0.6492 - val_acc: 0.6970\n",
      "\n",
      "Epoch 00164: loss improved from 0.73745 to 0.73549, saving model to best_model.hdf5\n",
      "Epoch 165/2000\n",
      "100/100 [==============================] - 0s 53us/step - loss: 0.7335 - acc: 0.7000 - val_loss: 0.6474 - val_acc: 0.6970\n",
      "\n",
      "Epoch 00165: loss improved from 0.73549 to 0.73351, saving model to best_model.hdf5\n",
      "Epoch 166/2000\n",
      "100/100 [==============================] - 0s 53us/step - loss: 0.7323 - acc: 0.7000 - val_loss: 0.6458 - val_acc: 0.6970\n",
      "\n",
      "Epoch 00166: loss improved from 0.73351 to 0.73233, saving model to best_model.hdf5\n",
      "Epoch 167/2000\n",
      "100/100 [==============================] - 0s 70us/step - loss: 0.7299 - acc: 0.7000 - val_loss: 0.6455 - val_acc: 0.6970\n",
      "\n",
      "Epoch 00167: loss improved from 0.73233 to 0.72993, saving model to best_model.hdf5\n",
      "Epoch 168/2000\n",
      "100/100 [==============================] - 0s 73us/step - loss: 0.7279 - acc: 0.7000 - val_loss: 0.6432 - val_acc: 0.6970\n",
      "\n",
      "Epoch 00168: loss improved from 0.72993 to 0.72785, saving model to best_model.hdf5\n",
      "Epoch 169/2000\n",
      "100/100 [==============================] - 0s 72us/step - loss: 0.7255 - acc: 0.7000 - val_loss: 0.6424 - val_acc: 0.6970\n",
      "\n",
      "Epoch 00169: loss improved from 0.72785 to 0.72551, saving model to best_model.hdf5\n",
      "Epoch 170/2000\n",
      "100/100 [==============================] - 0s 54us/step - loss: 0.7279 - acc: 0.7000 - val_loss: 0.6404 - val_acc: 0.6970\n",
      "\n",
      "Epoch 00170: loss did not improve from 0.72551\n",
      "Epoch 171/2000\n",
      "100/100 [==============================] - 0s 76us/step - loss: 0.7223 - acc: 0.7000 - val_loss: 0.6391 - val_acc: 0.6970\n",
      "\n",
      "Epoch 00171: loss improved from 0.72551 to 0.72226, saving model to best_model.hdf5\n",
      "Epoch 172/2000\n",
      "100/100 [==============================] - 0s 48us/step - loss: 0.7243 - acc: 0.7000 - val_loss: 0.6380 - val_acc: 0.6970\n",
      "\n",
      "Epoch 00172: loss did not improve from 0.72226\n",
      "Epoch 173/2000\n",
      "100/100 [==============================] - 0s 73us/step - loss: 0.7193 - acc: 0.7000 - val_loss: 0.6373 - val_acc: 0.6970\n",
      "\n",
      "Epoch 00173: loss improved from 0.72226 to 0.71929, saving model to best_model.hdf5\n",
      "Epoch 174/2000\n",
      "100/100 [==============================] - 0s 62us/step - loss: 0.7176 - acc: 0.7000 - val_loss: 0.6361 - val_acc: 0.6970\n",
      "\n",
      "Epoch 00174: loss improved from 0.71929 to 0.71756, saving model to best_model.hdf5\n",
      "Epoch 175/2000\n",
      "100/100 [==============================] - 0s 72us/step - loss: 0.7162 - acc: 0.7000 - val_loss: 0.6359 - val_acc: 0.6970\n",
      "\n",
      "Epoch 00175: loss improved from 0.71756 to 0.71620, saving model to best_model.hdf5\n",
      "Epoch 176/2000\n",
      "100/100 [==============================] - 0s 65us/step - loss: 0.7139 - acc: 0.7000 - val_loss: 0.6352 - val_acc: 0.6970\n",
      "\n",
      "Epoch 00176: loss improved from 0.71620 to 0.71385, saving model to best_model.hdf5\n",
      "Epoch 177/2000\n",
      "100/100 [==============================] - 0s 59us/step - loss: 0.7125 - acc: 0.7000 - val_loss: 0.6338 - val_acc: 0.6970\n",
      "\n",
      "Epoch 00177: loss improved from 0.71385 to 0.71252, saving model to best_model.hdf5\n",
      "Epoch 178/2000\n",
      "100/100 [==============================] - 0s 58us/step - loss: 0.7114 - acc: 0.7000 - val_loss: 0.6333 - val_acc: 0.6970\n",
      "\n",
      "Epoch 00178: loss improved from 0.71252 to 0.71141, saving model to best_model.hdf5\n",
      "Epoch 179/2000\n",
      "100/100 [==============================] - 0s 70us/step - loss: 0.7097 - acc: 0.7000 - val_loss: 0.6320 - val_acc: 0.6970\n",
      "\n",
      "Epoch 00179: loss improved from 0.71141 to 0.70974, saving model to best_model.hdf5\n",
      "Epoch 180/2000\n",
      "100/100 [==============================] - 0s 61us/step - loss: 0.7072 - acc: 0.7000 - val_loss: 0.6311 - val_acc: 0.6970\n",
      "\n",
      "Epoch 00180: loss improved from 0.70974 to 0.70720, saving model to best_model.hdf5\n",
      "Epoch 181/2000\n",
      "100/100 [==============================] - 0s 70us/step - loss: 0.7055 - acc: 0.7100 - val_loss: 0.6288 - val_acc: 0.6970\n",
      "\n",
      "Epoch 00181: loss improved from 0.70720 to 0.70555, saving model to best_model.hdf5\n",
      "Epoch 182/2000\n",
      "100/100 [==============================] - 0s 55us/step - loss: 0.7041 - acc: 0.7000 - val_loss: 0.6275 - val_acc: 0.6970\n",
      "\n",
      "Epoch 00182: loss improved from 0.70555 to 0.70414, saving model to best_model.hdf5\n",
      "Epoch 183/2000\n",
      "100/100 [==============================] - 0s 64us/step - loss: 0.7025 - acc: 0.7000 - val_loss: 0.6271 - val_acc: 0.6970\n",
      "\n",
      "Epoch 00183: loss improved from 0.70414 to 0.70251, saving model to best_model.hdf5\n",
      "Epoch 184/2000\n",
      "100/100 [==============================] - 0s 71us/step - loss: 0.7011 - acc: 0.7100 - val_loss: 0.6256 - val_acc: 0.6970\n",
      "\n",
      "Epoch 00184: loss improved from 0.70251 to 0.70108, saving model to best_model.hdf5\n",
      "Epoch 185/2000\n",
      "100/100 [==============================] - 0s 58us/step - loss: 0.6993 - acc: 0.7000 - val_loss: 0.6243 - val_acc: 0.6970\n",
      "\n",
      "Epoch 00185: loss improved from 0.70108 to 0.69930, saving model to best_model.hdf5\n",
      "Epoch 186/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 0s 92us/step - loss: 0.6971 - acc: 0.7000 - val_loss: 0.6236 - val_acc: 0.6970\n",
      "\n",
      "Epoch 00186: loss improved from 0.69930 to 0.69705, saving model to best_model.hdf5\n",
      "Epoch 187/2000\n",
      "100/100 [==============================] - 0s 54us/step - loss: 0.6956 - acc: 0.7000 - val_loss: 0.6229 - val_acc: 0.6970\n",
      "\n",
      "Epoch 00187: loss improved from 0.69705 to 0.69559, saving model to best_model.hdf5\n",
      "Epoch 188/2000\n",
      "100/100 [==============================] - 0s 72us/step - loss: 0.6948 - acc: 0.7000 - val_loss: 0.6223 - val_acc: 0.6970\n",
      "\n",
      "Epoch 00188: loss improved from 0.69559 to 0.69481, saving model to best_model.hdf5\n",
      "Epoch 189/2000\n",
      "100/100 [==============================] - 0s 50us/step - loss: 0.6930 - acc: 0.7100 - val_loss: 0.6214 - val_acc: 0.6970\n",
      "\n",
      "Epoch 00189: loss improved from 0.69481 to 0.69302, saving model to best_model.hdf5\n",
      "Epoch 190/2000\n",
      "100/100 [==============================] - 0s 56us/step - loss: 0.6906 - acc: 0.7200 - val_loss: 0.6197 - val_acc: 0.6970\n",
      "\n",
      "Epoch 00190: loss improved from 0.69302 to 0.69055, saving model to best_model.hdf5\n",
      "Epoch 191/2000\n",
      "100/100 [==============================] - 0s 50us/step - loss: 0.6884 - acc: 0.7200 - val_loss: 0.6187 - val_acc: 0.6970\n",
      "\n",
      "Epoch 00191: loss improved from 0.69055 to 0.68839, saving model to best_model.hdf5\n",
      "Epoch 192/2000\n",
      "100/100 [==============================] - 0s 57us/step - loss: 0.6869 - acc: 0.7200 - val_loss: 0.6178 - val_acc: 0.6970\n",
      "\n",
      "Epoch 00192: loss improved from 0.68839 to 0.68695, saving model to best_model.hdf5\n",
      "Epoch 193/2000\n",
      "100/100 [==============================] - 0s 80us/step - loss: 0.6860 - acc: 0.7100 - val_loss: 0.6174 - val_acc: 0.6970\n",
      "\n",
      "Epoch 00193: loss improved from 0.68695 to 0.68604, saving model to best_model.hdf5\n",
      "Epoch 194/2000\n",
      "100/100 [==============================] - 0s 58us/step - loss: 0.6885 - acc: 0.7100 - val_loss: 0.6174 - val_acc: 0.6970\n",
      "\n",
      "Epoch 00194: loss did not improve from 0.68604\n",
      "Epoch 195/2000\n",
      "100/100 [==============================] - 0s 46us/step - loss: 0.6839 - acc: 0.7200 - val_loss: 0.6159 - val_acc: 0.6970\n",
      "\n",
      "Epoch 00195: loss improved from 0.68604 to 0.68395, saving model to best_model.hdf5\n",
      "Epoch 196/2000\n",
      "100/100 [==============================] - 0s 56us/step - loss: 0.6806 - acc: 0.7100 - val_loss: 0.6151 - val_acc: 0.6970\n",
      "\n",
      "Epoch 00196: loss improved from 0.68395 to 0.68056, saving model to best_model.hdf5\n",
      "Epoch 197/2000\n",
      "100/100 [==============================] - 0s 53us/step - loss: 0.6794 - acc: 0.7200 - val_loss: 0.6144 - val_acc: 0.6970\n",
      "\n",
      "Epoch 00197: loss improved from 0.68056 to 0.67941, saving model to best_model.hdf5\n",
      "Epoch 198/2000\n",
      "100/100 [==============================] - 0s 66us/step - loss: 0.6794 - acc: 0.7200 - val_loss: 0.6135 - val_acc: 0.6970\n",
      "\n",
      "Epoch 00198: loss improved from 0.67941 to 0.67936, saving model to best_model.hdf5\n",
      "Epoch 199/2000\n",
      "100/100 [==============================] - 0s 63us/step - loss: 0.6762 - acc: 0.7200 - val_loss: 0.6127 - val_acc: 0.6970\n",
      "\n",
      "Epoch 00199: loss improved from 0.67936 to 0.67623, saving model to best_model.hdf5\n",
      "Epoch 200/2000\n",
      "100/100 [==============================] - 0s 71us/step - loss: 0.6783 - acc: 0.7000 - val_loss: 0.6126 - val_acc: 0.6970\n",
      "\n",
      "Epoch 00200: loss did not improve from 0.67623\n",
      "Epoch 201/2000\n",
      "100/100 [==============================] - 0s 78us/step - loss: 0.6738 - acc: 0.7200 - val_loss: 0.6119 - val_acc: 0.6970\n",
      "\n",
      "Epoch 00201: loss improved from 0.67623 to 0.67379, saving model to best_model.hdf5\n",
      "Epoch 202/2000\n",
      "100/100 [==============================] - 0s 97us/step - loss: 0.6717 - acc: 0.7200 - val_loss: 0.6109 - val_acc: 0.6970\n",
      "\n",
      "Epoch 00202: loss improved from 0.67379 to 0.67168, saving model to best_model.hdf5\n",
      "Epoch 203/2000\n",
      "100/100 [==============================] - 0s 58us/step - loss: 0.6700 - acc: 0.7200 - val_loss: 0.6101 - val_acc: 0.6970\n",
      "\n",
      "Epoch 00203: loss improved from 0.67168 to 0.67003, saving model to best_model.hdf5\n",
      "Epoch 204/2000\n",
      "100/100 [==============================] - 0s 69us/step - loss: 0.6683 - acc: 0.7200 - val_loss: 0.6094 - val_acc: 0.6970\n",
      "\n",
      "Epoch 00204: loss improved from 0.67003 to 0.66835, saving model to best_model.hdf5\n",
      "Epoch 205/2000\n",
      "100/100 [==============================] - 0s 52us/step - loss: 0.6671 - acc: 0.7200 - val_loss: 0.6083 - val_acc: 0.6970\n",
      "\n",
      "Epoch 00205: loss improved from 0.66835 to 0.66709, saving model to best_model.hdf5\n",
      "Epoch 206/2000\n",
      "100/100 [==============================] - 0s 48us/step - loss: 0.6677 - acc: 0.7200 - val_loss: 0.6085 - val_acc: 0.6970\n",
      "\n",
      "Epoch 00206: loss did not improve from 0.66709\n",
      "Epoch 207/2000\n",
      "100/100 [==============================] - 0s 94us/step - loss: 0.6669 - acc: 0.7200 - val_loss: 0.6080 - val_acc: 0.6970\n",
      "\n",
      "Epoch 00207: loss improved from 0.66709 to 0.66693, saving model to best_model.hdf5\n",
      "Epoch 208/2000\n",
      "100/100 [==============================] - 0s 56us/step - loss: 0.6627 - acc: 0.7300 - val_loss: 0.6068 - val_acc: 0.6970\n",
      "\n",
      "Epoch 00208: loss improved from 0.66693 to 0.66270, saving model to best_model.hdf5\n",
      "Epoch 209/2000\n",
      "100/100 [==============================] - 0s 52us/step - loss: 0.6616 - acc: 0.7200 - val_loss: 0.6062 - val_acc: 0.6970\n",
      "\n",
      "Epoch 00209: loss improved from 0.66270 to 0.66156, saving model to best_model.hdf5\n",
      "Epoch 210/2000\n",
      "100/100 [==============================] - 0s 73us/step - loss: 0.6593 - acc: 0.7300 - val_loss: 0.6058 - val_acc: 0.6970\n",
      "\n",
      "Epoch 00210: loss improved from 0.66156 to 0.65928, saving model to best_model.hdf5\n",
      "Epoch 211/2000\n",
      "100/100 [==============================] - 0s 75us/step - loss: 0.6599 - acc: 0.7200 - val_loss: 0.6050 - val_acc: 0.6970\n",
      "\n",
      "Epoch 00211: loss did not improve from 0.65928\n",
      "Epoch 212/2000\n",
      "100/100 [==============================] - 0s 78us/step - loss: 0.6562 - acc: 0.7200 - val_loss: 0.6046 - val_acc: 0.6970\n",
      "\n",
      "Epoch 00212: loss improved from 0.65928 to 0.65618, saving model to best_model.hdf5\n",
      "Epoch 213/2000\n",
      "100/100 [==============================] - 0s 66us/step - loss: 0.6550 - acc: 0.7200 - val_loss: 0.6039 - val_acc: 0.6970\n",
      "\n",
      "Epoch 00213: loss improved from 0.65618 to 0.65502, saving model to best_model.hdf5\n",
      "Epoch 214/2000\n",
      "100/100 [==============================] - 0s 61us/step - loss: 0.6546 - acc: 0.7200 - val_loss: 0.6033 - val_acc: 0.6970\n",
      "\n",
      "Epoch 00214: loss improved from 0.65502 to 0.65464, saving model to best_model.hdf5\n",
      "Epoch 215/2000\n",
      "100/100 [==============================] - 0s 52us/step - loss: 0.6523 - acc: 0.7300 - val_loss: 0.6028 - val_acc: 0.6970\n",
      "\n",
      "Epoch 00215: loss improved from 0.65464 to 0.65227, saving model to best_model.hdf5\n",
      "Epoch 216/2000\n",
      "100/100 [==============================] - 0s 66us/step - loss: 0.6504 - acc: 0.7300 - val_loss: 0.6028 - val_acc: 0.6970\n",
      "\n",
      "Epoch 00216: loss improved from 0.65227 to 0.65037, saving model to best_model.hdf5\n",
      "Epoch 217/2000\n",
      "100/100 [==============================] - 0s 57us/step - loss: 0.6487 - acc: 0.7200 - val_loss: 0.6020 - val_acc: 0.6970\n",
      "\n",
      "Epoch 00217: loss improved from 0.65037 to 0.64868, saving model to best_model.hdf5\n",
      "Epoch 218/2000\n",
      "100/100 [==============================] - 0s 73us/step - loss: 0.6495 - acc: 0.7200 - val_loss: 0.6016 - val_acc: 0.6970\n",
      "\n",
      "Epoch 00218: loss did not improve from 0.64868\n",
      "Epoch 219/2000\n",
      "100/100 [==============================] - 0s 60us/step - loss: 0.6465 - acc: 0.7300 - val_loss: 0.6015 - val_acc: 0.7273\n",
      "\n",
      "Epoch 00219: loss improved from 0.64868 to 0.64654, saving model to best_model.hdf5\n",
      "Epoch 220/2000\n",
      "100/100 [==============================] - 0s 66us/step - loss: 0.6454 - acc: 0.7300 - val_loss: 0.6010 - val_acc: 0.6970\n",
      "\n",
      "Epoch 00220: loss improved from 0.64654 to 0.64539, saving model to best_model.hdf5\n",
      "Epoch 221/2000\n",
      "100/100 [==============================] - 0s 60us/step - loss: 0.6445 - acc: 0.7400 - val_loss: 0.6004 - val_acc: 0.6970\n",
      "\n",
      "Epoch 00221: loss improved from 0.64539 to 0.64449, saving model to best_model.hdf5\n",
      "Epoch 222/2000\n",
      "100/100 [==============================] - 0s 57us/step - loss: 0.6413 - acc: 0.7400 - val_loss: 0.6002 - val_acc: 0.6970\n",
      "\n",
      "Epoch 00222: loss improved from 0.64449 to 0.64134, saving model to best_model.hdf5\n",
      "Epoch 223/2000\n",
      "100/100 [==============================] - 0s 65us/step - loss: 0.6401 - acc: 0.7400 - val_loss: 0.5999 - val_acc: 0.7273\n",
      "\n",
      "Epoch 00223: loss improved from 0.64134 to 0.64011, saving model to best_model.hdf5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 224/2000\n",
      "100/100 [==============================] - 0s 53us/step - loss: 0.6385 - acc: 0.7400 - val_loss: 0.5993 - val_acc: 0.6970\n",
      "\n",
      "Epoch 00224: loss improved from 0.64011 to 0.63849, saving model to best_model.hdf5\n",
      "Epoch 225/2000\n",
      "100/100 [==============================] - 0s 46us/step - loss: 0.6374 - acc: 0.7300 - val_loss: 0.5983 - val_acc: 0.6970\n",
      "\n",
      "Epoch 00225: loss improved from 0.63849 to 0.63742, saving model to best_model.hdf5\n",
      "Epoch 226/2000\n",
      "100/100 [==============================] - 0s 71us/step - loss: 0.6374 - acc: 0.7400 - val_loss: 0.5975 - val_acc: 0.7576\n",
      "\n",
      "Epoch 00226: loss did not improve from 0.63742\n",
      "Epoch 227/2000\n",
      "100/100 [==============================] - 0s 49us/step - loss: 0.6342 - acc: 0.7400 - val_loss: 0.5969 - val_acc: 0.6970\n",
      "\n",
      "Epoch 00227: loss improved from 0.63742 to 0.63419, saving model to best_model.hdf5\n",
      "Epoch 228/2000\n",
      "100/100 [==============================] - 0s 49us/step - loss: 0.6322 - acc: 0.7400 - val_loss: 0.5971 - val_acc: 0.6970\n",
      "\n",
      "Epoch 00228: loss improved from 0.63419 to 0.63224, saving model to best_model.hdf5\n",
      "Epoch 229/2000\n",
      "100/100 [==============================] - 0s 76us/step - loss: 0.6311 - acc: 0.7400 - val_loss: 0.5971 - val_acc: 0.7576\n",
      "\n",
      "Epoch 00229: loss improved from 0.63224 to 0.63110, saving model to best_model.hdf5\n",
      "Epoch 230/2000\n",
      "100/100 [==============================] - 0s 80us/step - loss: 0.6292 - acc: 0.7400 - val_loss: 0.5965 - val_acc: 0.6970\n",
      "\n",
      "Epoch 00230: loss improved from 0.63110 to 0.62924, saving model to best_model.hdf5\n",
      "Epoch 231/2000\n",
      "100/100 [==============================] - 0s 79us/step - loss: 0.6303 - acc: 0.7500 - val_loss: 0.5958 - val_acc: 0.7576\n",
      "\n",
      "Epoch 00231: loss did not improve from 0.62924\n",
      "Epoch 232/2000\n",
      "100/100 [==============================] - 0s 65us/step - loss: 0.6267 - acc: 0.7400 - val_loss: 0.5952 - val_acc: 0.7576\n",
      "\n",
      "Epoch 00232: loss improved from 0.62924 to 0.62665, saving model to best_model.hdf5\n",
      "Epoch 233/2000\n",
      "100/100 [==============================] - 0s 57us/step - loss: 0.6247 - acc: 0.7400 - val_loss: 0.5950 - val_acc: 0.6970\n",
      "\n",
      "Epoch 00233: loss improved from 0.62665 to 0.62474, saving model to best_model.hdf5\n",
      "Epoch 234/2000\n",
      "100/100 [==============================] - 0s 72us/step - loss: 0.6232 - acc: 0.7500 - val_loss: 0.5952 - val_acc: 0.6970\n",
      "\n",
      "Epoch 00234: loss improved from 0.62474 to 0.62319, saving model to best_model.hdf5\n",
      "Epoch 235/2000\n",
      "100/100 [==============================] - 0s 59us/step - loss: 0.6258 - acc: 0.7400 - val_loss: 0.5947 - val_acc: 0.7576\n",
      "\n",
      "Epoch 00235: loss did not improve from 0.62319\n",
      "Epoch 236/2000\n",
      "100/100 [==============================] - 0s 60us/step - loss: 0.6203 - acc: 0.7400 - val_loss: 0.5950 - val_acc: 0.7273\n",
      "\n",
      "Epoch 00236: loss improved from 0.62319 to 0.62028, saving model to best_model.hdf5\n",
      "Epoch 237/2000\n",
      "100/100 [==============================] - 0s 60us/step - loss: 0.6198 - acc: 0.7500 - val_loss: 0.5945 - val_acc: 0.7576\n",
      "\n",
      "Epoch 00237: loss improved from 0.62028 to 0.61983, saving model to best_model.hdf5\n",
      "Epoch 238/2000\n",
      "100/100 [==============================] - 0s 92us/step - loss: 0.6213 - acc: 0.7500 - val_loss: 0.5937 - val_acc: 0.7879\n",
      "\n",
      "Epoch 00238: loss did not improve from 0.61983\n",
      "Epoch 239/2000\n",
      "100/100 [==============================] - 0s 59us/step - loss: 0.6168 - acc: 0.7400 - val_loss: 0.5939 - val_acc: 0.7576\n",
      "\n",
      "Epoch 00239: loss improved from 0.61983 to 0.61684, saving model to best_model.hdf5\n",
      "Epoch 240/2000\n",
      "100/100 [==============================] - 0s 58us/step - loss: 0.6147 - acc: 0.7600 - val_loss: 0.5930 - val_acc: 0.7576\n",
      "\n",
      "Epoch 00240: loss improved from 0.61684 to 0.61465, saving model to best_model.hdf5\n",
      "Epoch 241/2000\n",
      "100/100 [==============================] - 0s 56us/step - loss: 0.6133 - acc: 0.7600 - val_loss: 0.5933 - val_acc: 0.7576\n",
      "\n",
      "Epoch 00241: loss improved from 0.61465 to 0.61326, saving model to best_model.hdf5\n",
      "Epoch 242/2000\n",
      "100/100 [==============================] - 0s 73us/step - loss: 0.6140 - acc: 0.7500 - val_loss: 0.5924 - val_acc: 0.6970\n",
      "\n",
      "Epoch 00242: loss did not improve from 0.61326\n",
      "Epoch 243/2000\n",
      "100/100 [==============================] - 0s 66us/step - loss: 0.6106 - acc: 0.7600 - val_loss: 0.5922 - val_acc: 0.7273\n",
      "\n",
      "Epoch 00243: loss improved from 0.61326 to 0.61057, saving model to best_model.hdf5\n",
      "Epoch 244/2000\n",
      "100/100 [==============================] - 0s 53us/step - loss: 0.6118 - acc: 0.7400 - val_loss: 0.5915 - val_acc: 0.7879\n",
      "\n",
      "Epoch 00244: loss did not improve from 0.61057\n",
      "Epoch 245/2000\n",
      "100/100 [==============================] - 0s 78us/step - loss: 0.6075 - acc: 0.7600 - val_loss: 0.5923 - val_acc: 0.7576\n",
      "\n",
      "Epoch 00245: loss improved from 0.61057 to 0.60755, saving model to best_model.hdf5\n",
      "Epoch 246/2000\n",
      "100/100 [==============================] - 0s 62us/step - loss: 0.6062 - acc: 0.7600 - val_loss: 0.5917 - val_acc: 0.7879\n",
      "\n",
      "Epoch 00246: loss improved from 0.60755 to 0.60622, saving model to best_model.hdf5\n",
      "Epoch 247/2000\n",
      "100/100 [==============================] - 0s 63us/step - loss: 0.6051 - acc: 0.7600 - val_loss: 0.5921 - val_acc: 0.7879\n",
      "\n",
      "Epoch 00247: loss improved from 0.60622 to 0.60507, saving model to best_model.hdf5\n",
      "Epoch 248/2000\n",
      "100/100 [==============================] - 0s 67us/step - loss: 0.6042 - acc: 0.7600 - val_loss: 0.5916 - val_acc: 0.7879\n",
      "\n",
      "Epoch 00248: loss improved from 0.60507 to 0.60424, saving model to best_model.hdf5\n",
      "Epoch 249/2000\n",
      "100/100 [==============================] - 0s 87us/step - loss: 0.6028 - acc: 0.7600 - val_loss: 0.5908 - val_acc: 0.7879\n",
      "\n",
      "Epoch 00249: loss improved from 0.60424 to 0.60277, saving model to best_model.hdf5\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00249: early stopping\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(xtr, ytr, batch_size=50, epochs=2000, verbose =1, validation_data=(xval,yval), callbacks=[checkpoint,monitor])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_acc = history.history[\"acc\"]\n",
    "valid_acc = history.history[\"val_acc\"]\n",
    "epochs = range(len(history.history[\"loss\"]))\n",
    "\n",
    "plt.plot(epochs, train_acc, \"b\", label=\"Training Accuracy\")\n",
    "plt.plot(epochs, valid_acc, \"g\", label=\"Validation Accuracy\")\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.title(\"Model Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(epochs, history.history[\"loss\"], \"b\", label=\"Training Loss\")\n",
    "plt.plot(epochs, history.history[\"val_loss\"], \"g\", label=\"Validation Loss\")\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.title(\"Model Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discusion: Without early stopping, overfitting occurs on the model. In other words, the neural network is trained to the point that it begins to memorize rather than generalize "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Most Optimal Weights and Biases within each layer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dense_1\n",
      "weights [[-0.22897628  0.513282    0.05713973 -0.43601793 -0.4804559  -0.14245792\n",
      "   0.01129556  0.5746346   0.64044976  0.44766864]\n",
      " [-0.2190117   0.30412945  0.53668875  0.21852142  0.43790215 -0.4651344\n",
      "   0.08242571 -0.17832989 -0.13850231  0.36887208]\n",
      " [-0.44933432  0.4802062   0.558697    0.12508911 -0.35598284  0.20744509\n",
      "  -0.05983207 -0.07090319 -0.05361281  0.5526459 ]\n",
      " [-0.47867167 -0.50564337 -0.4973859   0.14333284 -0.495076    0.430201\n",
      "  -0.2551809  -0.3483635  -0.13039115 -0.30792758]\n",
      " [-0.27695203 -0.21720053  0.18218935 -0.2463823   0.12206793  0.13296449\n",
      "  -0.4799095   0.43215716 -0.10068083  0.3283863 ]\n",
      " [-0.19385824  0.21901219  0.23924176  0.06914127  0.360779    0.33291563\n",
      "  -0.12794676  0.19302893  0.10069299  0.1787871 ]\n",
      " [ 0.47951907 -0.16689965 -0.16920784 -0.35490477 -0.48822272  0.3245185\n",
      "  -0.19565973  0.51778775 -0.15216652 -0.20747411]\n",
      " [-0.1990146   0.23575501 -0.22250915 -0.29543746 -0.34832415 -0.18723805\n",
      "  -0.11503771 -0.47052962  0.18557186 -0.2180974 ]\n",
      " [-0.36863253  0.09845542 -0.25587055  0.00953156 -0.15264592  0.13857473\n",
      "   0.00661969 -0.3099449   0.28122783 -0.47858894]\n",
      " [ 0.12016433  0.20373118  0.49651238 -0.03966874 -0.16733578 -0.25147384\n",
      "  -0.24052185  0.04343314  0.32089874 -0.0203773 ]\n",
      " [-0.38262013 -0.17889325  0.15974139  0.02906072 -0.49901047  0.26086453\n",
      "  -0.35340357  0.2644919  -0.07948858 -0.4754343 ]\n",
      " [-0.05104271 -0.06164923  0.032589   -0.30571473 -0.09590846 -0.15821695\n",
      "  -0.4140001  -0.20995535 -0.10435066  0.6184652 ]\n",
      " [-0.27694654 -0.00975826  0.14177324  0.3318727   0.08840102  0.01585884\n",
      "   0.38692003  0.54016     0.66129637 -0.0949768 ]] \n",
      "\n",
      "bias [ 0.         -0.0803912  -0.1483349   0.          0.          0.1735286\n",
      "  0.         -0.18116406 -0.11845675 -0.08334552] \n",
      "\n",
      "=====================================================\n",
      "dense_2\n",
      "weights [[ 0.3547669   0.56363213  0.13718164 -0.2885452   0.45054913  0.376675\n",
      "   0.47445464 -0.51404274]\n",
      " [-0.09301371  0.58004344  0.11777178  0.3767962   0.01950455  0.2961487\n",
      "  -0.24276565 -0.02445286]\n",
      " [-0.04036864  0.03154669  0.16853382  0.3310438   0.42221808  0.6868163\n",
      "   0.48242226  0.78410167]\n",
      " [ 0.23057705  0.3694182   0.3881002   0.12986904  0.1490612   0.08592963\n",
      "  -0.5577443  -0.5367359 ]\n",
      " [-0.4451114   0.33528376 -0.2712232  -0.02086473  0.3467319   0.52004457\n",
      "  -0.19228363  0.2616859 ]\n",
      " [ 0.23852313 -0.18386504  0.01988636  0.5677073  -0.18512863 -0.5435536\n",
      "   0.27155697 -0.5945015 ]\n",
      " [-0.28398675 -0.5127481  -0.01837653  0.37089163 -0.18662727  0.16521442\n",
      "   0.05976146 -0.44258964]\n",
      " [-0.35429    -0.87972456  0.0801359   0.45133606  0.34016642  0.7216592\n",
      "   0.21149582 -0.61634547]\n",
      " [-0.17927583  0.3889108   0.55241066 -0.20834072  0.69018084  0.4982694\n",
      "   0.50154555  0.7115011 ]\n",
      " [-0.04272427  0.13799776  0.5491499  -0.17707159  0.09284794  0.5919854\n",
      "   0.70842093 -0.19696504]] \n",
      "\n",
      "bias [-0.04767338  0.10921393 -0.1522656  -0.08413567 -0.11211178 -0.15484379\n",
      " -0.1588166   0.08422972] \n",
      "\n",
      "=====================================================\n",
      "dense_3\n",
      "weights [[-0.00912349  0.5879381  -0.10527602  0.23166406 -0.40658262 -0.30947235]\n",
      " [ 0.4346555   0.42537782 -0.44866547 -0.61568314 -0.80354583  0.05556646]\n",
      " [ 0.04705703  0.28302488  0.82640254  0.3556284  -0.03561703  0.27243492]\n",
      " [-0.46738473  0.529657   -0.05750578  0.25435767  0.32610273 -0.7710282 ]\n",
      " [-0.34718758  0.05775081  0.77624923  0.12323625 -0.46732342  0.06041732]\n",
      " [-0.5101853   0.9480238   0.6411072   0.9005858  -0.65595174  0.14185648]\n",
      " [-0.19702335  0.2403896   0.41769463  0.53592044  0.5412021   0.30457747]\n",
      " [-0.06332886  0.40024975 -0.5051035  -0.42680213 -0.50756216  0.7890286 ]] \n",
      "\n",
      "bias [ 0.0031621  -0.17003576 -0.11697526 -0.2025853   0.23663238 -0.13430332] \n",
      "\n",
      "=====================================================\n",
      "dense_4\n",
      "weights [[ 0.06541848  0.44890016  0.20982486  0.2997951   0.63747686 -0.39187506]\n",
      " [-0.15907115  0.3945572  -0.64570725 -0.34864125 -0.01076385  0.40577596]\n",
      " [ 0.1587406   0.39016098  0.5982031   0.55263186 -0.6849638   0.99555033]\n",
      " [-0.20812029 -0.2625273  -0.40108228 -0.59714067 -0.28290665  0.74380326]\n",
      " [-0.6619745  -0.14735182 -0.05847398  0.8378751   0.36911806 -0.41856378]\n",
      " [ 0.23392029 -0.5244103   0.44435912 -0.15749952  0.4976239  -0.8355031 ]] \n",
      "\n",
      "bias [ 0.04294352  0.13519195 -0.00172485  0.19005455 -0.0120054  -0.16566327] \n",
      "\n",
      "=====================================================\n",
      "dense_5\n",
      "weights [[ 0.07377821  0.50956047  0.67815745  0.66315883]\n",
      " [-0.08252865 -0.6210434   0.06280544 -0.8255849 ]\n",
      " [ 0.19851065 -0.6438997  -0.7343165   0.5293553 ]\n",
      " [-0.38291317 -0.09900898 -0.3438057   0.22955322]\n",
      " [-0.5584291   0.31564867 -0.20085813  0.18494779]\n",
      " [-0.752844   -0.03869828  1.1436431   0.16753617]] \n",
      "\n",
      "bias [ 0.         -0.00316216 -0.15852608 -0.11638008] \n",
      "\n",
      "=====================================================\n",
      "dense_6\n",
      "weights [[-0.42188025  0.7068535  -0.41628933]\n",
      " [-0.45520404  0.09675019  0.42996898]\n",
      " [ 0.8914791  -1.2865999   0.21787691]\n",
      " [-0.32093346 -0.02705397  0.6057599 ]] \n",
      "\n",
      "bias [-0.4562816   0.48499125 -0.08551809] \n",
      "\n",
      "=====================================================\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "model = load_model('best_model.hdf5')\n",
    "for num, layer in enumerate(model.layers):\n",
    "    if len(layer.weights) > 0:\n",
    "        print(layer.name)\n",
    "        print(\"weights\", model.layers[num].get_weights()[0], \"\\n\")\n",
    "        print(\"bias\", model.layers[num].get_weights()[1], \"\\n\")\n",
    "        print(\"=====================================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Testing Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_7 (Dense)              (None, 10)                140       \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 8)                 88        \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 6)                 54        \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 6)                 42        \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 4)                 28        \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 3)                 15        \n",
      "=================================================================\n",
      "Total params: 367\n",
      "Trainable params: 367\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "evalModel = Sequential()\n",
    "evalModel.add(Dense(10, input_dim=13,activation='relu'))\n",
    "evalModel.add(Dense(8, activation='relu'))\n",
    "evalModel.add(Dense(6, activation='relu'))\n",
    "evalModel.add(Dense(6, activation='relu'))\n",
    "evalModel.add(Dense(4, activation='relu'))\n",
    "evalModel.add(Dense(3, activation='softmax'))\n",
    "\n",
    "evalModel.summary()\n",
    "\n",
    "evalModel.load_weights('best_model.hdf5')\n",
    "evalModel.compile(loss=\"categorical_crossentropy\", optimizer = optimizers.RMSprop(lr = 1e-3), metrics = [\"acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45/45 [==============================] - 0s 783us/step\n",
      "Loss= 0.7285899413956536\n",
      "Accuracy= 0.6888889074325562\n"
     ]
    }
   ],
   "source": [
    "lossAndAcc = evalModel.evaluate(xtest, ytest)\n",
    "print(\"Loss=\", lossAndAcc[0])\n",
    "print(\"Accuracy=\", lossAndAcc[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Use 20%, 30%, 40%, 50%, 60%, 70%,80%,90%,100% of the total training dataset to train the neural work and test on it respectively"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 We need to partition the training set first to 20% to our validation then 30%, then 40%, and so on "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xtrain (133, 13)\n",
      "xtest (45, 13)\n",
      "ytrain (133, 3)\n",
      "ytest (45, 3)\n"
     ]
    }
   ],
   "source": [
    "xtrain, xtest, ytrain, ytest = train_test_split(features, labels)\n",
    "ytrain = to_categorical(ytrain, 3)\n",
    "ytest = to_categorical(ytest,3)\n",
    "\n",
    "print(\"xtrain\", xtrain.shape)\n",
    "print(\"xtest\", xtest.shape)\n",
    "print(\"ytrain\", ytrain.shape)\n",
    "print(\"ytest\", ytest.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Build the training model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(10, input_dim=13,activation='relu'))\n",
    "model.add(Dense(8, activation='relu'))\n",
    "model.add(Dense(6, activation='relu'))\n",
    "model.add(Dense(6, activation='relu'))\n",
    "model.add(Dense(4, activation='relu'))\n",
    "model.add(Dense(3, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\", optimizer = optimizers.RMSprop(lr = 1e-3), metrics = [\"acc\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(\"best_model.hdf5\", monitor=\"loss\", verbose=1, save_best_only = True, mode=\"auto\", period=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = []\n",
    "train_sizes = [0.20, 0.30, 0.40, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\n",
    "\n",
    "def buildTrainingModel(): \n",
    "    model = Sequential()\n",
    "    model.add(Dense(10, input_dim=13,activation='relu'))\n",
    "    model.add(Dense(8, activation='relu'))\n",
    "    model.add(Dense(6, activation='relu'))\n",
    "    model.add(Dense(6, activation='relu'))\n",
    "    model.add(Dense(4, activation='relu'))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "    \n",
    "    model.compile(loss=\"categorical_crossentropy\", optimizer = optimizers.RMSprop(lr = 1e-3), metrics = [\"acc\"])\n",
    "    \n",
    "    checkpoint = ModelCheckpoint(\"model.hdf5\", monitor=\"loss\", verbose=1, save_best_only = True, mode=\"auto\", period=1)\n",
    "    \n",
    "    for i in range(len(train_sizes)): \n",
    "        print(train_sizes[i])\n",
    "        monitor = EarlyStopping(monitor='val_loss', min_delta=1e-3, patience=5, verbose=1, mode='auto',restore_best_weights=True)\n",
    "        history = model.fit(xtrain, ytrain, batch_size=50, epochs=2000, verbose =1, validation_split=train_sizes[i], callbacks=[checkpoint,monitor])\n",
    "\n",
    "        buildTestingModel(model, checkpoint)\n",
    "    return acc \n",
    "\n",
    "def buildTestingModel(model, checkpoint): \n",
    "\n",
    "        evalModel = Sequential()\n",
    "        evalModel.add(Dense(10, input_dim=13,activation='relu'))\n",
    "        evalModel.add(Dense(8, activation='relu'))\n",
    "        evalModel.add(Dense(6, activation='relu'))\n",
    "        evalModel.add(Dense(6, activation='relu'))\n",
    "        evalModel.add(Dense(4, activation='relu'))\n",
    "        evalModel.add(Dense(3, activation='softmax'))\n",
    "\n",
    "#         evalModel.summary()\n",
    "\n",
    "        evalModel.load_weights('model.hdf5')\n",
    "        evalModel.compile(loss=\"categorical_crossentropy\", optimizer = optimizers.RMSprop(lr = 1e-3), metrics = [\"acc\"])\n",
    "        lossAndAcc = evalModel.evaluate(xtest, ytest)\n",
    "        print(\"Loss=\", lossAndAcc[0])\n",
    "        print(\"Accuracy=\", lossAndAcc[1])\n",
    "        acc.append(lossAndAcc[1])\n",
    "        return acc \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2\n",
      "Train on 106 samples, validate on 27 samples\n",
      "Epoch 1/2000\n",
      "106/106 [==============================] - 0s 3ms/step - loss: 9.2755 - acc: 0.4245 - val_loss: 10.1484 - val_acc: 0.3704\n",
      "\n",
      "Epoch 00001: loss improved from inf to 9.27551, saving model to model.hdf5\n",
      "Epoch 2/2000\n",
      "106/106 [==============================] - 0s 102us/step - loss: 9.2755 - acc: 0.4245 - val_loss: 10.1484 - val_acc: 0.3704\n",
      "\n",
      "Epoch 00002: loss improved from 9.27551 to 9.27551, saving model to model.hdf5\n",
      "Epoch 3/2000\n",
      "106/106 [==============================] - 0s 75us/step - loss: 9.2755 - acc: 0.4245 - val_loss: 10.1484 - val_acc: 0.3704\n",
      "\n",
      "Epoch 00003: loss did not improve from 9.27551\n",
      "Epoch 4/2000\n",
      "106/106 [==============================] - 0s 165us/step - loss: 9.2755 - acc: 0.4245 - val_loss: 10.1484 - val_acc: 0.3704\n",
      "\n",
      "Epoch 00004: loss did not improve from 9.27551\n",
      "Epoch 5/2000\n",
      "106/106 [==============================] - 0s 128us/step - loss: 9.2755 - acc: 0.4245 - val_loss: 10.1484 - val_acc: 0.3704\n",
      "\n",
      "Epoch 00005: loss did not improve from 9.27551\n",
      "Epoch 6/2000\n",
      "106/106 [==============================] - 0s 167us/step - loss: 9.2755 - acc: 0.4245 - val_loss: 10.1484 - val_acc: 0.3704\n",
      "\n",
      "Epoch 00006: loss did not improve from 9.27551\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00006: early stopping\n",
      "Model: \"sequential_16\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_91 (Dense)             (None, 10)                140       \n",
      "_________________________________________________________________\n",
      "dense_92 (Dense)             (None, 8)                 88        \n",
      "_________________________________________________________________\n",
      "dense_93 (Dense)             (None, 6)                 54        \n",
      "_________________________________________________________________\n",
      "dense_94 (Dense)             (None, 6)                 42        \n",
      "_________________________________________________________________\n",
      "dense_95 (Dense)             (None, 4)                 28        \n",
      "_________________________________________________________________\n",
      "dense_96 (Dense)             (None, 3)                 15        \n",
      "=================================================================\n",
      "Total params: 367\n",
      "Trainable params: 367\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "45/45 [==============================] - 0s 2ms/step\n",
      "Loss= 10.387217161390517\n",
      "Accuracy= 0.35555556416511536\n",
      "0.3\n",
      "Train on 106 samples, validate on 27 samples\n",
      "Epoch 1/2000\n",
      "106/106 [==============================] - 0s 61us/step - loss: 9.2755 - acc: 0.4245 - val_loss: 10.1484 - val_acc: 0.3704\n",
      "\n",
      "Epoch 00001: loss did not improve from 9.27551\n",
      "Epoch 2/2000\n",
      "106/106 [==============================] - 0s 126us/step - loss: 9.2755 - acc: 0.4245 - val_loss: 10.1484 - val_acc: 0.3704\n",
      "\n",
      "Epoch 00002: loss did not improve from 9.27551\n",
      "Epoch 3/2000\n",
      "106/106 [==============================] - 0s 121us/step - loss: 9.2755 - acc: 0.4245 - val_loss: 10.1484 - val_acc: 0.3704\n",
      "\n",
      "Epoch 00003: loss improved from 9.27551 to 9.27551, saving model to model.hdf5\n",
      "Epoch 4/2000\n",
      "106/106 [==============================] - 0s 94us/step - loss: 9.2755 - acc: 0.4245 - val_loss: 10.1484 - val_acc: 0.3704\n",
      "\n",
      "Epoch 00004: loss did not improve from 9.27551\n",
      "Epoch 5/2000\n",
      "106/106 [==============================] - 0s 81us/step - loss: 9.2755 - acc: 0.4245 - val_loss: 10.1484 - val_acc: 0.3704\n",
      "\n",
      "Epoch 00005: loss improved from 9.27551 to 9.27551, saving model to model.hdf5\n",
      "Epoch 6/2000\n",
      "106/106 [==============================] - 0s 83us/step - loss: 9.2755 - acc: 0.4245 - val_loss: 10.1484 - val_acc: 0.3704\n",
      "\n",
      "Epoch 00006: loss did not improve from 9.27551\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00006: early stopping\n",
      "Model: \"sequential_17\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_97 (Dense)             (None, 10)                140       \n",
      "_________________________________________________________________\n",
      "dense_98 (Dense)             (None, 8)                 88        \n",
      "_________________________________________________________________\n",
      "dense_99 (Dense)             (None, 6)                 54        \n",
      "_________________________________________________________________\n",
      "dense_100 (Dense)            (None, 6)                 42        \n",
      "_________________________________________________________________\n",
      "dense_101 (Dense)            (None, 4)                 28        \n",
      "_________________________________________________________________\n",
      "dense_102 (Dense)            (None, 3)                 15        \n",
      "=================================================================\n",
      "Total params: 367\n",
      "Trainable params: 367\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "45/45 [==============================] - 0s 2ms/step\n",
      "Loss= 10.387217161390517\n",
      "Accuracy= 0.35555556416511536\n",
      "0.4\n",
      "Train on 106 samples, validate on 27 samples\n",
      "Epoch 1/2000\n",
      "106/106 [==============================] - 0s 103us/step - loss: 9.2755 - acc: 0.4245 - val_loss: 10.1484 - val_acc: 0.3704\n",
      "\n",
      "Epoch 00001: loss did not improve from 9.27551\n",
      "Epoch 2/2000\n",
      "106/106 [==============================] - 0s 104us/step - loss: 9.2755 - acc: 0.4245 - val_loss: 10.1484 - val_acc: 0.3704\n",
      "\n",
      "Epoch 00002: loss did not improve from 9.27551\n",
      "Epoch 3/2000\n",
      "106/106 [==============================] - 0s 203us/step - loss: 9.2755 - acc: 0.4245 - val_loss: 10.1484 - val_acc: 0.3704\n",
      "\n",
      "Epoch 00003: loss did not improve from 9.27551\n",
      "Epoch 4/2000\n",
      "106/106 [==============================] - 0s 213us/step - loss: 9.2755 - acc: 0.4245 - val_loss: 10.1484 - val_acc: 0.3704\n",
      "\n",
      "Epoch 00004: loss did not improve from 9.27551\n",
      "Epoch 5/2000\n",
      "106/106 [==============================] - 0s 238us/step - loss: 9.2755 - acc: 0.4245 - val_loss: 10.1484 - val_acc: 0.3704\n",
      "\n",
      "Epoch 00005: loss did not improve from 9.27551\n",
      "Epoch 6/2000\n",
      "106/106 [==============================] - 0s 149us/step - loss: 9.2755 - acc: 0.4245 - val_loss: 10.1484 - val_acc: 0.3704\n",
      "\n",
      "Epoch 00006: loss did not improve from 9.27551\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00006: early stopping\n",
      "Model: \"sequential_18\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_103 (Dense)            (None, 10)                140       \n",
      "_________________________________________________________________\n",
      "dense_104 (Dense)            (None, 8)                 88        \n",
      "_________________________________________________________________\n",
      "dense_105 (Dense)            (None, 6)                 54        \n",
      "_________________________________________________________________\n",
      "dense_106 (Dense)            (None, 6)                 42        \n",
      "_________________________________________________________________\n",
      "dense_107 (Dense)            (None, 4)                 28        \n",
      "_________________________________________________________________\n",
      "dense_108 (Dense)            (None, 3)                 15        \n",
      "=================================================================\n",
      "Total params: 367\n",
      "Trainable params: 367\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "45/45 [==============================] - 0s 2ms/step\n",
      "Loss= 10.387217161390517\n",
      "Accuracy= 0.35555556416511536\n",
      "0.5\n",
      "Train on 106 samples, validate on 27 samples\n",
      "Epoch 1/2000\n",
      "106/106 [==============================] - 0s 79us/step - loss: 9.2755 - acc: 0.4245 - val_loss: 10.1484 - val_acc: 0.3704\n",
      "\n",
      "Epoch 00001: loss did not improve from 9.27551\n",
      "Epoch 2/2000\n",
      "106/106 [==============================] - 0s 118us/step - loss: 9.2755 - acc: 0.4245 - val_loss: 10.1484 - val_acc: 0.3704\n",
      "\n",
      "Epoch 00002: loss did not improve from 9.27551\n",
      "Epoch 3/2000\n",
      "106/106 [==============================] - 0s 98us/step - loss: 9.2755 - acc: 0.4245 - val_loss: 10.1484 - val_acc: 0.3704\n",
      "\n",
      "Epoch 00003: loss did not improve from 9.27551\n",
      "Epoch 4/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106/106 [==============================] - 0s 106us/step - loss: 9.2755 - acc: 0.4245 - val_loss: 10.1484 - val_acc: 0.3704\n",
      "\n",
      "Epoch 00004: loss did not improve from 9.27551\n",
      "Epoch 5/2000\n",
      "106/106 [==============================] - 0s 102us/step - loss: 9.2755 - acc: 0.4245 - val_loss: 10.1484 - val_acc: 0.3704\n",
      "\n",
      "Epoch 00005: loss did not improve from 9.27551\n",
      "Epoch 6/2000\n",
      "106/106 [==============================] - 0s 113us/step - loss: 9.2755 - acc: 0.4245 - val_loss: 10.1484 - val_acc: 0.3704\n",
      "\n",
      "Epoch 00006: loss did not improve from 9.27551\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00006: early stopping\n",
      "Model: \"sequential_19\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_109 (Dense)            (None, 10)                140       \n",
      "_________________________________________________________________\n",
      "dense_110 (Dense)            (None, 8)                 88        \n",
      "_________________________________________________________________\n",
      "dense_111 (Dense)            (None, 6)                 54        \n",
      "_________________________________________________________________\n",
      "dense_112 (Dense)            (None, 6)                 42        \n",
      "_________________________________________________________________\n",
      "dense_113 (Dense)            (None, 4)                 28        \n",
      "_________________________________________________________________\n",
      "dense_114 (Dense)            (None, 3)                 15        \n",
      "=================================================================\n",
      "Total params: 367\n",
      "Trainable params: 367\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "45/45 [==============================] - 0s 2ms/step\n",
      "Loss= 10.387217161390517\n",
      "Accuracy= 0.35555556416511536\n",
      "0.6\n",
      "Train on 106 samples, validate on 27 samples\n",
      "Epoch 1/2000\n",
      "106/106 [==============================] - 0s 66us/step - loss: 9.2755 - acc: 0.4245 - val_loss: 10.1484 - val_acc: 0.3704\n",
      "\n",
      "Epoch 00001: loss did not improve from 9.27551\n",
      "Epoch 2/2000\n",
      "106/106 [==============================] - 0s 74us/step - loss: 9.2755 - acc: 0.4245 - val_loss: 10.1484 - val_acc: 0.3704\n",
      "\n",
      "Epoch 00002: loss did not improve from 9.27551\n",
      "Epoch 3/2000\n",
      "106/106 [==============================] - 0s 78us/step - loss: 9.2755 - acc: 0.4245 - val_loss: 10.1484 - val_acc: 0.3704\n",
      "\n",
      "Epoch 00003: loss did not improve from 9.27551\n",
      "Epoch 4/2000\n",
      "106/106 [==============================] - 0s 109us/step - loss: 9.2755 - acc: 0.4245 - val_loss: 10.1484 - val_acc: 0.3704\n",
      "\n",
      "Epoch 00004: loss did not improve from 9.27551\n",
      "Epoch 5/2000\n",
      "106/106 [==============================] - 0s 129us/step - loss: 9.2755 - acc: 0.4245 - val_loss: 10.1484 - val_acc: 0.3704\n",
      "\n",
      "Epoch 00005: loss did not improve from 9.27551\n",
      "Epoch 6/2000\n",
      "106/106 [==============================] - 0s 120us/step - loss: 9.2755 - acc: 0.4245 - val_loss: 10.1484 - val_acc: 0.3704\n",
      "\n",
      "Epoch 00006: loss did not improve from 9.27551\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00006: early stopping\n",
      "Model: \"sequential_20\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_115 (Dense)            (None, 10)                140       \n",
      "_________________________________________________________________\n",
      "dense_116 (Dense)            (None, 8)                 88        \n",
      "_________________________________________________________________\n",
      "dense_117 (Dense)            (None, 6)                 54        \n",
      "_________________________________________________________________\n",
      "dense_118 (Dense)            (None, 6)                 42        \n",
      "_________________________________________________________________\n",
      "dense_119 (Dense)            (None, 4)                 28        \n",
      "_________________________________________________________________\n",
      "dense_120 (Dense)            (None, 3)                 15        \n",
      "=================================================================\n",
      "Total params: 367\n",
      "Trainable params: 367\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "45/45 [==============================] - 0s 2ms/step\n",
      "Loss= 10.387217161390517\n",
      "Accuracy= 0.35555556416511536\n",
      "0.7\n",
      "Train on 106 samples, validate on 27 samples\n",
      "Epoch 1/2000\n",
      "106/106 [==============================] - 0s 75us/step - loss: 9.2755 - acc: 0.4245 - val_loss: 10.1484 - val_acc: 0.3704\n",
      "\n",
      "Epoch 00001: loss did not improve from 9.27551\n",
      "Epoch 2/2000\n",
      "106/106 [==============================] - 0s 65us/step - loss: 9.2755 - acc: 0.4245 - val_loss: 10.1484 - val_acc: 0.3704\n",
      "\n",
      "Epoch 00002: loss did not improve from 9.27551\n",
      "Epoch 3/2000\n",
      "106/106 [==============================] - 0s 92us/step - loss: 9.2755 - acc: 0.4245 - val_loss: 10.1484 - val_acc: 0.3704\n",
      "\n",
      "Epoch 00003: loss did not improve from 9.27551\n",
      "Epoch 4/2000\n",
      "106/106 [==============================] - 0s 101us/step - loss: 9.2755 - acc: 0.4245 - val_loss: 10.1484 - val_acc: 0.3704\n",
      "\n",
      "Epoch 00004: loss did not improve from 9.27551\n",
      "Epoch 5/2000\n",
      "106/106 [==============================] - 0s 125us/step - loss: 9.2755 - acc: 0.4245 - val_loss: 10.1484 - val_acc: 0.3704\n",
      "\n",
      "Epoch 00005: loss did not improve from 9.27551\n",
      "Epoch 6/2000\n",
      "106/106 [==============================] - 0s 72us/step - loss: 9.2755 - acc: 0.4245 - val_loss: 10.1484 - val_acc: 0.3704\n",
      "\n",
      "Epoch 00006: loss did not improve from 9.27551\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00006: early stopping\n",
      "Model: \"sequential_21\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_121 (Dense)            (None, 10)                140       \n",
      "_________________________________________________________________\n",
      "dense_122 (Dense)            (None, 8)                 88        \n",
      "_________________________________________________________________\n",
      "dense_123 (Dense)            (None, 6)                 54        \n",
      "_________________________________________________________________\n",
      "dense_124 (Dense)            (None, 6)                 42        \n",
      "_________________________________________________________________\n",
      "dense_125 (Dense)            (None, 4)                 28        \n",
      "_________________________________________________________________\n",
      "dense_126 (Dense)            (None, 3)                 15        \n",
      "=================================================================\n",
      "Total params: 367\n",
      "Trainable params: 367\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "45/45 [==============================] - 0s 2ms/step\n",
      "Loss= 10.387217161390517\n",
      "Accuracy= 0.35555556416511536\n",
      "0.8\n",
      "Train on 106 samples, validate on 27 samples\n",
      "Epoch 1/2000\n",
      "106/106 [==============================] - 0s 82us/step - loss: 9.2755 - acc: 0.4245 - val_loss: 10.1484 - val_acc: 0.3704\n",
      "\n",
      "Epoch 00001: loss did not improve from 9.27551\n",
      "Epoch 2/2000\n",
      "106/106 [==============================] - 0s 77us/step - loss: 9.2755 - acc: 0.4245 - val_loss: 10.1484 - val_acc: 0.3704\n",
      "\n",
      "Epoch 00002: loss did not improve from 9.27551\n",
      "Epoch 3/2000\n",
      "106/106 [==============================] - 0s 79us/step - loss: 9.2755 - acc: 0.4245 - val_loss: 10.1484 - val_acc: 0.3704\n",
      "\n",
      "Epoch 00003: loss did not improve from 9.27551\n",
      "Epoch 4/2000\n",
      "106/106 [==============================] - 0s 83us/step - loss: 9.2755 - acc: 0.4245 - val_loss: 10.1484 - val_acc: 0.3704\n",
      "\n",
      "Epoch 00004: loss did not improve from 9.27551\n",
      "Epoch 5/2000\n",
      "106/106 [==============================] - 0s 73us/step - loss: 9.2755 - acc: 0.4245 - val_loss: 10.1484 - val_acc: 0.3704\n",
      "\n",
      "Epoch 00005: loss did not improve from 9.27551\n",
      "Epoch 6/2000\n",
      "106/106 [==============================] - 0s 89us/step - loss: 9.2755 - acc: 0.4245 - val_loss: 10.1484 - val_acc: 0.3704\n",
      "\n",
      "Epoch 00006: loss did not improve from 9.27551\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00006: early stopping\n",
      "Model: \"sequential_22\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_127 (Dense)            (None, 10)                140       \n",
      "_________________________________________________________________\n",
      "dense_128 (Dense)            (None, 8)                 88        \n",
      "_________________________________________________________________\n",
      "dense_129 (Dense)            (None, 6)                 54        \n",
      "_________________________________________________________________\n",
      "dense_130 (Dense)            (None, 6)                 42        \n",
      "_________________________________________________________________\n",
      "dense_131 (Dense)            (None, 4)                 28        \n",
      "_________________________________________________________________\n",
      "dense_132 (Dense)            (None, 3)                 15        \n",
      "=================================================================\n",
      "Total params: 367\n",
      "Trainable params: 367\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45/45 [==============================] - 0s 2ms/step\n",
      "Loss= 10.387217161390517\n",
      "Accuracy= 0.35555556416511536\n",
      "0.9\n",
      "Train on 106 samples, validate on 27 samples\n",
      "Epoch 1/2000\n",
      "106/106 [==============================] - 0s 62us/step - loss: 9.2755 - acc: 0.4245 - val_loss: 10.1484 - val_acc: 0.3704\n",
      "\n",
      "Epoch 00001: loss did not improve from 9.27551\n",
      "Epoch 2/2000\n",
      "106/106 [==============================] - 0s 71us/step - loss: 9.2755 - acc: 0.4245 - val_loss: 10.1484 - val_acc: 0.3704\n",
      "\n",
      "Epoch 00002: loss did not improve from 9.27551\n",
      "Epoch 3/2000\n",
      "106/106 [==============================] - 0s 103us/step - loss: 9.2755 - acc: 0.4245 - val_loss: 10.1484 - val_acc: 0.3704\n",
      "\n",
      "Epoch 00003: loss did not improve from 9.27551\n",
      "Epoch 4/2000\n",
      "106/106 [==============================] - 0s 78us/step - loss: 9.2755 - acc: 0.4245 - val_loss: 10.1484 - val_acc: 0.3704\n",
      "\n",
      "Epoch 00004: loss did not improve from 9.27551\n",
      "Epoch 5/2000\n",
      "106/106 [==============================] - 0s 75us/step - loss: 9.2755 - acc: 0.4245 - val_loss: 10.1484 - val_acc: 0.3704\n",
      "\n",
      "Epoch 00005: loss did not improve from 9.27551\n",
      "Epoch 6/2000\n",
      "106/106 [==============================] - 0s 90us/step - loss: 9.2755 - acc: 0.4245 - val_loss: 10.1484 - val_acc: 0.3704\n",
      "\n",
      "Epoch 00006: loss did not improve from 9.27551\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00006: early stopping\n",
      "Model: \"sequential_23\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_133 (Dense)            (None, 10)                140       \n",
      "_________________________________________________________________\n",
      "dense_134 (Dense)            (None, 8)                 88        \n",
      "_________________________________________________________________\n",
      "dense_135 (Dense)            (None, 6)                 54        \n",
      "_________________________________________________________________\n",
      "dense_136 (Dense)            (None, 6)                 42        \n",
      "_________________________________________________________________\n",
      "dense_137 (Dense)            (None, 4)                 28        \n",
      "_________________________________________________________________\n",
      "dense_138 (Dense)            (None, 3)                 15        \n",
      "=================================================================\n",
      "Total params: 367\n",
      "Trainable params: 367\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "45/45 [==============================] - 0s 2ms/step\n",
      "Loss= 10.387217161390517\n",
      "Accuracy= 0.35555556416511536\n",
      "1\n",
      "Train on 106 samples, validate on 27 samples\n",
      "Epoch 1/2000\n",
      "106/106 [==============================] - 0s 92us/step - loss: 9.2755 - acc: 0.4245 - val_loss: 10.1484 - val_acc: 0.3704\n",
      "\n",
      "Epoch 00001: loss did not improve from 9.27551\n",
      "Epoch 2/2000\n",
      "106/106 [==============================] - 0s 152us/step - loss: 9.2755 - acc: 0.4245 - val_loss: 10.1484 - val_acc: 0.3704\n",
      "\n",
      "Epoch 00002: loss did not improve from 9.27551\n",
      "Epoch 3/2000\n",
      "106/106 [==============================] - 0s 110us/step - loss: 9.2755 - acc: 0.4245 - val_loss: 10.1484 - val_acc: 0.3704\n",
      "\n",
      "Epoch 00003: loss did not improve from 9.27551\n",
      "Epoch 4/2000\n",
      "106/106 [==============================] - 0s 105us/step - loss: 9.2755 - acc: 0.4245 - val_loss: 10.1484 - val_acc: 0.3704\n",
      "\n",
      "Epoch 00004: loss did not improve from 9.27551\n",
      "Epoch 5/2000\n",
      "106/106 [==============================] - 0s 92us/step - loss: 9.2755 - acc: 0.4245 - val_loss: 10.1484 - val_acc: 0.3704\n",
      "\n",
      "Epoch 00005: loss did not improve from 9.27551\n",
      "Epoch 6/2000\n",
      "106/106 [==============================] - 0s 97us/step - loss: 9.2755 - acc: 0.4245 - val_loss: 10.1484 - val_acc: 0.3704\n",
      "\n",
      "Epoch 00006: loss did not improve from 9.27551\n",
      "Restoring model weights from the end of the best epoch\n",
      "Epoch 00006: early stopping\n",
      "Model: \"sequential_24\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_139 (Dense)            (None, 10)                140       \n",
      "_________________________________________________________________\n",
      "dense_140 (Dense)            (None, 8)                 88        \n",
      "_________________________________________________________________\n",
      "dense_141 (Dense)            (None, 6)                 54        \n",
      "_________________________________________________________________\n",
      "dense_142 (Dense)            (None, 6)                 42        \n",
      "_________________________________________________________________\n",
      "dense_143 (Dense)            (None, 4)                 28        \n",
      "_________________________________________________________________\n",
      "dense_144 (Dense)            (None, 3)                 15        \n",
      "=================================================================\n",
      "Total params: 367\n",
      "Trainable params: 367\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "45/45 [==============================] - 0s 2ms/step\n",
      "Loss= 10.387217161390517\n",
      "Accuracy= 0.35555556416511536\n"
     ]
    }
   ],
   "source": [
    "acc = buildTrainingModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.35555556416511536, 0.35555556416511536, 0.35555556416511536, 0.35555556416511536, 0.35555556416511536, 0.35555556416511536, 0.35555556416511536, 0.35555556416511536, 0.35555556416511536]\n"
     ]
    }
   ],
   "source": [
    "print(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_sizes, acc)\n",
    "plt.xlabel(\"% Training Sizes\")\n",
    "plt.ylabel(\"% \")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
